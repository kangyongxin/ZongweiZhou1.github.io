<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>阅读笔记-Repulsion loss:Detecting Pedestrians in a Crowd</title>
      <link href="/2019/05/21/repulsion-loss/"/>
      <url>/2019/05/21/repulsion-loss/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>尽管目标检测目前已经取得了非常好的性能，但是针对于特定领域内的跟踪方法还可以进一步探讨。本篇文章的研究重点在于更好的检测拥挤场景下的行人。</p><p>文章首先分析了拥挤场景下SOTA检测方法存在的问题，然后提出了一种针对于拥挤场景专门设计的回归损失。该损失函数的启发源主要有两点：预测框应该尽可能和目标接近；同时预测框应尽可能地狱surrounding目标区分。</p><p>实验证明本文提出的损失函数能够在拥挤场景中很大提升SOTA方法的性能。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>一般情形下，遮挡可以分为两种： 类间遮挡(inter-class occlusion)和类内遮挡(intra-class occlusion). 在行人检测任务上， inter-class occlusion例如行人被汽车，树木等他类遮挡；intra-class occlusion 如人与人之间的遮挡。</p><p>crowd occlusion对行人检测的影响主要体现在增加了pedestrian定位的难度。例如两个目标$T,B$发生了遮挡， 那么检测器就会因为这两个target overlap部分的特征相同而出现较大的误差。导致$T$的预测窗口可能更偏向于$B$ 或者相反。更糟糕的是，一般detection之后的结果需要进行NMS操作，NMS操作可能凭IOU将发生shift的预测框直接剔除，导致miss detection的产生。所以crowd occlusion环境对于NMS的阈值较敏感。 阈值较高剔除的框越少可能产生的false positive越多， 阈值较小剔除较多导致miss detections很多。</p><p>传统的检测方法使用box regression技术localized 目标，具体而言就是让proposals和ground-truth之间的距离尽可能地接近，距离度量可以使用$\text{Smooth}_{L1}$ 或者IoU等。但是这种约束只是让predict尽可能地和target接近，而并没有考虑到target周围的影响。如下图所示， 一般检测方法只越是红色虚线框与棕色框尽可能接近，而对于和蓝色框的关系不加约束。本文解决的正是这个问题。</p><p><img src="/2019/05/21/repulsion-loss/illustration.png" alt="illustration"></p><p>本文的贡献点：</p><ul><li>实验分析了crowd occlusion对于行人检测的影响。在CityPersons benchmark上定量的分析了因crowd occlusion导致的false positives和miss detections的变化。</li><li>提出了两种repulsion loss处理crowd occlusion 问题： RepGT Loss和RepBox Loss。 RepGT loss惩罚的是预测框偏向其他的ground truth的程度， RepBox是为了让预测的box之间尽可能地分开，以削弱对NMS阈值的依赖。</li><li>利用提出的repulsion losses，训练了一个crowd-robust的端到端网络，并在CityPerson和Caltech-USA上验证了模型性能。另外PASCAL VOC数据集表明提出的repulsion loss对于通用目标的检测同样有积极作用。</li></ul><hr><h3 id="拥挤遮挡分析"><a href="#拥挤遮挡分析" class="headerlink" title="拥挤遮挡分析"></a>拥挤遮挡分析</h3><p><strong>数据集合度量</strong>   CityPersons是语义分割数据集CityScapes上新创建的行人检测数据集。35000个人以及附加的13000左右的ignored regions， 提供了行人的bounding box和可见部分的标注。本文的实验都是在CityPerson的reasonable 的train/validation子集上进行的。评估使用FPPI(False Positives Per Image)的对数值.($MR^{-2}$) 该指标越小越好。</p><p><strong>Detector</strong> 检测器使用的还是Faster RCNN框架。不同点在于将backbone由VGG-16替换成ResNet-50.<strong>注意</strong> ResNet-50很少在行人检测中使用，主要是因为ResNet-50下采样的倍数太大，最终得到的特征太少。所以文章中使用了膨胀卷积(dilated convolution)使最后输出的特征大小事输入图像的1/8.ResNet-based 检测器在验证集上取得了$14.6 MR^{-2}$</p><p><strong>检测失败原因分析</strong> </p><ul><li><p>Miss detections.</p><p>数据集中样本的可见部分也使用box标注，所以样本的遮挡率可以得到  $occ = 1 - \frac{area(BBox_{visible})}{area(BBox)}$ </p><p>在ground-truth上定义不同的子集。</p><p>​        <em>occlusion case</em>:    $occ \ge 0.1$;</p><p>​        <em>crowd occlusion case</em>: $occ \ge 0.1$ 且存在至少一个gt box与其$IoU\ge 0.1$ .</p><p>按照上述定义， reasonable 验证集中1579个标注行人中，有$810(51.3)$的遮挡样本，称为reasonable-occ子集， $479(30.3%)$个crowd occlusion样本，称为reasonable-crowd子集。</p><p>下图分析了在reasonable, reasonable-occ, reasonable-crowd三种子集上miss detection的统计情况。</p><p><img src="/2019/05/21/repulsion-loss/miss_detection.png" alt="miss_detection"></p><p>横轴的label表示在指定false positive数目条条件下，miss detection的数目， 比如#Miss@20fp表示调节参数使得保留20个false positive下的miss detection数目。可以看到在所有的miss detections中，源于reasonable-crowd的大约占了60%，表明crowd occlusion是影响detector性能的主要因素。另外通过降低NMS的阈值，如false positive从100到500， 因crowd occlusion引起的miss detections比例从60.7%上升到了69.2%， 表明<strong>NMS降低阈值并不能解决因crowd occlusion引起的漏检问题</strong>。</p><p>在下图(a)中，给出了miss detection 随着检测器置信度阈值增大的变化曲线。可以发现baseline方法在置信度较高时会产生更多的漏检，这部分主要是由于detections之间的重叠导致的。同时该图表明了RepGT对于miss detection和False positive的作用</p><p><img src="/2019/05/21/repulsion-loss/miss detection_det_score.png" alt="miss_detections_det_score"><img src="/2019/05/21/repulsion-loss/proportion of crowed error.png" alt="proportion of crowed error"></p></li><li><p>False Positives</p><p>false positives可以分为三类：</p><ul><li>background error: 预测框和任意的target $IoU \le 0.1$; </li><li>localization error: 与一个且只有一个target的$IoU \ge 0.1$ ，注意这里是指没有匹配上，但依然与某些目标存在IOU; </li><li>Crowd error: predictions 与至少两个ground-truth的$IoU\ge 0.1$ </li></ul><p>上图中的(b)图表示crowed errors随着false positives的变化所占的比例。可以发现其几乎稳定在$20\%$左右。</p><p>也表明在相同的False positive 数中RepGT loss中Crowd error的占比一直低于baseline中产生的Crowd error占比。</p><p>下图一些crowd error的例子。</p><p><img src="/2019/05/21/repulsion-loss/crowd error1.png" style="zoom:10"><img src="/2019/05/21/repulsion-loss/crowd error2.png" style="zoom:10"></p><p>其中绿色表示正确的匹配，红色表示crowd error。错误一般是由于向临近的目标偏移导致的。</p></li><li><p>Conclusion</p><blockquote><p>The analysis on failure cases validates our observation: <strong>pedestrian detectors are surprisingly tainted by crowd occlusion, as it constitutes the majority of missed detections and results in more false positives by increasing the difficulty in localization</strong>.</p></blockquote></li></ul><hr><h3 id="Repulsion-Loss"><a href="#Repulsion-Loss" class="headerlink" title="Repulsion Loss"></a>Repulsion Loss</h3><p>总的损失 $L = L_{Attr} + \alpha <em> L_{RepGT} + \beta </em> L_{RepBox}$ </p><p>其中attraction term $L_{Attr}$表示预测框和真实框之间的相似度， $L_{RepGT}$表示的预测框与其它真实框尽可能地远离， $L_{RepBox}$表示预测框与其他target的预测框尽可能地远离。 </p><p>$P = (l_p, t_p, w_p, h_p), G=(l_G, t_G, w_G, h_G)$ 分别表示提取的框和真实的框， $P_+=\{P\}$  表示所有正样本组成的集合 (存在至少一个ground-truth背景 $IoU \ge 0.5$ ), $\mathcal{G} = \{G\}$表示所有的ground-truth </p><p><strong>Attraction Term</strong> </p><script type="math/tex; mode=display">L_{Attr} = \frac{\sum_{P\in P_+}Smooth_{L_1}(B^P, G_{Attr}^P)}{|P_+|}</script><p>其中 $G_{Attr}^P = arg max_{G\in \mathcal{G}} IoU(G, P)$ </p><p> <strong>Repulsion Term (RepGT) </strong></p><script type="math/tex; mode=display">G_{Rep}^P = arg max_{G\in\mathcal{G\setminus{G_{Attr}^P}}}IoU(G, P)</script><p>表示的是与预测框的IoU第二大的ground truth</p><script type="math/tex; mode=display">L_{Attr} = \frac{\sum_{P\in P_+}Smooth_{ln}(IoG(B^P, G_{Rep}^P))}{|P_+|}</script><p>其中$IoG(B, G) = \frac{area(B\and G)}{area(G)}$ </p><script type="math/tex; mode=display">Smooth_{ln} = \begin{cases}-ln(1-x), ~~~~~~~~~~x\le \sigma \\\frac{x-\sigma}{1-\sigma} - ln(1-\sigma), ~~x\gt \sigma\end{cases}</script><p>$\sigma \in [0, 1)$是光滑参数。</p><p><strong>RepulsionTerm（RepBox）</strong></p><p>为了让检测结果对NMS尽可能地鲁棒， RepBox Loss的目的是让predict与其他目标的target的尽可能地远离。将$P_+$ 根据IoU分配到$|\mathcal{G}|$ 中，$P_+ = P_1 \and P_2 \and \cdots \and P_{|\mathcal{G}|}$. 那么对于任意两个来自于不同子集的proposals，希望他们的重叠度尽可能的小</p><script type="math/tex; mode=display">L_{RepBox} = \frac{\sum_{i\neq j}Smooth_{ln}(IoU(B^{P_i}, B^{P_j}))}{\sum_{i\neq j}\bold{1}[IoU(B^{P_i}, B^{P_j}) > 0] + \epsilon}</script><p>其中$\bold{1}$ 表示identity 函数， $\epsilon$ 是很小的常量防止除零操作。</p><p><strong>Discussion</strong> </p><ul><li><p>Distance metric。 在计算repulsion term时， 使用IoG, IoU而不是$Smooth_{L1}$ 是由于 IoG, IoU 取值[0,1]，而SmoothL1无界， 使用SmoothL1会要求predicted box与repulsion ground-truth越远越好， 而IoG只是要求overlap越小越好，所以后者与我们设计思想更搭。另外在RepGT中使用IoG而不是IoU是因为，IoU-based loss可能导致enlarge bounding box增大union area来降低IoU，而IoG的降低则必须是overlap的降低.</p></li><li><p>Smooth Parameter $\sigma$ . 如下图. 所示$\sigma$ 可以调节repulsion loss对outliers的鲁棒性。</p><p><img src="/2019/05/21/repulsion-loss/sigma.png" alt="sigma"></p></li></ul><hr><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><strong>Dataset</strong></p><ul><li>CityPersons</li><li>Caltech-USA 2.5小时的视频，划分为训练集(42500帧)和测试集(4024帧)</li></ul><p><strong>Training Details</strong></p><ul><li>CityPersons: 80k iterations, init_lr = 0.016, 60k之后lr下降10倍</li><li>Caltech-USA：160k iterations, init_lr = 0.016, 120k 之后lr下降10倍</li><li>SGD</li><li>4 GPU</li><li>minibatch=4</li><li>weight decay=0.0001， momentum=0.9</li><li>No multi-scale training/testing set used</li><li>For caltech-USA, 10x set(~42k frames) is used for training</li><li>Online Hard Example Mining (OHEM) is used to accelerate covergence</li></ul><p><strong>Ablation Study</strong></p><ul><li><p>RepGT Loss</p><p>Table 1中比较了不同的$\sigma$ 对SmoothL1loss的影响， $\sigma=0$ 意味着直接将 $-ln(1-IoG)$相加。</p><p><img src="/2019/05/21/repulsion-loss/ablation.png" alt="ablation"></p><p><img src="/2019/05/21/repulsion-loss/parameters.png" alt="parameters"></p></li><li><p>RepBox Loss</p><p>由table1可以发现在$\sigma=0$的时候RepBox Loss的性能最好， $\sigma=0$时相当于对IoU进行求和。这个可能是因为 RepGT处理的是预测和ground truth之间的重叠，这种情形下outliers其实很少，所以对outliers的抑制较大， 而RepBox处理的框更多，显然outliers也更多，这时候因为都是predict所以不知道之间哪个更可靠，就惩罚的少一点。</p><p>下图左给出了NMS在FPPI=0.01时不同阈值下，对应Miss Rate的变化，发现RepBox Loss 相对于baseline 对NMS的阈值更加鲁棒。下图右给的例子中显示的是在Crowd中也很少有predict处于两个ground-truth box之间，这样对NMS的阈值依赖更低.</p><p><img src="/2019/05/21/repulsion-loss/RepBoxLoss.png" alt="repboxloss"><img src="/2019/05/21/repulsion-loss/visualize.png" alt></p></li><li><p>Balance of RepGT and RepBox</p><p>Table 2给出了不同的$\alpha, \beta$下RepGT和RepBox Loss的作用，实验表明$\alpha=0.5, \beta=0.5$时性能做好</p></li></ul><p><strong>Comparisons with state-of-the-art methods</strong></p><p><img src="/2019/05/21/repulsion-loss/table3.png" alt="table3"></p><p>其中Reasonable set： $occlusion\le 35\%$;   Partial set: $10\% \lt occlusion \le 35\%$; Bare set: $occlusion \le 10\%$ Heavy set: $occlusion\gt 35\%$  。图8给出了具体的MR和FPPI的曲线图， 右上角的label对应IoU=0.5下的MR</p><p><img src="/2019/05/21/repulsion-loss/table4.png" alt></p><p><img src="/2019/05/21/repulsion-loss/table5.png" alt></p><p><img src="/2019/05/21/repulsion-loss/fig8.png" alt> </p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文提出的loss其本质上创新性并不是很强，但作者针对于特定的问题提出了解决办法，并实验证明了算法有效。这点还是值得借鉴的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Pedestrian Detection </tag>
            
            <tag> Repulsion Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多目标跟踪总结(下)-资源汇总</title>
      <link href="/2019/05/21/MOT-overview-3rd/"/>
      <url>/2019/05/21/MOT-overview-3rd/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文主要收集MOT领域的一些资源， 包括数据集，相关论文以及部分开源代码等。</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><div class="table-container"><table><thead><tr><th>Dataset</th><th style="text-align:center">MultiView</th><th style="text-align:center">Groundtruth</th><th>Site</th></tr></thead><tbody><tr><td>PETS2006</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\times$</td><td><a href="www.cvg.rdg.ac.uk/PETS2006/data.html">www.cvg.rdg.ac.uk/PETS2006/data.html</a></td></tr><tr><td>PETS2007</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="http://www.cvg.reading.ac.uk/PETS2007/" target="_blank" rel="noopener">http://www.cvg.reading.ac.uk/PETS2007/</a></td></tr><tr><td>PETS2009</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.cvg.rdg.ac.uk/PETS2009/a.html">www.cvg.rdg.ac.uk/PETS2009/a.html</a></td></tr><tr><td>CAVIAR</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/" target="_blank" rel="noopener">http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/</a></td></tr><tr><td>TUD</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.d2.mpi-inf.mpg.de/datasets">www.d2.mpi-inf.mpg.de/datasets</a></td></tr><tr><td>TRECVID</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\times$</td><td><a href="http://www-nlpir-nist.gov/projects/" target="_blank" rel="noopener">http://www-nlpir-nist.gov/projects/</a></td></tr><tr><td>Caltech Pedestrian</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/">www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/</a></td></tr><tr><td>UBC Hockey</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="www.cs.ubc.ca/~okumak/research.html">www.cs.ubc.ca/~okumak/research.html</a></td></tr><tr><td>AVSS 2007</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.eecs.qmul.ac.uk/~andrea/avss2007_d.html">www.eecs.qmul.ac.uk/~andrea/avss2007_d.html</a></td></tr><tr><td>ETH pedestrian</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.vision.ee.ethz.ch/~aess/dataset/">www.vision.ee.ethz.ch/~aess/dataset/</a></td></tr><tr><td>ETHZ Central</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.vision.ee.ethz.ch/datasets">www.vision.ee.ethz.ch/datasets</a></td></tr><tr><td>Town Centre</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.htnl#datasets">www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.htnl#datasets</a></td></tr><tr><td>Zara</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data" target="_blank" rel="noopener">https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data</a></td></tr><tr><td>UCSD</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm" target="_blank" rel="noopener">http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm</a></td></tr><tr><td>UCF Crowds</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="www.crcv.ucf.edu/data/crowd.php">www.crcv.ucf.edu/data/crowd.php</a></td></tr><tr><td>KITTI</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">http://www.cvlibs.net/datasets/kitti/eval_tracking.php</a></td></tr><tr><td>MOT2015</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/2D_MOT_2015/" target="_blank" rel="noopener">https://motchallenge.net/data/2D_MOT_2015/</a></td></tr><tr><td>MOT2016</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/MOT16/" target="_blank" rel="noopener">https://motchallenge.net/data/MOT16/</a></td></tr><tr><td>MOT2017</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/MOT17/" target="_blank" rel="noopener">https://motchallenge.net/data/MOT17/</a></td></tr><tr><td>MOT2019</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/CVPR_2019_Tracking_Challenge/" target="_blank" rel="noopener">https://motchallenge.net/data/CVPR_2019_Tracking_Challenge/</a></td></tr></tbody></table></div><p>目前多目标跟踪主要使用的数据集是MOTChallenge数据集， 包括MOT15， MOT16， MOT17和MOT19.</p><h3 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h3><blockquote><p> <strong>Evaluation Metric</strong> </p></blockquote><p><strong>CLEAR MOT</strong> :       Bernardin, K. &amp; Stiefelhagen, R. “Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metric”  <a href="https://cvhci.anthropomatik.kit.edu/images/stories/msmmi/papers/eurasip2008.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>IDF1</strong>:                       Ristani, E., Solera, F., Zou, R., Cucchiara, R. &amp; Tomasi, C. “Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking” <a href="https://users.cs.duke.edu/~ristani/bmtt2016/ristani2016MTMC.pdf" target="_blank" rel="noopener">paper</a>       </p><p><strong>MTMCT</strong>                  Ristani, E., Solera, F., Zou, R. S., Cucchiara, R., &amp; Tomasi, C. (2016). Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking. <a href="doi.org/10.1007/978-3-319-48881-3_2">paper</a></p><p><strong>MOT15</strong>:                   Leal-Taixé L, Milan A, Reid I, et al. Motchallenge 2015: Towards a benchmark for multi-target tracking . <a href="https://arxiv.org/abs/1504.01942" target="_blank" rel="noopener">paper</a>                   </p><p><strong>MOT16</strong> :                  Milan A, Leal-Taixé L, Reid I, et al. MOT16: A benchmark for multi-object tracking. <a href="https://arxiv.org/pdf/1603.00831" target="_blank" rel="noopener">paper</a></p><p><strong>Evaluation Code</strong>: <a href="https://bitbucket.org/amilan/motchallenge-devkit/" target="_blank" rel="noopener">matlab</a>, <a href="https://github.com/cheind/py-motmetrics" target="_blank" rel="noopener">python</a></p><blockquote><p><strong>Overview</strong> </p></blockquote><p>Emami, P., Pardalos, P. M., Elefteriadou, L., &amp; Ranka, S. (2018). Machine Learning Methods for Solving Assignment Problems in Multi-Target Tracking, 1(1), 1–35. <a href="arxiv.org/abs/1802.06897">paper</a></p><p>Leal-Taixé, L., Milan, A., Schindler, K., Cremers, D., Reid, I., &amp; Roth, S. (2017). Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking. <a href="arxiv.org/abs/1704.0278">paper</a></p><p>Luo, W., Xing, J., Milan, A., Zhang, X., Liu, W., Zhao, X., &amp; Kim, T.-K. (2014). Multiple Object Tracking: A Literature Review, 1–18.  <a href="arxiv.org/abs/1409.7618">paper</a></p><p>Li, X., Hu, W., Shen, C., Zhang, Z., &amp; Dick, A. (2013). A Survey of Appearance Models in Visual Object Tracking, 1–42. <a href="arxiv.org/pdf/1303.4803">paper</a></p><p>Poore, A. B., &amp; Gadaleta, S. (2006). Some assignment problems arising from multiple target tracking, 43, 1074–1091. <a href="doi.org/10.1016/j.mcm.2">paper</a></p><p>Yilmaz, A., &amp; Javed, O. (2006). Object Tracking : A Survey, 38(4).  <a href="doi.org/10.1145/1177352">paper</a></p><p>A 101 slide . <a href="http://vision.stanford.edu/teaching/cs231b_spring1415/slides/greedy_fahim_albert.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p><strong>2019</strong> </p></blockquote><p><strong>NT</strong>:                 Longyin Wen<em>, Dawei Du</em>, Shengkun Li, Xiao Bian, Siwei Lyu Learning Non-Uniform Hypergraph for Multi-Object Tracking, In AAAI 2019.  <a href="http://www.cs.albany.edu/~lsw/papers/aaai19a.pdf  from  github.com/longyin880815" target="_blank" rel="noopener">paper</a></p><p><strong>FMA</strong>:           Zhang, J., Zhou, S., Wang, J., &amp; Huang, D. (2019). Frame-wise Motion and Appearance for Real-time Multiple Object Tracking, (1).  <a href="arxiv.org/abs/1905.02292">paper</a></p><p><strong>STRN</strong>:           Xu, J., Cao, Y., Zhang, Z., &amp; Hu, H. (2019). Spatial-Temporal Relation Networks for Multi-Object Tracking.  <a href="arxiv.org/abs/1904.11489">paper</a></p><p><strong>LSST</strong>:             Feng, W., Hu, Z., Wu, W., Yan, J., &amp; Ouyang, W. (2019). Multi-Object Tracking with Multiple Cues and Switcher-Aware Classification.  <a href="arxiv.org/abs/1901.06129">paper</a></p><p><strong>MOTS</strong>:            Voigtlaender, P., Krause, M., Osep, A., Luiten, J., Sekar, B. B. G., Geiger, A., &amp; Leibe, B. (2019). MOTS: Multi-Object Tracking and Segmentation.  <a href="arxiv.org/abs/1902.03604">paper</a></p><p><strong>FAMNet</strong>:     Chu, P., &amp; Ling, H. (2019). FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking. <a href="arxiv.org/abs/1904.04989">paper</a></p><p><strong>FANTrack</strong>: Baser, E., Balasubramanian, V., Bhattacharyya, P., &amp; Czarnecki, K. (2019). FANTrack: 3D Multi-Object Tracking with Feature Association Network.  <a href="https://arxiv.org/abs/1905.02843" target="_blank" rel="noopener">paper</a>, <a href="https://git.uwaterloo.ca/wise-lab/fantrack" target="_blank" rel="noopener">code</a></p><p><strong>IATracker</strong>:   Chu, P., Fan, H., Tan, C. C., &amp; Ling, H. (2019). Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic Model Refreshment.  <a href="arxiv.org/abs/1902.08231">paper</a></p><blockquote><p><strong>2018</strong> </p></blockquote><p><strong>SST</strong>:                 Sun. S., Akhtar, N., Song, H., Mian A., &amp; Shah M. (2018). Deep Affinity Network for Multiple Object Tracking.  <a href="https://arxiv.org/abs/1810.11780" target="_blank" rel="noopener">paper</a>, <a href="https://github.com/shijieS/SST" target="_blank" rel="noopener">code</a></p><p><strong>CCC</strong>:                 Keuper, M., Tang, S., Andres, B., Brox, T., &amp; Schiele, B. (2018). Motion Segmentation &amp; Multiple Object Tracking by Correlation Co-Clustering. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>8828</em>(c), 1–13.   <a href="doi.org/10.1109/TPAMI.2018.2876253">paper</a></p><p><strong>HAF</strong>:                 Sheng, H., Zhang, Y., Chen, J., Xiong, Z., &amp; Zhang, J. (2018). Heterogeneous Association Graph Fusion for Target Association in Multiple Object Tracking. IEEE Transactions on Circuits and Systems for Video Technology. <a href="doi.org/10.1109/TCSVT.2018.2882192">paper</a></p><p><strong>TNT</strong>:               Wang, G., Wang, Y., Zhang, H., Gu, R., &amp; Hwang, J.-N. (2018). Exploit the Connectivity: Multi-Object Tracking with TrackletNet.  <a href="arxiv.org/abs/1811.07258">paper</a></p><p><strong>PHD</strong>:              Fang, K., Xiang, Y., Li, X., &amp; Savarese, S. (2018). Recurrent Autoregressive Networks for Online Multi-Object Tracking. <em>WACV</em>.   <a href="yuxng.github.io/fang_wacv18.pdf">paper</a></p><p><strong>DMAN</strong>:            Zhu, Ji and Yang, Hua and Liu, Nian and Kim, Minyoung and Zhang, Wenjun and Yang, Ming-Hsuan “Online Multi-Object Tracking with Dual Matching Attention Networks”   <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ji_Zhu_Online_Multi-Object_Tracking_ECCV_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>C-DRL</strong>:             Ren, Liangliang and Lu, Jiwen and Wang, Zifeng and Tian, Qi and Zhou, Jie “Collaborative Deep Reinforcement Learning for Multi-Object Tracking” <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Liangliang_Ren_Collaborative_Deep_Reinforcement_ECCV_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>SADF</strong>:             Yoon, Y., Boragule, A., Song, Y., Yoon, K., &amp; Jeon, M. (2018). Online Multi-Object Tracking with Historical Appearance Matching and Scene Adaptive Detection Filtering.  <a href="ieeexplore.ieee.org/document/8639078">paper</a></p><p><strong>MOTDT</strong>:           Long Chen, Haizhou Ai “Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-identification” in ICME 2018.     <a href="https://www.researchgate.net/publication/326224594_Real-time_Multiple_People_Tracking_with_Deeply_Learned_Candidate_Selection_and_Person_Re-identification" target="_blank" rel="noopener">paper</a>, <a href="https://github.com/longcw/MOTDT" target="_blank" rel="noopener">code</a></p><p><strong>DeepCC</strong>:          Ristani and C. Tomasi “Features for Multi-Target Multi-Camera Tracking and Re-Identification” In CVPR 2018 <a href="https://arxiv.org/pdf/1803.10859.pdf" target="_blank" rel="noopener">paper</a>,  <a href="https://github.com/ergysr/DeepCC" target="_blank" rel="noopener">code</a></p><p><strong>THOPA-net</strong>:     Fabbri, M., Lanzi, F., Calderara, S., &amp; Vezzani, R. (2018). Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World. <a href="researchgate.net/publication/323957071_Learning_to_Detect_and_Track_Visible_and_Occluded_Body_Joints_in_a_Virtual_World">paper</a></p><p><strong>MHT-bLSTM</strong>:  Kim, Chanho and Li, Fuxin and Rehg, James M “Multi-object Tracking with Neural Gating Using Bilinear LSTM” .  <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chanho_Kim_Multi-object_Tracking_with_ECCV_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>Trajectory Factory</strong>: Cong Ma, Changshui Yang, Fan Yang, Yueqing Zhuang, Ziwei Zhang, Huizhu Jia, Xiaodong Xie “Trajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese Bi-GRU for Multiple Object Tracking” In ICME 2018.   <a href="https://arxiv.org/abs/1804.04555" target="_blank" rel="noopener">paper</a></p><p><strong>MOTBeyondPixels</strong>: Sarthak Sharma,  Junaid Ahmed Ansari,  J. Krishna Murthy,  and K. Madhava Krishna Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking In ICRA 2018 <a href="https://arxiv.org/abs/1802.09298" target="_blank" rel="noopener">paper</a>,  <a href="https://github.com/JunaidCS032/MOTBeyondPixels" target="_blank" rel="noopener">code</a></p><p>Henschel, R., Leal-Taixe, L., Cremers, D., &amp; Rosenhahn, B. (2018). Fusion of head and full-body detectors for multi-object tracking. <em>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</em>, <em>2018</em>–<em>June</em>, 1509–1518.   <a href="doi.org/10.1109/CVPRW.2018.00192">paper</a></p><p>Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes “Tracking by Prediction: A Deep Generative Model for Mutli-Person localisation and Tracking” In WACV 2018. <a href="https://arxiv.org/pdf/1803.03347.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p><strong>2017</strong></p></blockquote><p><strong>D2T</strong>:                  Feichtenhofer, C., Pinz, A., &amp; Zisserman, A. (2017). Detect to Track and Track to Detect. In ICCV2017.  <a href="doi.org/10.1109/ICCV.2017.330">paper</a>, <a href="github.com/feichtenhofer/Detect-Track">code</a></p><p><strong>IOU</strong>:                  Bochinski, E., Eiselein, V., &amp; Sikora, T. (2017). High-Speed tracking-by-detection without using image information. <em>AVSS 2017</em>. <a href="doi.org/10.1109/AVSS.2017.8078516">paper</a>, <a href="github.com/bochinski/iou-tracker/">code</a></p><p><strong>CIWT</strong>:                Aljosa Osep, Alexander Hermans Combined Image and World-Space Tracking in Traffic Scenes. In ICRA 2017.  <a href="vision.rwth-aachen.de/media/papers/paper_final_compressed.pdf">paper</a>,  <a href="github.com/aljosaosep/ciwt">code</a></p><p><strong>RCMSS</strong>:              Naiel, M. A., Ahmad, M. O., Swamy, M. N. S., Lim, J., &amp; Yang, M. H. (2017). Online multi-object tracking via robust collaborative model and sample selection. In CVIU 2017.  <a href="doi.org/10.1016/j.cviu.2016.07.003">paper</a>, <a href="users.encs.concordia.ca/~rcmss/">code</a></p><p><strong>EAMTT</strong>:              Tang, S., Andriluka, M., Andres, B., &amp; Schiele, B. (2017). Multiple people tracking by lifted multicut and person re-identification.  In CVPR 2017. <a href="doi.org/10.1109/CVPR.2017.394">paper</a></p><p><strong>STAM</strong>:                 Chu, Q., Ouyang, W., Li, H., Wang, X., Liu, B., &amp; Yu, N. (2017). Online Multi-object Tracking Using CNN-Based Single Object Tracker with Spatial-Temporal Attention Mechanism.  In ICCV2017. <a href="doi.org/10.1109/ICCV.2017.518">paper</a></p><p><strong>DeepSORT</strong>:         Wojke, N., Bewley, A., &amp; Paulus, D. (2017). Simple Online and Realtime Tracking with a Deep Association Metric.  In ICIP2017.  <a href="doi.org/10.1109/ICIP.2017.8296962">paper</a>, <a href="github.com/nwojke/deep_sort">code</a> </p><p><strong>Quad-CNN</strong>:         Son, J., Baek, M., Cho, M., &amp; Han, B. (2017). Multi-object tracking with quadruplet convolutional neural networks. In CVPR2017.  <a href="doi.org/10.1109/CVPR.2017.403">paper</a></p><p><strong>Art-Tracker</strong>:       Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, Bernt Schiele “Art Track: Articulated Multi-Person Tracking in the Wild”  In CVPR2017. <a href="https://arxiv.org/abs/1612.01465" target="_blank" rel="noopener">paper</a></p><p><strong>SOTforMOT</strong>:         He, Q., Wu, J., Yu, G., &amp; Zhang, C. (2017). SOT for MOT.  <a href="arxiv.org/abs/1712.01059">paper</a></p><p><strong>NMGC-MOT</strong>:         Maksai, A., Wang, X., Fleuret, F., &amp; Fua, P. (2017). Non-Markovian Globally Consistent Multi-Object Tracking.  In ICCV2017.  <a href="openaccess.thecvf.com/content_ICCV_2017/papers/Maksai_Non-Markovian_Globally_Consistent_ICCV_2017_paper.pdf">paper</a>  , <a href="github.com/maksay/ptrack_cpp">code</a></p><p><strong>RNN_LSTM</strong>:          Milan, A., Rezatofighi, S. H., Dick, A., Reid, I., &amp; Schindler, K. (2017). Online Multi-Target Tracking Using Recurrent Neural Networks.  AAAI 2017.  <a href="arxiv.org/abs/1604.03635">paper</a>,  <a href="bitbucket.org/amilan/rnntracking">code</a></p><p><strong>ReidTracking</strong>:               Beyer, L., Breuers, S., Kurin, V., &amp; Leibe, B. (2017). Towards a Principled Integration of Multi-Camera Re-Identification and Tracking through Optimal Bayes Filters.  <a href="arxiv.org/abs/1705.04608">paper</a>, <a href="github.com/VisualComputingInstitute/towards-reid-tracking">code</a></p><p><strong>DeepNetworkFlows</strong>:  Schulter, S., Vernaza, P., Choi, W., &amp; Chandraker, M. (2017). Deep network flow for multi-object tracking.  In CVPR 2017.  <a href="doi.org/10.1109/CVPR.2017.292">paper</a></p><p>Sadeghian, A., Alahi, A., &amp; Savarese, S. (2017). Tracking the Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies.  In ICCV2017. <a href="doi.org/10.1109/ICCV.2017.41">paper</a></p><blockquote><p><strong>2016</strong></p></blockquote><p><strong>CPD</strong>:                       Lee, B., Erdenee, E., Jin, S., &amp; Rhee, P. K. (2016). Multi-Class Multi-Object Tracking using Changing Point Detection.  <a href="doi.org/10.1007/978-3-319-48881-3">paper</a></p><p><strong>POI</strong> :                        Yu, F., Li, W., Li, Q., Liu, Y., Shi, X., &amp; Yan, J. (2016). POI: Multiple Object Tracking with High Performance Detection and Appearance Feature.  In BMTT 2016.  <a href="https://arxiv.org/pdf/1610.06136.pdf" target="_blank" rel="noopener">paper</a>,  <a href="https://drive.google.com/open?id=0B5ACiy41McAHMjczS2p0dFg3emM" target="_blank" rel="noopener">detections</a></p><p><strong>SORT</strong>:                      Bewley, A., Ge, Z., Ott, L., Ramos, F., &amp; Upcroft, B. (2016). Simple online and realtime tracking.   In ICIP 2016.  <a href="doi.org/10.1109/ICIP.2016.7533003">paper</a>, <a href="github.com/abewley/sort">code</a></p><p><strong>RCMSS</strong> :                  Mohamed A. Naiel1, M. Omair Ahmad, M.N.S. Swamy, Jongwoo Lim, and Ming-Hsuan Yang “Online Multi-Object Tracking Via Robust Collaborative Model and Sample Selection. In CVIU2016. <a href="https://users.encs.concordia.ca/~rcmss/include/Papers/CVIU2016.pdf" target="_blank" rel="noopener">paper</a>, <a href="https://users.encs.concordia.ca/~rcmss/" target="_blank" rel="noopener">code</a> </p><p><strong>Social-LSTM</strong>:          Goel, K., Fei-Fei, L., Savarese, S., Alahi, A., Robicquet, A., &amp; Ramanathan, V. (2016). Social LSTM: Human Trajectory Prediction in Crowded Spaces.  In CVPR2016.  <a href="doi.org/10.1109/cvpr.2016.110">paper</a>, </p><blockquote><p><strong>2015</strong></p></blockquote><p><strong>MDP</strong>:                      Xiang, Y., Alahi, A., &amp; Savarese, S. (2015). Learning to Track: Online Multi-object Tracking by Decision Making.  In ICCV2015.  <a href="doi.org/10.1109/ICCV.2015.534">paper</a>, <a href="cvgl.stanford.edu/projects/MDP_tracking/">code</a></p><p><strong>CEM</strong>:                       Chari, V., Lacoste-Julien, S., Laptev, I., &amp; Sivic, J. (2014). On Pairwise Costs for Network Flow Multi-Object Tracking.  In CVPR2015.  <a href="arxiv.org/abs/1408.3304">paper</a>, <a href="milanton.de/contracking/">code</a> </p><p><strong>ALFD</strong>:                      Choi, W. (2015). Near-online multi-target tracking with aggregated local flow descriptor. In ICCV2015. <a href="doi.org/10.1109/ICCV.2015.347">paper</a> </p><p><strong>LDCT</strong>:                      Solera, F. (2015). Learning to Divide and Conquer for Online Multi-Target Tracking. In ICCV 2105.  <a href="imagelab.ing.unimore.it/imagelab/researchActivity.asp?idActivity=09">paper</a>, <a href="github.com/francescosolera/LDCT">code</a></p><p><strong>TMPORT</strong>:               Ristani, E., &amp; Tomasi, C. (2015). Tracking multiple people online and in real time. <a href="doi.org/10.1007/978-3-319-16814-2_29">paper</a>, <a href="vision.cs.duke.edu/DukeMTMC/">code</a></p><p><strong>JPDArevisited</strong>:     Rezatofighi, S. H., Milan, A., Zhang, Z., Shi, Q., Dick, A., &amp; Reid, I. (2015). Modified Joint Probabilistic Data Association.  In ICCV2015.  <a href="doi.org/10.1109/ICCV.2015.349">paper</a></p><p><strong>MHTrevisited</strong>:      Vinet, L., &amp; Zhedanov, A. (2015). Multiple Hypothesis Tracking Revisited. In ICCV2015. <a href="doi.org/10.1088/1751-8113/44/8/085201">paper</a>, <a href="rehg.org/mht/">code</a></p><p><strong>headTracking</strong>:     Zhang, S., Wang, J., Wang, Z., Gong, Y., &amp; Liu, Y. (2015). Multi-target tracking by learning local-to-global trajectory models.  In PR, 48(2), 580-590.  <a href="doi.org/10.1016/j.patcog.2014.08.013">paper</a>,  <a href="github.com/gengshan-y/headTracking">code</a></p><blockquote><p><strong>2014</strong></p></blockquote><p><strong>H2T</strong>:                         Wen, L., Li, W., Yan, J., Lei, Z., Yi, D., &amp; Li, S. Z. (2014). Multiple target tracking based on undirected hierarchical relation hypergraph.  In CSC on CVPR2014.  <a href="doi.org/10.1109/CVPR.2014.167">paper</a>,  <a href="cbsr.ia.ac.cn/users/lywen/">code</a></p><p><strong>CMOT</strong>:                     Bae, S. H., &amp; Yoon, K. J. (2014). Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning.  In CSC on CVPR2014.  <a href="doi.org/10.1109/CVPR.2014.159">paper</a>, <a href="cvl.gist.ac.kr/project/cmot.html">code</a></p><p><strong>OPCNF</strong>:                  Chari, V., Lacoste-Julien, S., Laptev, I., &amp; Sivic, J. (2014). Continuous Energy Minimization for Multi-Target Tracking,  TPAMI 2014.  <a href="milanton.de/files/pami2014/pami2014-anton.pdf">paper</a>,  <a href="di.ens.fr/willow/research/flowtrack/">code</a></p><p>Tang, S., Andriluka, M., &amp; Schiele, B. (2014). Detection and tracking of occluded people. IJCV, <em>110</em>(1), 58–69.   <a href="doi.org/10.1007/s11263-013-0664-6">paper</a></p><p>Yang, B., &amp; Nevatia, R. (2014). Multi-target tracking by online learning a CRF model of appearance and motion patterns.  IJCV, <em>107</em>(2), 203–217.  <a href="doi.org/10.1007/s11263-013-0666-4">paper</a></p><blockquote><p><strong>2013</strong></p></blockquote><p><strong>SMOT</strong>:                      Dicle, C., Camps, O. I., &amp; Sznaier, M. (2013). The way they move: Tracking multiple targets with similar appearance.  In ICCV2013.  <a href="doi.org/10.1109/ICCV.2013.286">paper</a>,, <a href="bitbucket.org/cdicle/smot">code</a></p><p>Milan, A., Schindler, K., &amp; Roth, S. (2013). Detection- and trajectory-level exclusion in multiple object tracking.  In CSC on CVPR2013.  <a href="doi.org/10.1109/CVPR.2013.472">paper</a></p><p>Salvi, D., Waggoner, J., Temlyakov, A., &amp; Wang, S. (2013). A graph-based algorithm for multi-target tracking with occlusion. In WACV 2013. <a href="doi.org/10.1109/WACV.2013.6475059">paper</a></p><blockquote><p><strong>2012</strong></p></blockquote><p><strong>GMCP-Tracker</strong>:          Zamir, A. R., Dehghan, A., &amp; Shah, M. (2012). GMCP-Tracker : Global Multi-object Tracking Using Generalized Minimum Clique Graphs, 343–356. <a href="crcv.ucf.edu/papers/eccv2012/GMCP-Tracker_ECCV12.pdf">paper</a>,  <a href="crcv.ucf.edu/projects/GMCP-Tracker/">code</a></p><p><strong>OMPTTH</strong>:                     Zhang, J., Lo Presti, L., &amp; Sclaroff, S. (2012). Online multi-person tracking by tracker hierarchy. In AVSS 2012. <a href="doi.org/10.1109/AVSS.2012.51">paper</a>, <a href="cs-people.bu.edu/jmzhang/tracker_hierarchy/Tracker_Hierarchy.htm">code</a> </p><p>Yan, X., Wu, X., Kakadiaris, I. A., &amp; Shah, S. K. (2012). To Track or To Detect ? An Ensemble Framework for Optimal Selection, 594–607. <a href="link.springer.com/conter/10.1007%2F978-3-642-33715-4_43">paper</a></p><p>Hu, W., Li, X., Luo, W., Zhang, X., Maybank, S., &amp; Zhang, Z. (2012). Single and multiple object tracking using log-euclidean riemannian subspace and block-division appearance model. PAMI, 34(12), 2420-2440. <a href="doi.org/10.1109/TPAMI.2012.42">paper</a></p><p>Yang, B., &amp; Nevatia, R. (2012). Online learned discriminative part-based appearance models for multi-human tracking. In ECCV 2012. <a href="doi.org/10.1007/978-3-642-33718-5_35">paper</a></p><p>Shu, G., Dehghan, A., Oreifej, O., Hand, E., &amp; Shah, M. (2012). Part-based multiple-person tracking with partial occlusion handling.  In CSC on CVPR2012. <a href="doi.org/10.1109/CVPR.2012.6247879">paper</a></p><blockquote><p><strong>2011 and Before</strong></p></blockquote><p><strong>KSP</strong>:                               Berclaz. (2011). Multiple Object Tracking using K-shortes Paths.  PAMI2011. <a href="cvlab.epfl.ch/files/content/sites/cvlab2/files/publications/publications/2011/BerclazFTF11.pdf">paper</a>, <a href="cvlab.epfl.ch/software/ksp">code</a> </p><p><strong>MTDF</strong>:                           Pedro F. Felzenszwalb, Ross B. Girshick, D. M. and D. R. (2010). Object detection with discriminatively trained part-based models. in TPAMI 2010. <a href="doi.org/10.1109/MC.2014.42">paper</a></p><p>Andriyenko, A., Roth, S., &amp; Schindler, K. (2011). An analytical formulation of global occlusion reasoning for multi-target tracking.  ICCV 2011.  <a href="doi.org/10.1109/ICCVW.2011.6130472">paper</a></p><p>Andriyenko, A., &amp; Schindler, K. (2011). Multi-target tracking by continuous energy minimization.  In CVPR 2011. <a href="doi.org/10.1109/CVPR.2011.5995311">paper</a></p><p>Pirsiavash, H., Ramanan, D., &amp; Fowlkes, C. (2011). Globally-Optimal Greedy Algorithms for Tracking a Variable Number of Objects.  CVPR2011. <a href="people.csail.mit.edu/hpirsiav/papers/tracking_cvpr11.pdf">paper</a></p><p>Mitzel, D., Horbert, E., Ess, A., &amp; Leibe, B. (2010). Multi-person tracking with sparse detection and continuous segmentation.  ECCV2010.  <a href="doi.org/10.1007/978-3-642-15549-9_29">paper</a></p><p>Hu, M., Ali, S., &amp; Shah, M.  Detecting global motion patterns in complex videos. ICPR2008. <a href="doi.org/10.1109/icpr.2008.4760950">paper</a></p><p>Breitenstein, M. D., Reichlin, F., Leibe, B., Koller-Meier, E., &amp; Van Gool, L. (2009). Robust tracking-by-detection using a detector confidence particle filter. ICCV2009.  <a href="doi.org/10.1109/ICCV.2009.5459278">paper</a></p><p>Zhang, L., Li, Y., &amp; Nevatia, R. (2008). Global data association for multi-object tracking using network flows. CVPR2008. <a href="doi.org/10.1109/CVPR.2008.4587584">paper</a></p><h3 id="Other-resources"><a href="#Other-resources" class="headerlink" title="Other resources"></a>Other resources</h3><p><a href="http://bbs.cvmart.net/articles/265/zi-yuan-duo-mu-biao-zhui-zong-zi-yuan-lie-biao-shu-ju-ji-lun-wen-dai-ma-he-niu-ren-zhu-ye-deng" target="_blank" rel="noopener">SpyderXu</a>,  <a href="https://github.com/SpyderXu/multi-object-tracking-paper-list" target="_blank" rel="noopener">github</a> , <a href="github.com/huanglianghua/mot-papers/blob/master/README.md">github</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> overview </tag>
            
            <tag> code and paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多目标跟踪总结(中)-深度方法</title>
      <link href="/2019/05/20/MOT-overview-2nd/"/>
      <url>/2019/05/20/MOT-overview-2nd/</url>
      
        <content type="html"><![CDATA[<h3 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h3><p>参考[SIGAI]公众号， <img src="/2019/05/20/MOT-overview-2nd/sigai.png" alt="sigai"></p><p>随着深度学习的推进，基于深度学习的检测器性能提升明显。因此目前的多目标跟踪算法大都基于tracking-by-detection框架。于是MOT任务可以转化为detection+ReID问题。</p><p>但相对于与传统的ReID问题，MOT问题会更加复杂。首先，MOT任务中目标轨迹变化频繁， 图像样本库的数量和种类都不固定； 其次，检测结果可能出现新的目标，也可能出现漏检；另外，检测图像并不像行人重识别中的查询图像都是比较准确地检测结果， MOT任务中行人检测往往混杂着误检或者不准确的检测，尤其是相互遮挡产生时。检测行人的不对齐，相互遮挡给目标匹配带来了极大的挑战性。</p><p>本文将主要总结一些基于深度网络，希望更好解决MOT任务中ReID子任务的方法。</p><hr><h3 id="基于深度学习的多目标跟踪算法分类"><a href="#基于深度学习的多目标跟踪算法分类" class="headerlink" title="基于深度学习的多目标跟踪算法分类"></a>基于深度学习的多目标跟踪算法分类</h3><p>基于深度学习的多目标跟踪算法的主要任务是优化检测之间相似性或距离度量的设计。根据 学习特征的不同，基于深度学习的多目标跟踪可以分为表观特征的深度学习，基于相似性度 量的深度学习，以及基于高阶匹配特征的深度学习。</p><p><img src="/2019/05/20/MOT-overview-2nd/catagetory.png" alt="catagetory"></p><ol><li>利用深度网络学习目标检测的表观鉴别特征能有效提升匹配跟踪性能。比如利用图像识别或者行人重识别中的特征抽取网络替换MOT框架中的hand-craft表观特征，或者基于深度网络的光流特征计算运动相关性。</li><li>利用深度学习方法进行度量学习。比如设计网络计算检测和轨迹的距离函数，或者设计二分类代价函数，判断目标是否属于同一类。</li><li>基于深度网络学习高阶特征。比如考虑以跟踪轨迹和检测之间的匹配或者tracklets之间的匹配等。深度网络学习高阶特征匹配可以学习多帧表观特征的高阶匹配相似性，也可以学习运动特征的匹配相关度。</li></ol><h3 id="深度视觉多目标跟踪算法介绍"><a href="#深度视觉多目标跟踪算法介绍" class="headerlink" title="深度视觉多目标跟踪算法介绍"></a>深度视觉多目标跟踪算法介绍</h3><h4 id="基于Siamese网络结构的跟踪算法"><a href="#基于Siamese网络结构的跟踪算法" class="headerlink" title="基于Siamese网络结构的跟踪算法"></a>基于Siamese网络结构的跟踪算法</h4><p>该类方法以两个经crop之后尺寸相同的检测图像块作为输入，输出的是二分类判别是否属于同一类。 一般Siamese结构有三种形式。如下所示：</p><p><img src="/2019/05/20/MOT-overview-2nd/siamese.png" alt="siamese"></p><p>一般而言第一种方式属于特征学习，第二三两种属于度量学习。实验表明，第三种拓扑结构能够生成更好地实验效果。所以Lealtaixe等人采用第三种形式结构计算检测之间的匹配度。原始的检测经过预处理成对和光流对的输入网络中。这些预处理包括正则化LUV空间，resize到固定大小$121\times 53$等。</p><p>网络的输入层通道数为$10$, 后面跟有$3$个卷积层，$4$个全连接层和binary-softmax损失层。损失函数为</p><script type="math/tex; mode=display">E=\frac{1}{2N}\sum_{n=1}^N(y\Phi(d_1, d_2)+(1-y)\max(\tau-\Phi(d_1, d_2), 0))</script><p>训练过程中，从真实跟踪数据中抽取训练样本。利用检测算法得到的同一条轨迹的检测作为正样本，不同轨迹的检测作为负样本，为了增加样本多样性，增强模型的泛化能力， 负样本还包括从检测响应周围随机采集的重叠率较小的图像块。训练细节：SGD优化器， bs=128， lr=0.01, num_epoches=50</p><p>在孪生网络训练完成后， 作者采用第六层全连接网络的输出作为表观特征，然后通过gradient boosting算法融合目标的上下文信息，包括：尺寸相对变化、位置相对变化和速度相对变化。其结构图如下所示：</p><p><img src="/2019/05/20/MOT-overview-2nd/siamese2.png" alt="siamese2"></p><p>多目标跟踪过程采用全局最优算法框架，通过对任意两个检测建立连接关系，生成匹配矩阵，采用最小代价网络流的方式转化为线性规划进行求解。</p><h4 id="基于最小多个图模型的多目标跟踪算法"><a href="#基于最小多个图模型的多目标跟踪算法" class="headerlink" title="基于最小多个图模型的多目标跟踪算法"></a>基于最小多个图模型的多目标跟踪算法</h4><p>Siyu Tang等人利用深度学习计算的类似光流特征结合子图多割模型在MOT任务上取得了很好的效果。</p><p>该方法属于离线确定性推导方法。作者认为对于检测响应的处理，例如NMS过于粗糙，会导致有些正确的检测被抑制掉，同时同一帧图像中检测之间的关系也没有考虑。于是作者剔除了MOT任务中的一般假设：一条轨迹最多对应一个检测，同样一个检测最多对应一条轨迹。使用多帧之间的所有检测响应构建图模型，并提出子图划分的算法进行求解轨迹。</p><p>如下图所示, 每一个顶点对应一个检测响应，不同的颜色对应不同的目标响应，相同颜色对应相同目标的不同检测响应。</p><p><img src="/2019/05/20/MOT-overview-2nd/subgraph.png" alt="subgraph"></p><p>子图多割的模型：</p><script type="math/tex; mode=display">\min_{x\in\{0, 1\}}\sum_{e\in E}c_ex_e\\\text{s.t.}\quad \forall C\in cycles(G), \\ \forall e\in C:x_e\le\sum_{e'\in C\backslash \{e\}}x_e'</script><p>其中$c_e$表示每条边的代价， $x_e$表示每条边是否关联的示性变量， 约束条件表示对于图$G$中任意的环路$cycles(G)$之一$C$，任意两点如果存在一条通路，即不等式右边为0， 那么这两点之间必定相同，即$x_e=0$.也就是说一个环路中若存在分割边，那么至少存在两条，于是环路就可以分割成不同的图，代表不同的轨迹。该模型可以使用KLj算法求解。</p><p>在计算检测之间的关联关系时，作者利用deepmatching光流方法构建了$5$维特征。</p><script type="math/tex; mode=display">f_1 = MI/MU\\f_2 = \min(\varepsilon_v, \varepsilon_w)\\f_3 = f_1\cdot f_2~~~~~~~~~\\f_4 = f_1^2, f_5 = f_2^2</script><p>MI, MU分别表示对应响应中光流点集的交集和并集的基数比， $\varepsilon_v, \varepsilon_w$分别表示置信度。 利用5维特征学习逻辑回归分类器用于计算两者属于相同目标的概率$p_e$, 注意这里的训练是在训练集上利用特征学习回归分类器，而计算匹配概率$p_e$时则是在推断阶段。获得了匹配概率$p_e$其实已经可以采用匈牙利算法等进行匹配，但是为了实现本文的motivation， 计算(2)式中的代价$c_e=\frac{p_e}{1-p_E}$, 进而更好地计算匹配关联。</p><p>为了更好地利用长时信息，已解决遮挡或者错误关联问题，作者又在子图多割基础上提出了提升边的改进。其基本思想是将图中的连接边进一步划分为常规边和提升边，分别用来刻画短期和长期匹配关系。</p><p>示例</p><p><img src="/2019/05/20/MOT-overview-2nd/lmp.png" alt="lmp"></p><p>在子图(a),(b)中有3条真实轨迹， $v_1$对应$t$时刻的一个目标， $v_2, v_3$对应$t, t+1$时刻的同一个目标， $v_4$对应$t+1$时刻的新目标， 每条轨迹的代价如图所示，因此采用MP方法，则将$v_1v_2, v_3v_4$切断，这样就会导致$v_1, v_4$划分为一类，发生错误匹配。而对于LMP， $v_1v_4$属于提升边，提升边另外考虑，因此并不会分错。同理的是图(c)(d), 同一个目标可能因为遮挡或者光照原因，中间若干帧发生表观变化，这时候采用MP算法会导致跟踪片段， 而采用提升边方法可以有效避免这种现象。</p><p>提升边的子图多割相对于子图多割方法额外增加了两条约束</p><script type="math/tex; mode=display">\forall vw\in E'\backslash E\forall P\in vw-paths(G): x_{vw}\le \sum_{e\in P}x_e</script><p>这里$E$ 表示时间空间约束的常规边， $E’$则是常规边和提升边的合集， 提升边是指时间间隔大于某阈值，而表观非常相似的检测之间的关联。上述约束表示对于任意的两个顶点$v,w$其中存在的提升边$vw$和任意的通路$P$,那么如果常规边是连通的则提升边必定连通。</p><script type="math/tex; mode=display">\forall vw\in E' \backslash E \forall C\in vw-cuts(G): 1-x_{vw} \le \sum_{e\in C}(1-x_e)</script><p>$vw-cuts(G)$表示节点$v,w$之间所有可能存在切边的路径， 于是可以知道，如果$vw$之间不存在任何通路，则提升边必定是切边。</p><p>目标函数和MP的目标函数相同，不同的是MP构造了5维特征， 而LMP则是利用siamese结构网络提取表观特征再与MP中特征结合构建目标函数。</p><p>siamese结构的表观相似度网络</p><p><img src="/2019/05/20/MOT-overview-2nd/lmp_appearance.png" alt="lmp_appearance"></p><p>LMP算法目前在MOT16benchmark上的性能依然算是SOTA。</p><p><img src="/2019/05/20/MOT-overview-2nd/lmp_mot16.png" alt="lmp_mot16"></p><h4 id="时空注意力机制的多目标跟踪-STAM"><a href="#时空注意力机制的多目标跟踪-STAM" class="headerlink" title="时空注意力机制的多目标跟踪(STAM)"></a>时空注意力机制的多目标跟踪(STAM)</h4><p>该方法借助注意力机制处理多目标跟踪中目标遮挡问题， 并对每一个目标建立单独的分类器，判断检测和轨迹是否匹配，其本质上是单目标跟踪算法在多目标任务上的扩展。其流程图如下：</p><p><img src="/2019/05/20/MOT-overview-2nd/stam_architecture.png" alt="stam_architecture"></p><p>空间注意力机制用于给不同程度的遮挡部分不同的特征权重，从而能够更好地抽取可鉴别特征， 时间注意力机制则主要是给历史样本不同的权重用于在线更新模型。</p><p>空间注意力模型如下图中(b)所示,对于经过ROIPooling之后的对齐特征学习其每个空间位置的权重，由于训练样本不足，直接学习spatial attention map比较困难，因此作者采用了分阶段的思想。</p><p>首先希望能够回归出目标的visibility map，这部分可以提供额外的标签信息，因此学习起来相对简单，然后再有visibility map学习spatial attention map. 获得spatial attention map之后采用乘性方式叠加到池化特征上获得refined feature maps，然后从该特征训练二值分类器。</p><p><img src="/2019/05/20/MOT-overview-2nd/stam_model.png" alt="stam_model"></p><h4 id="基于LSTM的多线索融合多目标跟踪"><a href="#基于LSTM的多线索融合多目标跟踪" class="headerlink" title="基于LSTM的多线索融合多目标跟踪"></a>基于LSTM的多线索融合多目标跟踪</h4><p>前面叙述的方法很少利用时间上的性质，即使用到的话也是简单的attention机制提供权重， 而这篇文章则直接将LSTM网络应用到MOT的时序信息上，并同时融合了时序，表观，空间等线索用于多目标跟踪。</p><p>其基本流程如下所示，使用lstm分别刻画提取appearance， motion和interaction的相似特征。然后通过在线方式将三种不同的特征融合起来进行度量学习，最后有学习的度量计算相似度，构建二部图再进行数据关联。</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_architecture.png" alt="lstm_architecture"></p><ul><li><p>基于LSTM的表观特征</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_appearance.png" alt="lstm_appearance"></p><p>输入是crop和resize只有的每个目标，lstm用于抽取已获得轨迹在最后时刻的特征， 然后将lstm特征和当前待匹配的检测响应的特征输入到siamese的浅层网络中融合特征。</p></li><li><p>基于LSTM的运动特征</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_motion.png" alt="lstm_motion"></p><p>基于lstm的motion特征和appearance特征网络相似，不同点在于motio输入的是历史跟踪结果的瞬时速度信息，而appearance输入的经过CNN抽取的表观特征。</p></li><li><p>基于LSTM的空间特征</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_spatial.png" alt="lstm_spatial"></p><p>值得一提的是，基于LSTM的interaction特征，输入是2D拓扑图，刻画的是目标的spatial context。将目标的邻域范围划分网格，然后有目标的网格则置为1，否则置为0.这种做法具有一定的刻画空间上下文的能力，但其实相当粗暴。</p><h4 id="基于双线性LSTM的多目标跟踪"><a href="#基于双线性LSTM的多目标跟踪" class="headerlink" title="基于双线性LSTM的多目标跟踪"></a>基于双线性LSTM的多目标跟踪</h4><p>Kim等人在基于LSTM融合特征的多目标跟踪算法基础上提出了基于双线性LSTM的多目标跟踪方法。</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_BiLSTM.png" alt="lstm_BiLSTM"></p><p>图中(a)图示双线性LSTM的结构图，其区别在于将lstm提取的特征与检测响应的特征进行了乘性相关运算从而更好地计算相似度。</p><p>作者发现使用提出的基于双线性的LSTM结构能更好地刻画表观特征，但对于motion特征不如直接基于lstm的结构（这可能是因为LSTM目前对高维特征的刻画不够准确），所以最终融合两种方式在MHT框架下提出了MHT-bLSTM算法，其性能如下</p><p><img src="/2019/05/20/MOT-overview-2nd/mht_blstm.png" alt="mht_blstm"></p><p>可以发现本文方法其实MOTA性能并不是很好，其提升点主要在于IDF1指标，说明其对于保持跟踪的稳定具有不错的效果。</p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>近些年多目标跟踪领域的深度学习方法更多的集中在深度特征和深度度量的学习，很少有采用深度学习解决目标之间数据关联的工作，最近又一篇deep match 的文章值得关注，后续我们继续解读。</p><p>我认为目标跟踪未来的发展趋势应该集中于两点：首先如何确定轨迹的起点和终点，这部分目前存在一些利用强化学习处理的工作；其次如何直接使用深度学习的方式解决数据关联问题，这部分应该可以从图卷积网络入手。</p><p>除了上面的发展趋势，我认为目标CNN或者LSTM框架下的研究方向也包括两点：首先是继续提升目标检测的精度，尤其是遮挡环境，精确地目标检测结果对于目标跟踪影响很大；其次是如何解决遮挡导致跟踪失败的问题。</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多目标跟踪总结(上)-传统方法</title>
      <link href="/2019/05/20/MOT-overview-1st/"/>
      <url>/2019/05/20/MOT-overview-1st/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>参考SIGAI公众号推文，欢迎关注SIGAI公众号。</p><p>目标跟踪是计算机视觉中的一个重要任务。在计算机视觉的三层结构中，目标跟踪属于中间层，是其他高层任务，如动作识别、行为分析等，的基础。 其广泛应用于视频监控，人机交互，自动驾驶， 医学图像，虚拟现实和增强现实等现实场景中。目标跟踪根据每帧跟踪目标个数不同又划分为单目标跟踪(Single Object Tracking, SOT)和多目标跟踪(Multiple Object Tracking, MOT)。</p><p>单目标跟踪任务在图像序列第一帧中采用bounding box或者其他方式标定待跟踪目标，然后利用跟踪算法在之后帧中逐步确定该目标在每一帧的位置。单目标跟踪场景中一般会遇到物体形变，背景干扰，偶然遮挡等因素。</p><p>多目标跟踪任务是在图像序列的每一帧中同时确定目标的位置和身份，即轨迹ID。MOT任务除了会遇到单目标场景中的物体形变，背景干扰等因素，还具有一些独特的更复杂的问题。</p><ul><li>待跟踪目标如何处理目标的出现和消失。MOT任务中，目标是可以在任意时刻出现或者消失在视野中的。</li><li>如何鉴别不同的个体。MOT任务中有时多个目标来源于同一类别，比如行人跟踪，他们具有相似的表观和轮廓，如何区分他们是降低ID switch，提升性能的关键。</li><li>跟踪目标之间的交互与遮挡。MOT任务中目标个数较多，交互复杂，特别是拥挤场景中相互遮挡严重，如何解决相互遮挡是保障跟踪顺利的关键。</li><li>目标重出现时如何重识别。当目标长时间被遮挡后再次出现，如何正确的找回原来的轨迹编号也是个难题。</li></ul><p>多目标跟踪任务中出现的一些术语：</p><ul><li>目标： 在图像中，明显区别于周围环境的闭合区域往往被称为目标，这些目标一般是一些感兴趣区域ROI。</li><li>检测： 通过算法得到目标在图像中的位置的过程称为检测。近些年检测器的发展非常迅速，从最初的模板匹配到目前流行的深度学习方法，比如Faster RCNN系列， SSD系列， YOLO系列， CornerNet系列等。检测器的性能得到极大的提升。</li><li>跟踪：不同帧中，物理意义下的同一目标相关联的过程</li><li>检测响应：检测过程的输出量，即检测结果。在不引起混淆的环境中，也用‘检测’表示检测响应。</li><li>跟踪假设：每一帧中跟踪的结果。</li><li>轨迹：MOT系统的输出量，一条轨迹对应这一个目标在一个时间段内中的位置序列。</li><li>轨迹片段： 完整轨迹中的一些连贯的较短的轨迹碎片。</li><li>数据关联：目标之间的匹配。MOT任务中在数据关联阶段一般假设一条轨迹只能对应一个检测响应， 同时一个检测响应最多对应一条轨迹。</li><li></li></ul><p>这篇文章简单介绍和归纳一些经典的多目标跟踪方法。</p><hr><h3 id="多目标跟踪算法分类"><a href="#多目标跟踪算法分类" class="headerlink" title="多目标跟踪算法分类"></a>多目标跟踪算法分类</h3><p>多目标跟踪问题最早提出是在雷达信号中同时跟踪多架敌机和多枚导弹。这些算法后来被借鉴用于机器视觉领域的多目标跟踪任务。多目标跟踪算法分类不是很严格，根据不同的分类标准有不同的分类方法。比如</p><ol><li><p>按目标初始化方式的不同划分为Detection Based Tracking (DBT)和Detection Free Tracking (DFT)。 </p><p>DFT需要在目标出现的第一帧中标定目标位置，之后跟踪中边检测目标边跟踪目标。DBT是将MOT任务划分为两个阶段：Detection + Data Association. 随着检测器性能的大幅提升，基于检测的多目标跟踪算法是目前主流的跟踪算法。</p></li><li><p>按目标处理过程中用到的信息范围划分为在线跟踪和离线跟踪。</p><p>离线跟踪需要用到当前时刻之前和之后的信息，利用这些特征构建图，网络流等复杂的模型，然后跟踪当前时刻目标，这类方法能够很好的解决遮挡问题，一般性能较好。但是其明显的不足点在于很难适应实时跟踪的需求。在线跟踪则要求对当前时刻的检测响应进行跟踪是只能利用之前和当前的信息，该类算法的难点在于处理遮挡和检测的不准确问题。</p></li><li><p>按跟踪算法的表示形式和优化框架划分为确定性推导和概率统计最大化两类。</p><p>确定性推导是将检测和已跟踪的轨迹看做二元变量，通过构建整体的目标函数，计算最佳匹配，比如经典的二部图分配等。概率统计最大化方法是将检测和轨迹的关系通过概率模型表示，然后通过最大化该概率模型计算最优分配，比如基于贝叶斯的Kalman 滤波和粒子滤波， 马尔科夫链等。</p></li><li><p>按应用角度不同可分为运动场景、航拍场景等。</p><p>运动场景，比如运动员的跟踪。运动场中同一队伍的队员队服几乎相同，并且运动场景图像角度，尺寸变化较大给跟踪带来很大困难，当然运动场景中一些先验假设比如场地边缘等信息给目标跟踪带来一定帮助。航拍场景中目标往往太小，帧率太低，位移太大，较难处理。这类方法主要依靠上下文信息解决。</p></li></ol><p>一般而言，MOT的不同分类是存在重叠的。MOT的分类只是提供了不同角度理解跟踪算法，对算法本身并没有多大影响。MOT算法中起决定作用的是检测，鉴别特征和数据关联。</p><hr><h3 id="经典多目标跟踪算法介绍"><a href="#经典多目标跟踪算法介绍" class="headerlink" title="经典多目标跟踪算法介绍"></a>经典多目标跟踪算法介绍</h3><h4 id="多假设跟踪-MHT"><a href="#多假设跟踪-MHT" class="headerlink" title="多假设跟踪(MHT)"></a>多假设跟踪(MHT)</h4><p>多假设跟踪本质上是卡尔曼滤波方法在多目标跟踪任务上的拓展。定义$k$时刻之前的检测响应为$z^k$, 历史总的检测响应为$Z^k$, 多假设跟踪的目标是求解已有轨迹和当前检测之间关联的条件概率模型。 把似然关联假设$\Theta_k$划分为当前关联假设$\theta_k$和$k-1$时刻的关联假设$\Theta_{k-1}$, 那么依贝叶斯推理可以得到</p><script type="math/tex; mode=display">\begin{align}P\{\Theta_k|Z^k\} &= P\{\theta_k, \Theta_{k-1}|z^k, Z^{k-1}\} \\&= \frac{1}{c}p[Z^k|\theta_k,\Theta_{k-1}]P\{\theta_k|\Theta_{k-1}, Z^{k-1}\}P\{\Theta_{k-1}|Z^{k-1}\}\end{align}</script><p>后验概率通过贝叶斯公式转换为先验概率。最下面一行等式右边第3项是$k-1$时刻的后验概率，第2项是$k$时刻之前的检测响应和对应轨迹的条件下计算当前时刻关联假设的概率，第1项是在获得当前项和历史关联假设前提下出现当前检测响应的概率。$c$是推导过程中的分母，常数，用于保证概率范围。该公式逐帧计算是迭代过程，每一帧需要计算第1,2两项。</p><p>MHT采用两个概率模型建模第1,2两项</p><blockquote><ul><li>用均匀分布和高斯分布对关联对应的检测观察建模.</li><li>用泊松分布对当前假设的似然概率建模</li></ul></blockquote><p>前者表示，当检测响应来自轨迹$T$时，它应符合$T$在当前时刻的高斯分布，否则认为是一个均匀分布的噪声，类似于卡尔曼滤波跟踪。后者表示，在误检和新对象出现概率确定的情况下，出现当前关联的可能性可以通过泊松分布和二项分布的乘积表示。在以上假设下，关联假设的后验分布是历史累计概率密度的 连乘，转化为对数形式，可以看出总体后验概率的对数是每一步观察似然和关联假设似然的求和。因此，选择最佳的关联假设，转化为观察似然和关联假设似然累计求和的最大化。在具体算法中， I.J.Cox等人提出一种基于假设树的优化算法，如下图</p><p><img src="/2019/05/20/MOT-overview-1st/MHT.png" alt="MHT"></p><p>可以发现其实MHT有局部遍历的思想， 并且利用到了之后的信息，属于离线方式。</p><p>任何时刻都可能存在多种假设关联，因此到k时刻的假设构成了一种组合假设树的层次关系。 例如图4左边表示的是2个轨迹和3个观测之间可能形成的关联假设，可能存在的假设有{观测 23=&gt;轨迹1，观测22=&gt;轨迹2, 观测21=&gt;新轨迹}或者{观测22=&gt;轨迹1，观测21=&gt;轨迹 2, 观测23=&gt;新轨迹}，因此产生2个假设分支。图4右侧是从这2个关联假设出发的三层假设 树关系，可以看出随着假设层数的增多，关联假设出现组合爆炸的可能。因此进行必要的剪 枝减少假设空间的数目是必须的步骤。那么如何选择最佳的关联呢？I.J.Cox采用了2个步骤 来实现。首先，限制假设树的层数为3层。其次，是对每个分支的叶节点概率对数进行求和， 最大的分支进行保留，即选择边缘概率最大的那个分支假设作为最后选择的关联。可以把这 种选择方法简单的表示为：</p><script type="math/tex; mode=display">S(k-3) = \frac{1}{|H(k)|}\sum_{\theta^k\in H(k)}\log([Z^k|\theta_k,\Theta_{k-1}])+\log(P\{\theta_k|\Theta_{k-1}, Z^{k-1}\})</script><p>其中$H(k)$是所有可能的假设集合，所以上式是从多个假设中筛选置信度最高的假设作为跟踪结果。</p><p>这种基于似然概率对数累加的方法虽然方便迅速，但是存在一个主要的限制，即假定 观测关联符合高斯模型，并且在每一步选择关联假设之后，需要利用Kalman滤波更新轨迹状 态。通过对MHT基本公式(1)的扩展，可以建立不同的概率模型描述这种多假设关联的全 局概率，例如Kim等人在ICCV2015和ECCV2018通过归一化的最小均方差优化算法引入表 观模型来扩展MHT算法，取得不错的多行人跟踪结果.</p><h4 id="基于检测置信度的粒子滤波算法"><a href="#基于检测置信度的粒子滤波算法" class="headerlink" title="基于检测置信度的粒子滤波算法"></a>基于检测置信度的粒子滤波算法</h4><p>该算法可分为两个步骤：</p><ul><li>利用贪心算法，将每一帧检测结果分配给已有的轨迹结果</li><li>利用匹配结果，计算每个对象的粒子权重，作为粒子滤波框架中的观测似然概率。</li></ul><p>粒子滤波框架如下图所示：</p><p><img src="/2019/05/20/MOT-overview-1st/pf.png" alt="PF"></p><p>分别表示检测响应，构建相似度矩阵，计算每个粒子群权重，贪心算法或者匹配，重新计算当前目标位置。</p><p>具体步骤如下：</p><ol><li><p>计算历史轨迹与当前检测响应的相似度</p><script type="math/tex; mode=display">S(tr, d) = g(tr, d)[c_{tr}(d) + \alpha \sum_{p\in tr}^Np_N(d-p)]</script><p>tr表示轨迹， d表示检测。所以相似度包含3方面：$c_{tr}(d)$表示在线学习的分类器结果；粒子与检测的匹配度采用高斯密度函数度量，$p\in tr$表示轨迹对应的粒子群， $p_N(d-p)$表示高斯相似度；与检测尺寸大小相关的阈值函数$g(tr,d)$表示轨迹与轨迹尺度上的契合程度， $\alpha$用于控制粒子群的松紧度。</p><p>计算出匹配亲和度矩阵之后，可以采用二部图匹配的Hungarian算法计算匹配结果。不过作 者采用了近似的贪心匹配算法，即首先找到亲和度最大的那个匹配，然后删除这个亲和度， 寻找下一个匹配，依次类推。贪心匹配算法复杂度是线性，大部分情况下，也能得到最优匹 配结果。</p></li><li><p>采用粒子滤波框架，更新每条轨迹的粒子权重。</p><script type="math/tex; mode=display">w(tr, p) = \beta\cdot I(tr)\cdot p_N(p-d^*) + \gamma \cdot d_c(p)\cdot P_o(tr) + \eta\cdot c_{tr}(p)</script><p>$I(tr)$是示性函数,表示轨迹在该帧中是否关联成功，成功的话，则计算粒子在关联检测为中心的高斯概率；$d_c(p)$是粒子的检测可信度， $p_o(tr)$是权重函数</p><script type="math/tex; mode=display">p_o(tr)=\begin{cases}1, \quad\quad\quad&\text{if} \quad\quad I(tr)=1,\\\max_{tr':I(tr')=1}p_N(tr-tr') \quad\quad &\text{elif}\quad \exists ~~I(tr')=1 \\0 &\text{otherwise}\end{cases}</script><p>表示当轨迹匹配的时候权重为1， 否则去附近成功匹配轨迹的最大高斯密度，若周围没有成功匹配，则权重为0.</p><p>结合检测可信度的粒子滤波算法对轨迹的初始化采用了感兴趣区域的简单启发式策略。即， 进入图像区域边框时，初始化对象；当连续多帧没有关联到检测时终止跟踪。在一些典型数 据集上，基于检测可信度的粒子滤波算法可以得到不错的结果。</p></li></ol><h4 id="基于最小代价流的跟踪算法"><a href="#基于最小代价流的跟踪算法" class="headerlink" title="基于最小代价流的跟踪算法"></a>基于最小代价流的跟踪算法</h4><p>该算法是一种确定性优化离线方法，其目标函数是从已知的检测结合中找出最优的轨迹集合</p><script type="math/tex; mode=display">T^* =arg\max_T P(T|D) = arg\max_T\prod_iP(d_i|T)P(T)</script><p>表示成对数形式</p><script type="math/tex; mode=display">T^* = arg\min_T\sum_{T_k\in T}(-\log P(T_k) + \sum_i-\log P(d_i|T_k))</script><p>第一项刻画轨迹存在的概率， 第二项表示该轨迹存在前提下对应该检测的概率。</p><p>每条轨迹表示成链接的检测后，其能量可以表示为</p><script type="math/tex; mode=display">f^* = arg\min\sum_i C_{si}f_{si} + \sum_i C_if_i+\sum_{i,j}C_{ij}f_{ij} + \sum_iC_{it}f_{it}</script><p>其中$f_{si}, f_i, f_{ij}, f_{it}$均为示性变量分别表示是否是起点，是否在轨迹上，是否后续连接$j$节点，是否是终点。由于MOT任务中一一对应假设，所以节点$i$前后只存在一个连接，$f_{si} + \sum_j f_{ji} = f_i = \sum_{j}f_{ij}+f_{it}=1 ~\text{or}~0$</p><p>网络流优化问题满足最小代价流模型：</p><p><img src="/2019/05/20/MOT-overview-1st/mcn.png" alt="min-cost-flow"></p><p>边的代价函数计算规则：</p><script type="math/tex; mode=display">\begin{align}C_{si} &= -\log P_{enter}(x_i)\\C_{it} &= -\log P_{exit}(x_i)\\C_{ij} &= -\log P_{link}(x_j|x_i)\\C_i &= \log(\beta_i/(1-\beta_i))\end{align}</script><p>迹数目通过迭代比较的方法确定. 注意到检测节点代价Ci 的值是一个负数，所以轨迹对应的网络流代价也可能小于0。因此，通过遍历不同轨迹数目，可以确定一个全局 代价最小的解。这种方式效率较低。</p><h4 id="基于马尔科夫决策-MDP-的跟踪算法"><a href="#基于马尔科夫决策-MDP-的跟踪算法" class="headerlink" title="基于马尔科夫决策(MDP)的跟踪算法"></a>基于马尔科夫决策(MDP)的跟踪算法</h4><p>MDP是一种在线确定性推导方法。</p><p>该方法把目标跟踪看作是状态转移过程。 马尔科夫决策过程包含四个元素$(S, A, R, T)$, 分别表示状态集合、动作集合、状态转移集合和奖励函数集合。一个目标的跟踪过程包括如下决策过程：</p><ol><li>从Active状态转移到Tracked状态或者Inactivate状态，表示新出现的检测是否确实是轨迹。</li><li>从Tracked状态转移到Tracked状态或者Lost状态，表示跟踪继续或者终止。</li><li>从lost状态转移到Lost状态或者Inactivate状态或者Tracked状态， 即判断丢失的对象被重新跟踪或者终止或者继续处于跟丢状态。</li></ol><p><img src="/2019/05/20/MOT-overview-1st/MDP.png" alt="MDP"></p><p>三种决策过程分别对应着不同的奖励函数。</p><ol><li><script type="math/tex; mode=display">R_{activate}(s, a) = y(a)(w_{activate}^T\phi_{activate}(s)+b_{activate})</script><script type="math/tex; mode=display">y(a) = \begin{cases}1, &\text{if} \quad a=a_1\\-1, &\text{else} \quad a=a_2\end{cases}</script><p>这一步主要用来判断检测是否是目标。论文中使用离线训练的SVM实现该过程。检测样本的特征包括$(x,y, w, h, d_s)$</p></li><li><script type="math/tex; mode=display">R_{Tracked}(s, a)=\begin{cases}y(a) &\text{if}\quad e_{medFB}<e_0, \text{and} \quad o_{mean} > o_0\\-y(a) &\text(else)\end{cases}</script><p>$a=a_4$时， $y(a)=1$， 否则$y(a_3)=-1$, $e_{medFB}, o_{mean}$分别表示光流中心偏差和平均重合度， $e_0, o_0$分别表示阈值。</p></li><li><script type="math/tex; mode=display">R_{Lost}(s, a) = y(a)(\max_{k=1}^M(w^T\phi(t, d_k)+b))</script><p>计算$M$个匹配模板与检测的匹配程度，判断是否存在使得上述奖励最大且非负的匹配，当存在是选择$a=a_6$， 不存在是判断是否连续丢失次数超出阈值$T_{Lost}=50$，超出则$a=a_7$, 否则$a=a_6$. </p></li></ol><p>这部分可能理解起来存在些困难。其实需要弄明白的是，强化学习中首先根据当前的状态计算能量，然后根据能量来选择动作使得奖励最大。比如第一种奖励，首先计算等式右边第二项，根据其值大小来选择$a$是执行$a_1$还是$a_2$从而使$R_{active}$最大。</p><h4 id="基于局部流特征的近似在线多目标跟踪（NOMT）"><a href="#基于局部流特征的近似在线多目标跟踪（NOMT）" class="headerlink" title="基于局部流特征的近似在线多目标跟踪（NOMT）"></a>基于局部流特征的近似在线多目标跟踪（NOMT）</h4><p>这种方法称为近似在线多目标跟踪的原因在于跟踪$k$时刻目标时仅利用了之前的信息，但是该时刻依然允许对之前时刻跟踪结果进行修改。</p><p>其主要思想：对于当前帧$t$, 回看$\tau$帧，在这个时间间隔内构造若干轨迹片段, 利用这些轨迹片段跟之前的轨迹进行关联跟踪。轨迹片段包含了最新的$\tau$帧信息，因此如果该时间段内发生了匹配错误，依然允许进行修改。其基本思想如下图所示</p><p><img src="/2019/05/20/MOT-overview-1st/NOMT.png" alt="NOMT"></p><p>该部分的具体内容，请参考<a href="https://www.cnblogs.com/YiXiaoZhou/p/6798860.html" target="_blank" rel="noopener">博客园</a></p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>以上讨论了一些经典的传统多目标跟踪方法，这些方法虽然目前相对于基于深度的方法性能稍弱，但是作为基本的baseline，思想值得借鉴。</p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> overview </tag>
            
            <tag> traditional method </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Evaluation Multiple Object Tracking Performance-The CLEAR MOT Metrics</title>
      <link href="/2019/05/19/CLEAR/"/>
      <url>/2019/05/19/CLEAR/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>原发表于<a href="https://www.cnblogs.com/YiXiaoZhou/p/5937980.html" target="_blank" rel="noopener">CLEAR</a>， 参考之前笔记<a href="https://zongweizhou1.github.io/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/" target="_blank" rel="noopener">MOT16</a></p><p>多目标跟踪问题的评价指标应该能够评估三个方面：</p><ul><li>目标是否都及时的找到。</li><li>目标跟踪到的位置和真实位置一致性程度。</li><li>是否能够保持跟踪的一致性。</li></ul><p>另外作为metric的还需要具有的基本特点：</p><ul><li>参数（可调节的阈值等）需要尽可能的少，从而使评估简单直接，方法对比性较强。</li><li>尽可能直观，易解释。</li><li>应该具有较好的普适性，比如适应2D和3D的情况。</li><li>指标个数尽可能少，每个指标都具有较高表达能力。</li></ul><p>于是本文提出了CLEAR评估指标系统，用于评估MOT方法</p><h3 id="CLEAR"><a href="#CLEAR" class="headerlink" title="CLEAR"></a>CLEAR</h3><p>假设每一帧中目标为$\{o_1, o_2, …, o_n\}$, 跟踪假设为$\{h_1, h_2, …, h_m\}$, 检测结果为$\{d_1, d_2, …d_k\}$。跟踪假设和检测不同，跟踪假设是算法跟踪出来的结果。于是评价过程有以下基本步骤：</p><ol><li>建立假设和目标之间的对应关系。</li><li>对所有的对应关联关系，计算位置偏差，即一致性程度；</li><li>计算跟踪的累积结构误差：<ul><li>计算漏检数 FN</li><li>计算虚警数 FP</li><li>计算发生跳变数 IDs</li></ul></li></ol><p><strong>实现过程</strong></p><ol><li><p>确定依赖关系。通过假设与目标之间的距离判断是否关联，比如选择IOU作为相似度指标，然后给定阈值$\tau$， 采用匈牙利算法对匹配进行关联，之后将超出阈值的关联剔除，剩下的就是最终的依赖关系。如下图</p><p><img src="/2019/05/19/CLEAR/clear_corresponding.png" alt="corresponds"></p><p>注意MOT中这个匹配还不完全是这样的，MOT更注重一致性，所以假设第$t$帧有关联$o_i, h_j$ , 第$t+1$帧关联$(o_i, h_k) &lt; (o_i, h_j)&lt;\tau$这时候依然选择$(o_i, h_j)$. 如下图在$t+2$时刻，虽然$(o_1, h_2)$更匹配，但是为了连贯性，依然选择匹配$(o_1, h_1)$</p></li></ol><p>   <img src="/2019/05/19/CLEAR/IDKeep.png" alt="IDKeep"></p><ol><li><p>一致性刻画。追踪一致性能力就是指追踪器使追踪假设和对应目标长时间保持对应关系不变的能力。</p><p>一致性度量有两种方式可选。第一种是选择真实轨迹的一条最优匹配跟踪结果，然后其他的匹配都认为是错误匹配，记为IDs，如下图所示</p><p><img src="/2019/05/19/CLEAR/IDS.png" alt="IDS"></p><p>case1中认为红色的是成功匹配轨迹，于是错误数为2，同理case2中错误数为4.</p><p>但是这种方式有时候不符合常理，我们更应该关注的ID发生转变的那一刻，而不是跳变后带来的影响。比如case2中，两个跟踪轨迹都挺长，采用上一种方式就不合理。 所以文章采用了第二种方式，即只统计跳变的次数。具体而言，某一时刻的tid与之前最近的tid不同则认为发生了跳变。</p></li><li><p>指标计算过程</p><ul><li><p>计算第$t$帧中， $t-1$时刻存在的关联$(o_i, h_i)$关联是否依然有效。</p></li><li><p>对于不再有效的关联，在新的假设中寻找最有匹配，此时选择的策略是IOU最小。该过程中可以统计IDS发生的次数：若出现新的 $ (o_i, h_j)$不在历史匹配中，则认为发生一次跳变， 将$(o_i, h_j)$替代之前的$(o_i, h_i)$, 该时刻的ids记为$mmr_t$。</p></li><li><p>找到所有帧的关联后，统计所有匹配个数$c_t$, 计算每一个匹配的匹配距离$d_t^i$.</p></li><li><p>剩下未匹配的假设和目标分别记为虚警和漏检$m_t, fp_t$, 并用$g_t$表示当前时刻真正目标的个数。</p></li><li><p>从起始帧逐帧计算上述变量，最终统计下面的指标</p><script type="math/tex; mode=display">MOTP = \frac{\sum_{i,t}d_t^i}{\sum_tc_t}</script><p>即所有匹配的平均距离，该值使用IOU时取值越大越好，一般范围在$0.50\sim1$之间，因为$IOU&gt;0.50$的才认为是正确匹配。</p><script type="math/tex; mode=display">MOTA = 1-\frac{\sum_t(m_t+fp_t+mmr_t)}{\sum_tg_t}</script></li></ul></li></ol><pre><code> 其中$\frac{\sum_tm_t}{\sum_tg_T}, \frac{\sum_tfp_t}{\sum_tg_t}, \frac{\sum_tmmr_t}{\sum_tg_t}$分别表示总体的漏检率，虚警率和跳变几率。 MOTA的取值范围在$(-\infty, 1)$之间， 越大越好。</code></pre><p>   注意， MOTP，MOTA都是整体过程的均值，而不是每一帧均值之后的平均值，两个之间差别还是挺大的。如下图所示，一个8帧的数据，前面4帧都被漏检，第五帧开始只有第4个目标匹配成功，这样助阵计算漏检率的话， $(1 \times 4 + 0 \times 4)/8=0.5$, 而如果统计整个跟踪过程在计算漏检率为$\frac{4\times 4}{20}=0.8$。 显然采用整体过程更合适。</p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>论文在CLEARs workshops上使用提出的两个metric评价了不同的track system，发现</p><blockquote><p>the proposed metrics indeed reflect the strengths and weaknesses of the various used systems in an intuitive and meaningful way, allow for easy comparison of overall performance, and are applicable to a variety of scenarios.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> CLEAR </tag>
            
            <tag> Metric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Spatial-Temporal Relation Networks for Multi-Object Tracking</title>
      <link href="/2019/05/19/STRN/"/>
      <url>/2019/05/19/STRN/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Spatial-Temporal Relational Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-High-speed tracking with kernelized correlation filters</title>
      <link href="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/"/>
      <url>/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>​        KCF是一种鉴别式追踪方法，这类方法一般都是在追踪过程中训练一个目标检测器，使用目标检测器去检测下一帧预测位置是否是目标，然后再使用新检测结果去更新训练集进而更新目标检测器。而在训练目标检测器时一般选取目标区域为正样本，目标的周围区域为负样本，当然越靠近目标的区域分为正样本的概率越高。</p><p>​        本篇博文希望借这篇文章阐述KCF的原理和过程，以及存在的一些问题。</p><p>原发表于<a href="https://www.cnblogs.com/YiXiaoZhou/p/5925019.html" target="_blank" rel="noopener">博客园</a></p><hr><h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><p>​        论文中关于向量是行向量还是列向量总是指示不清楚，所以本文对变量符号统一之后进行推导，首先所有的小写字母均表示列向量，所有的大写字母表示矩阵，其中矩阵的每一行是一个样本，文中的函数除了$\phi(h)$是对行向量操作，其余都是对元素操做的，四则运算符号也都是针对元素操作的。还有所有对循环矩阵使用傅里叶变换时使用的生成向量都是循环矩阵的第一行向量，这点很重要。</p><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>使用目标周围区域的循环矩阵采集正负样本，利用脊回归训练目标检测器，并成功的利用循环矩阵在傅里叶空间可对角化的性质将矩阵的运算转化为向量的Hadamad积，即元素的点乘，大大降低了运算量，提高了运算速度，使算法满足实时性要求。</li><li>将线性空间的脊回归通过核函数映射到非线性空间，在非线性空间通过求解一个对偶问题和某些常见的约束，同样的可以使用循环矩阵傅里叶空间对角化简化计算。</li><li>给出了一种将多通道数据融入该算法的途径。</li></ul><hr><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="一维脊回归"><a href="#一维脊回归" class="headerlink" title="一维脊回归"></a>一维脊回归</h4><p>给定训练集$(x_i, y_i)， x_i\in R^{1\times n}, y_i\in R$, 其线性回归函数表示为$f(x_i) = w^Tx_i$, 回归系数$w\in R^{n\times 1}$ 可以通过最小二乘法计算：</p><script type="math/tex; mode=display">w^* = \min_w\sum_i(f(x_i)-y_i)^2 + \lambda \Vert w\Vert^2</script><p>$\lambda$是正则化系数， 限制模型的VC维从而保证模型泛化性能。</p><p>其矩阵形式表示为:</p><script type="math/tex; mode=display">w^* = \min_w \Vert Xw - y\Vert^2 +\lambda \Vert w\Vert^2</script><p>$X=[x_1, x_2, …,x_n]^T$. </p><p>采用最小二乘法求解，可得：</p><script type="math/tex; mode=display">w^* = (X^TX + \lambda I)^{-1}X^Ty</script><p>将模型扩展到复数空间，即：</p><script type="math/tex; mode=display">w^* = (X^HX+\lambda I)^{-1}X^Hy</script><p>$X^H$是$X$的复共轭矩阵。</p><h4 id="循环矩阵"><a href="#循环矩阵" class="headerlink" title="循环矩阵"></a>循环矩阵</h4><p>KCF中 训练样本是由目标样本经过循环位移得到。向量的循环可以使用排列矩阵获得, 下式中$P, Q$分别表示循环右移和左移矩阵：</p><script type="math/tex; mode=display">x_i = [x_{i1}, x_{i2},\dots x_{in}]^T\\P = \left[\begin{array}{ccccc}0, 0, \cdots, 0, 1\\1, 0, \cdots, 0, 0\\0, 1, \cdots, 0, 0\\...\\0, 0, \cdots, 1, 0\end{array}\right],~~~~Q= P^T\\Px_i = [x_{in}, x_{i1},\dots x_{i(n-1)}]^T\\x_iQ = [ x_{i2},\dots x_{in}, x_{i1}]^T</script><p>因此样本向量经过不断的左乘或者右乘排列矩阵，就可以实现循环位移，多个循环结果按序stack就成为循环矩阵$C(x)$如下图：</p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/1D xunhuan.png" alt="1D循环矩阵"></p><p>​                                                                        1D 循环矩阵示意    </p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/2D xunhuan.png" alt="2d xunhuan"></p><p>​                                                                        2D 图像循环示例</p><h4 id="循环矩阵傅氏空间对角化"><a href="#循环矩阵傅氏空间对角化" class="headerlink" title="循环矩阵傅氏空间对角化"></a>循环矩阵傅氏空间对角化</h4><p>所有循环矩阵$C(x)$都能够在傅氏空间中使用离散傅里叶变换进行对角化</p><script type="math/tex; mode=display">X = C(x) = F \cdot \text{diag}(\hat{x}) \cdot F^H</script><p>x是循环矩阵的生成向量，即对应于$C(x)$的第一行。 $\hat{x} = \mathcal{F}(x) =\sqrt{n}Fx$是$x$的傅里叶变换， $F$是傅里叶变换矩阵，常量：</p><script type="math/tex; mode=display">F=\frac{1}{\sqrt{n}}\left[\begin{array}{lllll}1,~~~~1,~~~\cdots,~~~~~~~1, ~~~~1\\1,~~~ w, ~~~\cdots, ~~~~~w^{n-2}, w^{n-1}\\~~~~~~~~~~~~\cdots \\1, w^{n-1}, \cdots, w^{(n-1)(n-2)}, w^{(n-1)^2}\end{array}\right]</script><p>关于矩阵傅里叶对角化请参考<a href="http://blog.csdn.net/shenxiaolu1984/article/details/50884830" target="_blank" rel="noopener">循环矩阵傅里叶对角化</a>., $F$是酉矩阵， 即$FF^H=F^HF=I$</p><h4 id="傅氏对角化简化的脊回归"><a href="#傅氏对角化简化的脊回归" class="headerlink" title="傅氏对角化简化的脊回归"></a>傅氏对角化简化的脊回归</h4><p>将$X=F\text{diag}(\hat{x})F^H$代入公式(4).</p><script type="math/tex; mode=display">\begin{align}w & = (F\cdot\text{diag}(\hat{x}^*)\cdot F^HF\cdot \text{diag}(\hat{x})\cdot F^H + \lambda F^HF)^{-1}F\cdot \text{diag}(\hat{x}^*)\cdot F^Hy \\&= (F\cdot\text{diag}(\hat{x}^*\odot\hat{x}+\lambda)\cdot F^H)^{-1}F\cdot\text{diag}(\hat{x}^*)F^Hy\\&= F\cdot \text{diag}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda})\cdot F^Hy\end{align}</script><p>有公式(6)可以进一步得到：</p><script type="math/tex; mode=display">\begin{align}w &= C(F^{-1}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda}))y \\\hat{w}&=\mathcal{F}^*(F^{-1}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda})) \odot \mathcal{F}(y)\\&= \frac{\hat{x}\odot \hat{y}}{\hat{x}^*\odot\hat{x}+\lambda}\\w &= \mathcal{F}^{-1}(\hat{w})\end{align}</script><p>这是因为$\mathcal{F}(C(x)y) = \mathcal{F}^*(x)\odot \mathcal{F}(y)$</p><p>于是可以使用向量电机代替矩阵运算，特别是牵涉到矩阵求逆运算。</p><h4 id="核空间的脊回归"><a href="#核空间的脊回归" class="headerlink" title="核空间的脊回归"></a>核空间的脊回归</h4><p>我们希望将样本数据映射到非线性空间内再进行回归，以期待新的空间内可分。于是新空间内回归函数为$f(x_i)=w^T\phi(x)$, 其中$\phi(\cdot)$是核函数。于是 $w = \min_w \Vert(\phi(X)w - y)\Vert^2 +\lambda \Vert w\Vert^2$</p><p>理想情形$w$是$\phi(X)w - y$零空间的向量，所以$w$可以由$\phi(X)=[\phi(x_1, x_2, \cdots, x_n)]^T$线性表示，于是$w=\sum_i \alpha_i\phi(x_i)=\phi(X)^T\alpha$, 于是</p><script type="math/tex; mode=display">\alpha = \min_a \Vert \phi(X)\phi(X)^T\alpha - y\Vert^2 + \lambda\Vert \phi(X)^T\alpha\Vert^2</script><p>该问题是$w$的对偶问题。</p><p>对$\alpha$计算导数并令其为0， 得到</p><script type="math/tex; mode=display">\begin{align}J(\alpha) &=  \alpha^T \phi(X)\phi(X)^T\phi(X)\phi(X)^T\alpha - 2y^T\phi(X)\phi(X)^T\alpha +C_{constant} + \lambda \alpha^T\phi(X)\phi(X)^T\alpha \\&= 2\phi(X)\phi(X)^T\phi(X)\phi(X)^T\alpha + 2\lambda \phi(X)\phi(X)^T\alpha - 2\phi(X)\phi(X)^Ty = 0\\\bar{\alpha} &= (\phi(X)\phi(X)^T+\lambda I)^{-1}y\end{align}</script><p>此处推导用到$\phi(X)\phi(X)^T$是协方差矩阵一定可逆的知识。</p><p>核方法中我们不知道核函数$\phi(\cdot)$的具体形式，但可以构造核矩阵$K=\phi(X)\phi(X)^T$, 于是$\bar{\alpha} = (K+\lambda I)^{-1}y$</p><script type="math/tex; mode=display">f(z) = w^T\phi(z) = \alpha^T\phi(X)\phi(z)</script><p>如果希望计算$\alpha $也可以利用傅氏空间对角化知识，那么$K$是一个循环位移矩阵。</p><blockquote><p><strong>Theorem 1.</strong> Given circulant data $C(x)$, the corresponding kernel matrix $K$ is circulatant if the kernel function satisfies $K(x, x’) = K(Mx, Mx’)$,for any permutation matrix $M$.<br>即核矩阵是循环矩阵应该满足两个条件：第一个样本和第二个样本都是由生成样本循环移位产生的，可以不是由同一个样本生成；满足$K(x, x’) = K(Mx, Mx’)$,其中$M$是排列矩阵。</p></blockquote><p>证明： 设$x\in R^n$, 则其生成向量$x’ = P^ix, \forall x’\in C(x)$, 于是：</p><script type="math/tex; mode=display">\begin{align}K_{ij} &= \phi(x_i)^T\phi(x_j)\\&= K(x_i, x_j)\\&= K(P^ix, P^jx) \\&= K(P^{-i}P^ix, P^{-i}P^{j}x), ~~\text{cause}~~ K(x, x')=K(Mx, Mx')\\&= K(x, P^{j-i}x) \\&= \phi(x)^T\phi(P^{j-i}x) \\&= \phi(x)^T\phi(x_{j-i})\end{align}</script><p>从而</p><script type="math/tex; mode=display">K_{0\cdot} = [\phi(x_0)^T\phi(x_0), \phi(x_0)^T\phi(x_1), ..., \phi(x_0)^T\phi(x_{n-1})]\\K_{1\cdot} = [\phi(x_0)^T\phi(x_{n-1}), \phi(x_0)^T\phi(x_0), ..., \phi(x_0)^T\phi(x_{n-2})]</script><p>这里，$j-i$表示位移数目， 负数表示左移数目。</p><p>因此可以证明Theorem1。</p><p>所以，只要选取核函数满足$K(x,x’)=K(Mx, Mx’)$， 那么核矩阵就是循环矩阵。当核矩阵是循环矩阵时，</p><script type="math/tex; mode=display">\begin{align}\alpha &= F\cdot \text{diag}(\hat{K}^{xx}+\lambda)^-1\cdot F y\\\hat{\alpha} &= \frac{\hat{y}}{(\hat{K}^{xx}+\lambda)^*}\\&= \frac{\hat{y}}{(\hat{K}^{xx}+\lambda)}\end{align}</script><p>其中$K^{xx}=\phi(x)^T\phi(X)^T$是核矩阵的生成向量，也是矩阵的第一行。</p><p><strong>注意，核函数一般满足对称性，即$K(x,y)=K(y,x)$, 于是可以证明$K^{xx}$是对称向量，即$K^{xx}_{0i} = K^{xx}_{0(n-i)}$, 对称向量的傅里叶变换为实数，因此其共轭可以忽略</strong></p><p>满足上述性质的核函数包括：</p><ul><li>Radial Basis Function kernels -e.g. Gaussian</li><li>Dot-Product kernels -e.g. linear, polynomial</li><li>Additive kernels - e.g. intersection, $\chi^2$ and Hellinger kernels</li><li>Exponentiated additive kernels.</li></ul><h4 id="快速检测"><a href="#快速检测" class="headerlink" title="快速检测"></a>快速检测</h4><p>首先由训练样本和标签训练检测器，其中训练集是由目标区域和由其移位得到的若干样本组成，对应的标签是根据距离越近正样本可能性越大的准则赋值的，于是得到训练样本集和对应的标签，采用上一节的核空间回归，可以计算得到$\alpha$</p><p>在检测样本时，认为待检测样本是target位移得到的，即$z_j = P^jz$, 于是其对应的回归值为$f(z_j) = \alpha^T\phi(X)\phi(z_j)$, 回归值最大则表明最可能是目标。</p><p>定义核矩阵$K^z= \phi(X)\phi(Z)^T$, 即 $K_{ij}^z = \phi(z_i)^T\phi(x_j)$, 由定理1可确定$K^z$是循环矩阵。</p><p> 于是在检测时，每个测试样本的回归值如下计算：</p><script type="math/tex; mode=display">\begin{align}f(z) &= (\alpha^T\phi(X)\phi(Z)^T)^T\\&= (K^z)^T\alpha\\&= F\cdot \text{diag}(\hat{x}^{xz})\cdot F^H\alpha\\\hat{f}(z) &= (\hat{K}^{xz})^*\hat{\alpha}\end{align}</script><p>$K^{zx}$是$K^z$的生成向量。</p><h4 id="核矩阵的快速计算"><a href="#核矩阵的快速计算" class="headerlink" title="核矩阵的快速计算"></a>核矩阵的快速计算</h4><p>目前计算瓶颈还有核矩阵的生成向量的计算。</p><h5 id="内积和多项式核"><a href="#内积和多项式核" class="headerlink" title="内积和多项式核"></a>内积和多项式核</h5><p>即$K(x_i, x_j’) = g(x_i^Tx_j’)$于是</p><script type="math/tex; mode=display">\begin{align}K^{xx'} &= g(C(x)x')^T \\&= g(\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}''))^T\end{align}</script><p>因此对于多项式核$K(x_i, x_j) = (x_i^Tx_j + a)^b$有</p><script type="math/tex; mode=display">K^{xx'} =((\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}''))^T+a)^b</script><h5 id="径向基核函数"><a href="#径向基核函数" class="headerlink" title="径向基核函数"></a>径向基核函数</h5><p>这类核函数重点在于计算$\Vert x_i-x_j\Vert^2$</p><p>$\Vert x_i-x_j\Vert^2 = \Vert x_i\Vert^2 + \Vert x_j\Vert^2 - 2 x_i^Tx_j$</p><p>所以</p><script type="math/tex; mode=display">K^{xx'} = h(\Vert x\Vert^2 + \Vert x' \Vert^2 -2\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}'))^T</script><p>对于高斯核有：</p><script type="math/tex; mode=display">K^{xx'} = exp(\frac{1}{\sigma^2}\Vert x\Vert^2 + \Vert x' \Vert^2 -2\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}'))^T</script><h4 id="1D-to-2D"><a href="#1D-to-2D" class="headerlink" title="1D to 2D"></a>1D to 2D</h4><p>2D数据时可以直接在核空间使用傅里叶变换对角化循环矩阵的特征，加速运算。</p><p>现有一个核函数$\varphi(\cdot)$, 自变量$X_i\in R^{m\times n}$, 于是$\varphi(X_i)\in R^k$, 于是核空间采用脊回归公式</p><script type="math/tex; mode=display">\alpha = (K+\lambda I)^{-1}y</script><blockquote><p>Theorem 2. The block matrix $K$ with elements $K_{(ii’)(jj’)} = \mathcal{K}(P^iXP^{i’}, P^jXP^{j’})$ is a Block-Circulant Matrix if $\mathcal{K}$ is a unitary invariant kernel.</p><p>Here, unitary invariant kernel satisfies $\mathcal{K}(X, X’) = \mathcal{K}(P^iXP^{i’}, P^iX’P^{i’})$</p></blockquote><p>块循环矩阵可以使用2D傅里叶变换实现矩阵对角化</p><script type="math/tex; mode=display">K = F_2\cdot \text{diag}(\hat{K}')\cdot F_2^H</script><p>其中$F_2$是2D傅里叶变换矩阵， $K’$是生成块循环矩阵的生成矩阵， $\hat{K}$表示对矩阵$K$进行2D傅里叶变换。</p><p>于是</p><script type="math/tex; mode=display">\alpha = F_2\cdot (\text{diag}(\hat{K}'+\lambda 1^m(1^n)^T))\cdot F_2^Hy</script><p>其中$1^m, 1^n$表示全1向量。</p><p>由公式6可得：</p><script type="math/tex; mode=display">\hat{\alpha}_M = (\hat{K}'+\lambda 1^m(1^n)^T)*\odot \hat{y}_M</script><p>$\alpha_M, y_M$分别表示矩阵形式的系数和标签。</p><p>于是回归值为</p><script type="math/tex; mode=display">\hat{f}_M = \hat{K}^{XZ}\odot \hat\alpha_M</script><p>其中$K^{XZ}$表示块循环矩阵的生成矩阵。</p><p>再往后的检测部分与1D类似，不作赘述。</p><h4 id="多通道问题"><a href="#多通道问题" class="headerlink" title="多通道问题"></a>多通道问题</h4><p>论文中在提取目标区域的特征时可以是灰度特征，但是使用Hog特征能够取得更好的效果，那么Hog特征该如何加入前面提到的模型呢？</p><p>Hog特征是将图像划分成较小的局部块，称为cell，在cell里提取梯度信息，绘制梯度方向直方图，然后为了减小光照影响，将几个cell的方向直方图串在一起进行block归一化，最终将所有的cell直方图串联起来就是图像的特征啦。</p><p>那么，按照传统的方式一张图像就提取出一个向量，但是这个向量怎么用啊？我们又不能通过该向量的移位来获得采样样本，因为，你想啊，把直方图的一个bin循环移位有什么意义啊？</p><p>所以论文中Hog特征的提取是将sample区域划分成若干的区域，然后再每个区域提取特征，代码中是在每个区域提取了32维特征，即划分9个梯度方向， 每个防线提取3个特征，然后再加上4个表观纹理特征和一个0元素表示阶段特征，详情参考<a href="http://www.cs.berkeley.edu/~rbg/latent/index.html" target="_blank" rel="noopener">FHOG</a>. 共 $3x9+5=32$维特征，于是可以使用$m\times n \times 32$表示图像，其中$m,n$是划分区域的网格数。那么这么操作之后矩阵为位移可以近似于patch块的位移，也就是网格的位移，于是矩阵的循环位移就是网格特征$m\times n \times 31$的每一通道上的循环位移，这里最后一维特征全0不考虑。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul><li><p>KCF相对于以前的单目标跟踪方法速度提升明显，性能也较好，思路简单。</p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/KCF example.png" alt="KCF example"></p><p>​                                                                            KCF example</p></li></ul><p>借上图来总结下KCF的过程，左图是刚开始我们使用红色虚线框框定了目标，然后红色实线框就是使用的padding了，其他的框就是将padding循环移位之后对齐目标得到的样本，由这些样本就可以训练出一个分类器，当分类器设计好之后，来到了下一帧图像，也就是右图，这时候我们首先在预测区域也就是红色实线框区域采样，然后对该采样进行循环移位，对齐目标后就像图中显示的那个样子 了，（这是为了理解，实际中不用对齐。。。），就是周围那些框框啦，使用分类器对这些框框计算响应，显然这时候白色框响应最大，因为他和之前一帧红色框一样，那我们通过白色框的相对移位就能推测目标的位移了。然后继续，再训练再检测。。。。</p><p>论文中还提到几点：</p><ol><li>对特征图像进行cosine window加权，这主要是为了减轻由于边界移位导致图像不光滑。</li><li>padding的size是目标框的2.5倍，肯定要使用padding窗口，要不然移位一次目标就被分解重组合了。。。效果能好哪去。。</li><li>对于标签使用了高斯加权。 这点尤其重要。</li><li>对$\alpha$前后帧结果进行了线性插值，为了让他长记性，不至于模型剧烈变化。</li></ol><ul><li>KCF的不足点<ol><li>依赖循环矩阵，对于多尺度的目标跟踪效果并不理想。当然可以通过设置多个size，在每个size上进行KCF运算，但这样的话很难确定应预先设置多少size，什么样的size，而且对size的遍历必将影响算法的速度。KCF最大的优势就是速度。</li><li>初始化矩阵不能自适应改变，其实这个问题和上一个缺点类似，这里强调的是非刚体运动，比如跳水运动员，刚开始选定区域肯定是个瘦长的矩形框，但当运动员开始屈体的时候显然这个预选定框就很大误差了。</li><li>难处理高速运动的目标， 高速运动目标很容易超出候选区域。</li><li>难处理低帧率中目标，这个和3类似，都是说相邻帧间目标位移过大。</li></ol></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> correlation filter </tag>
            
            <tag> kernelized correlation filter </tag>
            
            <tag> tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Frame-wise Motion and Appearance for Real-time Multiple Object Tracking</title>
      <link href="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/"/>
      <url>/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>文章认为MOT当前的主要挑战在于“不同帧中目标个数不定带来算法效率下降”。 </p><ul><li>LSTM 只处理了single object。(也有处理多目标的)</li><li><p>Re-ID exhausitively 匹配目标表观。（这个不客观，大多数方法都会通过先验条件约束范围）</p></li><li><p>单个box处理耗时严重，因为需要crop+resize+extract features</p></li></ul></li><li><p>文章针对该问题提出一种同时 关联不定数目目标的Deep Neural Network (DNN)</p><ul><li>Frame-wise Motion Fields (FMF) 估计目标运动位置，进行初步匹配。</li><li>Frame-wise Apearance Features(FAF)针对于FMF失败情形，采用表观再次匹配</li></ul></li><li><p>在MOT17 benchmark上保持SOTA性能同时，速度大幅提升。</p></li></ul><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>主要贡献点：</p><blockquote><p>1) Frame-wise Motion Fields (FMF) to represent the association among indefinite number of objects between frames.</p><p>2) Frame-wise Apearance Feature (FAF) to provide Re-ID features to assist FMF-based object association.</p><p>3) A simple yet effective inference algorithm to link the objects according to FMFs, and to fix a few uncertain associations using FAFs. </p><p>4)  Experiments on the challenging MOT17 benchmark show that our method achieves real-time MOT with competitive performance as the state-of-the-art approaches.</p></blockquote><hr><h3 id="Proposed-Method-FMA"><a href="#Proposed-Method-FMA" class="headerlink" title="Proposed Method (FMA)"></a>Proposed Method (FMA)</h3><p>FMA方法网络结构如下图：</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/architecture.png" alt="architecture"></p><h4 id="Frame-wise-Motion-Fields"><a href="#Frame-wise-Motion-Fields" class="headerlink" title="Frame-wise Motion Fields"></a>Frame-wise Motion Fields</h4><p>使用$I_1, I_2\in R^{w\times h\times 3}$分别表示时间先后的两帧图像， $b_i$表示第$i$个关联目标的bounding box， 两帧中$b_i$的$x,y$坐标分别表示为$\mathcal{X}_1(b_i), \mathcal{Y}_1(b_i), \mathcal{X}_2(b_i), \mathcal{Y}_2(b_i)$ , FMFs计算了两帧图像$I_1, I_2$之间的四个运动场信息：</p><script type="math/tex; mode=display">\begin{align}F_x^1(b_i) &= \mathcal{X}_2(b_i) - \mathcal{X}_1(b_i),\\F_y^1(b_i) &= \mathcal{Y}_2(b_i) - \mathcal(Y)_1(b_i)\\F_x^2(b_i) &= -F_x^1(b_i),\\F_y^2(b_i) &= -F_y^1(b_i)\end{align}</script><p>这里预测了目标的双向motion向量，主要是为了让motion向量更加鲁棒。两方面考虑： 1）前后两帧的表观和上下文信息可能差异较大，拆开考虑更鲁棒；2）可以解决部分遮挡问题，前后分别预测。示例如下图所示</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/FMFs.png" alt="FMFs"></p><p>注意上图(b)(d)中黑色十字标注的中心点.</p><p>损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_{MSE} = \sum_{k=1}^2\sum_{b_i\in B}\Vert H_x^k(b_i)-F_x^k(b_i)\Vert_F^2 +  \Vert H_y^k(b_i)-F_y^k(b_i)\Vert_F^2</script><p>$B$是关联的目标的bounding box的集合。</p><h4 id="Frame-wise-Appearance-Features"><a href="#Frame-wise-Appearance-Features" class="headerlink" title="Frame-wise Appearance Features"></a>Frame-wise Appearance Features</h4><p>FMF可以处理简单的情况，但是碰到拥挤或者干扰太多的时候就会预测失败，这时候希望通过FAF模块采用ReID实现匹配。</p><p>传统的ReID方法将每一个目标crop出来进行resize之后放到网络中抽取特征，耗时严重。该方法中利用FAF模块直接从整张图像中抽取每个目标的特征，同时FAF和FMF模块可以共享部分网络层。从而前向推理速度加快的同时也能够规避目标个数不同带来的问题。</p><p>给定同一个目标在两帧图像中的bounding boxes， FAF模块从FMF抽取的特征中crop patches，然后计算相似度。 在训练过程中同一个目标的不同特征concat一起作为正样本， 不同样本特征concat一起作为负样本。正负样本比例控制在$1:4$, 损失采用交叉熵损失</p><script type="math/tex; mode=display">\mathcal{L}_{BCE} -\frac{1}{N}\sum_{j=1}^N\mathcal{S}_j\log h(p_j) + (1-\mathcal{S_j})\log (1-h(p_j))</script><p>$p_j, \mathcal{S_j}$表示样本和对应label。</p><h4 id="Inference-Algorithm"><a href="#Inference-Algorithm" class="headerlink" title="Inference Algorithm"></a>Inference Algorithm</h4><p>$D=\{D_1, \cdots, D_N\}, T=\{T^1, \cdots, T^M\}$分别表示当前时刻的检测结果和已经存在的跟踪轨迹。 $IOU(\cdot)$表示IOU算子， $SIM(\cdot)$表示表观相似度。 Inference Algorithm分为3步：</p><ul><li>从former 到latter进行关联</li><li>从latter 到former进行关联</li><li>关联失败剩下的tracks和detections，利用FAFs进行关联。</li></ul><p>IOU和ReID的阈值分别为$\tau_1, \tau_2$</p><hr><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>Datasets: MOT17 benchmark</p><p>Setting: </p><p>​    训练样本对的选择： 两种时间间隔 每一帧和每4zhen</p><p>​    batchsize=4； lr=0.001, 70个epoch之后变为0.0001。</p><p>​    测试时样本对：连续两帧</p><p>​    $\tau_1=0.45, \tau_2=0.5$</p><h5 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h5><ol><li><p>MOT result of each component</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table2.png" alt="tab2"></p><p>训练的使用同时训练FAF和FMF模块，然后使用每个模块进行跟踪。</p><p>性能差别不是很大，但速度差异明显。 作者认为FAF需要对图像进行crop因此速度较慢。</p></li><li><p>Effectiveness of FMFs</p><p>根据FMFs, 利用IOU初步匹配效果如下：这张图分辨率太低，根本得不到什么结论。</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/FMFs_experiments.png" alt="FMFs"></p></li></ol><h5 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h5><ol><li><p>性能对比</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table3.png" alt="table3"></p><p>结论： a) 基于深度特征的方法性能优于传统特征； b）本文方法性能和MOTDT方法性能类似，但速度更快。</p><p>分析原因：a)，训练数据太少，只在MOT17训练集上训练，b)该方法比较依赖于准确的检测结果</p></li><li><p>检测器对比</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table4.png" alt="table4"></p><p>结论： 本文方法更适合于准确地检测器。</p></li></ol><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><blockquote><p>Practical and real-time MOT has to scale well with indefinite number of objects. This paper addresses this problem with frame-wise representations of object motion and appearance. In particular, FMFs simultaneously handle forward and backward motions of all bounding boxes in two input frames, which is the key to achieve real-time MOT inference. FAFs helps FMFs in handling some hard cases without significantly compromising the speed. The FMFs and FAFs are efficiently used in our inference algorithm, and achieved faster and more competitive results on the MOT17 benchmark. Our frame-wise representations are very efficient and general, making it possible to achieve real-time inference on more computationally expensive tracking tasks, such as instance segmentation tracking and scene mapping.</p></blockquote><hr><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><p>或许是因为preprint版本的原因，文章存在一些问题没有阐述明白。</p><ol><li>计算FMF的时候样本点的个数太少，过于稀疏是如何训练的？</li><li>训练细节没有提供多少epoch等</li><li>FAF进行crop时是从那一层crop的？</li><li>一般而言训练reid网络时，单独采用交叉熵损失似乎都不能取得较好效果。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Motion and Appearance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-MOT16:A Benchmark for Multi-Object Tracking</title>
      <link href="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/"/>
      <url>/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/</url>
      
        <content type="html"><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>这篇文章主要介绍了MOT的2016 benchmark库。相对于MOT15的benchmark而言，MOT16 benchmark视频数据标注更加规范严格，除了标注pedestrian之外，还标注了其他部分类别，同时给出了每个标注的可见度。新的benchmark数据也更加多样。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>MOT15 benchmark存在的不足：</p><ul><li>数据来源不同，标注协议也不完全相同。</li><li>测试集和训练集中人群密度分布不均衡。</li><li>部分训练集过于简单，不利用实际训练。如PETS09-S2L1.</li><li>benchmark中提供的检测结果太差，导致跟踪IDs较高。</li></ul></li><li><p>MOT16 benchmark的改进点：</p><ul><li>14段视频数据，分别取自crowed scenarios, different viewpoints, camera motions 和 weather conditions， 训练数据足够复杂。</li><li>所有的序列均采用相同的标注协议进行标注。</li><li>除了pedestrian之外还标注了一些其他类别，提供训练。</li></ul></li><li><p>MOT16 benchmark网站：</p><p>​    <a href="http://www.motchallenge.net" target="_blank" rel="noopener">MOTChallenge</a></p></li></ul><h4 id="MOT16-datasets"><a href="#MOT16-datasets" class="headerlink" title="MOT16 datasets"></a>MOT16 datasets</h4><ul><li><p>数据集overview</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/datasets_overview.png" alt="dataset overview"></p><p>MOT16 benchmark 数据集示意。上面一行是训练集，下面一行是测试集。</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/overview_table.png" alt="overview_table"></p></li><li><p>MOT16 benchmark 提供的检测结果</p><p>DPM检测结果。（另外，当前网站提供了FRCNN， SDP等多种检测结果）</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/detections statistics.png" alt="detection box statistics"></p><p><strong>detection 存储格式</strong></p><p>​    示例</p>   <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1, -1, 794.2, 47.5, 71.2, 174.8, 67.5, -1, -1</span><br><span class="line">1, -1, 164.1, 19.6, 66.5, 163.2, 29.4, -1, -1</span><br><span class="line">1, -1, 875.4, 39.9, 25.3, 145.0, 19.6, -1, -1</span><br><span class="line">2, -1, 781.7, 25.1, 69.2, 170.2, 58.1, -1, -1</span><br></pre></td></tr></table></figure><p><code>det.txt</code>文件的每一行存储一个box信息， 使用逗号隔开的9(实际是10)个数值表示。第一个数值表示box所在的帧号，第二个数值表示当前box对应的target id，对于detection而言，target id未知，都标注为-1；第三到六个值表示box的坐标，即<code>(topleft_x, topleft_y, width, height)</code>,<strong>标注从1开始</strong>；第七个值表示detections的置信度， 剩下三个值占位，全为 $-1$ 。</p></li></ul><h4 id="MOT16-annotation"><a href="#MOT16-annotation" class="headerlink" title="MOT16 annotation"></a>MOT16 annotation</h4><ul><li><p>target class</p><p>MOT16 benchmark标注可以分为三类</p><blockquote><p>(i) moving or standing pedestrians;<br>(ii) people that are not in an upright position or artificial representations of humans; and<br>(iii) vehicles and occluders.</p></blockquote><p>第一类的目标是希望跟踪的目标，包括所有的moving或者upright standing的行人，以及骑车或者滑板的也算。另外弯腰捡东西或者弯腰和孩童谈话的目标在评估时也会纳入考虑。</p><p>第二类目标没办法确切分类。比如非upright姿态的人，以及海报，倒影，镜像等，还用在玻璃之后的人都属于distractors。这类目标在评估时既不惩罚也不奖励，也就是说跟踪算法对这类数据的处理效果不影响最终评估性能。</p><p>第三类目标是各种车辆等遮挡物，这类目标在评估时也不考虑，标注的主要目的是提供额外信息用于估计遮挡程度等。</p></li><li><p>bounding box alignment</p><ul><li>box是目标的外接矩形框，所有的目标区域都在box内，所以对于行人而言，其宽度变化较剧烈。</li><li>对于遮挡目标，或者超出视野的目标，根据可见部分，估计整个目标的box。</li><li>如果遮挡的目标没法用一个box准确标注，那么可能用多个box标注。这主要针对于tree的大目标。</li><li>运输工具上的行人，只有充分可见时才会标注。比如汽车中的人不标注，自行车或者摩托上的人标注。</li></ul></li><li><p>start and end of trajectories</p><ul><li>轨迹尽可能的长。the annotation starts as early and ends as late as possible such that the accuracy is not forfeited</li><li>离开视野的目标再次出现时被赋予新的轨迹编号。</li></ul></li><li><p>minimal size 和 occlusions</p><ul><li>标注时对于size没有限制，多小的目标都会标注。</li><li>遮挡程度是计算出来的，不是显式指定的。</li><li>当目标被完全遮挡且不再可见时，认为该目标终止。</li><li>如果一个目标长时间遮挡后再次出现，但是位置变化很大，那么赋予新的轨迹id。</li></ul></li><li><p><code>gt.txt</code>存储格式</p><p>和<code>det.txt</code>类似，每一行存储10个数值表示当前box的跟踪结果。每个值的意义对应于下表</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/data_format.png" alt="data_format"></p><p>target所属类别编号如下：</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/class_map.png" alt="class map"></p></li></ul><h4 id="MOT16-evaluation"><a href="#MOT16-evaluation" class="headerlink" title="MOT16 evaluation"></a>MOT16 evaluation</h4><ul><li><p>tracker-to-target assignment</p><p>这部分主要包括指标FN, FP, FAF,用来评估检测目标和真实目标的匹配程度，<strong>注意，只是box的匹配程度，而没有考虑label是否一致</strong> </p><p>FN: False Negative,  所有的gt中没有匹配到检测的个数</p><p>FP: False Positive, 所有检测中没有匹配到gt的个数</p><p>FAF: the number of false alarms per frame, 也称为 FPPI: false positives per image</p></li><li><p>匹配的一致性</p><p>使用IDS指标，评估跟踪的一致性。IDS表示一条跟踪轨迹和真实轨迹在第$i$帧关联成功，但之前最近的时刻没有关联，则认为发生了一次ID switch。</p><p><strong>值得注意的是， 检测和gt是否关联取决于两者之间的距离，比如IoU等，而当多个IoU符合条件时选择最大的IoU关联。但是在MOT中为了尽可能保持轨迹跟踪的一致性，如果$t-1$时刻truth object $i$ 和假设$j$ 匹配，但是$t$时刻虽然匹配，但不是最优匹配，这时候我们仍然选择$i,j$匹配。</strong></p><p>为了让IDS与recovered targets无关，最终选择IDS/Recall度量一致性。</p></li><li><p>Distance measure</p><blockquote><p>the intersection over union (a.k.a. the Jaccard index) is usually employed as the similarity criterion, while the threshold td is set to 0.5 or 50%.</p></blockquote></li><li><p>Target-like annotations</p><p>计算evaluations之前，关联假设与gt的步骤：</p><blockquote><p>1) At each frame, all bounding boxes of the result file are matched to the ground truth via the Hungarian algorithm. (使用匈牙利算法根据IoU关联）<br>2) All result boxes that overlap &gt; 50% with one of these classes (distractor, static person, reflection, person on vehicle) are removed from the solution.（类似于NMS，先把distractor匹配的假设删除）<br>3) During the final evaluation, only those boxes that are annotated as pedestrians are used. (剩下的在计算性能)</p></blockquote><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/assignment_examples.png" alt="annotations_examples"></p></li><li><p><strong>Multiple Object Tracking Accuracy</strong></p><script type="math/tex; mode=display">MOTA = 1- \frac{\sum_t(FN_t+FP_t+IDS_t)}{\sum_t GT_t}</script><p>MOTA取值范围为$(-\infty, 100)$</p></li><li><p>Multiple Object Tracking Precision</p><script type="math/tex; mode=display">MOTP = \frac{\sum_{t,i} d_{t,i}}{\sum_{t}c_t}</script><p>MOTP取值范围$(50, 100)$。这里只计算匹配成功的平均匹配度，而匹配成功的阈值是$50$.</p></li><li><p>Track quality measures</p><ul><li>mostly tracked (MT): 至少$80\%$ 关联跟踪</li><li>mostly lost (ML): 至少$80\%$关联失败</li><li>partially tracked (PT): 少于$20\%$关联成功</li></ul><p><strong>注意我这里说关联，而没说跟踪是因为不论跟踪的trackid是否相同，只要gt匹配到假设就认为关联成功</strong></p><p>MT， ML最终计算的是相对于ground truth trajectories的总数。</p><ul><li>track fragmentation(FM): 表示gt trajectory中的某个时刻没有关联成功，但前面存在关联成功，后续也关联成功，则认为是一次Frag，如上图中(b),(d)示意。与IDS类似，FM最终计算为FM/Recall</li></ul></li></ul><h4 id="Baseline-methods"><a href="#Baseline-methods" class="headerlink" title="Baseline methods"></a>Baseline methods</h4><p>这部分提供的baseline都太陈旧，性能很容易超越，所以需要新的baseline。</p><ul><li>DP_NMS</li><li>CEM</li><li>SMOT</li><li>TBD</li><li>JPDA_M</li></ul><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><blockquote><p>We have presented a new challenging set of sequences within the MOTChallenge benchmark. The 2016 sequences contain 3 times more targets to be tracked when compared to the initial 2015 version. Furthermore, more accurate annotations were carried out following a strict protocol, and extra classes such as vehicles, sitting people, reflections or distractors were also annotated to provide further information to the community. We believe that the MOT16 release within the already established MOTChallenge benchmark provides a fairer comparison of state-of-the-art tracking methods, and challenges researchers to develop more generic methods that perform well in unconstrained environments and on unseen data. In the future, we plan to continue our workshops and challenges series, and also introduce various other (sub-)benchmarks for targeted applications, e.g. sport analysis, or biomedical cell tracking.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Benchmark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello world</title>
      <link href="/2019/05/17/hello-world/"/>
      <url>/2019/05/17/hello-world/</url>
      
        <content type="html"><![CDATA[<h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><p>使用Hexo + GithubPage创建个人网站总结与验证。</p><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ul><li><a href="https://www.simon96.online/2018/10/12/hexo-tutorial/" target="_blank" rel="noopener">最全Hexo博客搭建</a></li><li><a href="https://www.jianshu.com/p/8d28027fec76" target="_blank" rel="noopener">hexo+github上传图片到博客</a></li><li><a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo|Themes</a></li><li><a href="https://zongweizhou1.github.io/2019/05/17/hello-world/" target="_blank" rel="noopener">Hexo博客添加Latex</a></li></ul><h4 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h4><ol><li><p>创建新的md文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo n <span class="string">"new file"</span></span><br></pre></td></tr></table></figure></li><li><p>清除缓存</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure></li><li><p>打开本地服务，查看网页生成是否达到预期</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>然后浏览器输入<code>localhost:4000</code></p></li><li><p>上传到网站</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>此时便可登录网站查看生成的blog</p></li></ol><h4 id="主题选择"><a href="#主题选择" class="headerlink" title="主题选择"></a>主题选择</h4><p>主题的选择看个人需求，比如我博客内容更多的关于科研文章的解读，需要频繁的输入公式和贴图，论文的截图一般都是白色背景，所以我选择主题需要支持<code>mathjax</code>, 以及白色背景最好， 最终选择模板<a href="https://github.com/dongyuanxin/theme-bmw" target="_blank" rel="noopener">BMW</a></p><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><h5 id="测试公式是否能显示正常"><a href="#测试公式是否能显示正常" class="headerlink" title="测试公式是否能显示正常"></a>测试公式是否能显示正常</h5><p>行内公式： $y=\frac{\alpha^2+\sum_{i=1}^\beta\Phi(\hat{x}_i)}{\text{exp}(x^2)}$</p><p>行间公式：</p><script type="math/tex; mode=display">y=\begin{cases}x^2+z^2, \text{   if} x>z, \\(z-x)^2, \text{else}\end{cases}.</script><h5 id="测试图片是否能够正常显示"><a href="#测试图片是否能够正常显示" class="headerlink" title="测试图片是否能够正常显示"></a>测试图片是否能够正常显示</h5><p><img src="/2019/05/17/hello-world/girl.png" alt="girl"></p>]]></content>
      
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>About</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<p>Teis is a test</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Project</title>
      <link href="/project/index.html"/>
      <url>/project/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
