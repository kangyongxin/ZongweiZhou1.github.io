<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>阅读笔记-High-speed tracking with kernelized correlation filters</title>
      <link href="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/"/>
      <url>/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>​        KCF是一种鉴别式追踪方法，这类方法一般都是在追踪过程中训练一个目标检测器，使用目标检测器去检测下一帧预测位置是否是目标，然后再使用新检测结果去更新训练集进而更新目标检测器。而在训练目标检测器时一般选取目标区域为正样本，目标的周围区域为负样本，当然越靠近目标的区域分为正样本的概率越高。</p><p>​        本篇博文希望借这篇文章阐述KCF的原理和过程，以及存在的一些问题。</p><p>原发表于<a href="https://www.cnblogs.com/YiXiaoZhou/p/5925019.html" target="_blank" rel="noopener">博客园</a></p><hr><h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><p>​        论文中关于向量是行向量还是列向量总是指示不清楚，所以本文对变量符号统一之后进行推导，首先所有的小写字母均表示列向量，所有的大写字母表示矩阵，其中矩阵的每一行是一个样本，文中的函数除了$\phi(h)$是对行向量操作，其余都是对元素操做的，四则运算符号也都是针对元素操作的。还有所有对循环矩阵使用傅里叶变换时使用的生成向量都是循环矩阵的第一行向量，这点很重要。</p><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>使用目标周围区域的循环矩阵采集正负样本，利用脊回归训练目标检测器，并成功的利用循环矩阵在傅里叶空间可对角化的性质将矩阵的运算转化为向量的Hadamad积，即元素的点乘，大大降低了运算量，提高了运算速度，使算法满足实时性要求。</li><li>将线性空间的脊回归通过核函数映射到非线性空间，在非线性空间通过求解一个对偶问题和某些常见的约束，同样的可以使用循环矩阵傅里叶空间对角化简化计算。</li><li>给出了一种将多通道数据融入该算法的途径。</li></ul><hr><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="一维脊回归"><a href="#一维脊回归" class="headerlink" title="一维脊回归"></a>一维脊回归</h4><p>给定训练集$(x_i, y_i)， x_i\in R^{1\times n}, y_i\in R$, 其线性回归函数表示为$f(x_i) = w^Tx_i$, 回归系数$w\in R^{n\times 1}$ 可以通过最小二乘法计算：</p><script type="math/tex; mode=display">w^* = \min_w\sum_i(f(x_i)-y_i)^2 + \lambda \Vert w\Vert^2</script><p>$\lambda$是正则化系数， 限制模型的VC维从而保证模型泛化性能。</p><p>其矩阵形式表示为:</p><script type="math/tex; mode=display">w^* = \min_w \Vert Xw - y\Vert^2 +\lambda \Vert w\Vert^2</script><p>$X=[x_1, x_2, …,x_n]^T$. </p><p>采用最小二乘法求解，可得：</p><script type="math/tex; mode=display">w^* = (X^TX + \lambda I)^{-1}X^Ty</script><p>将模型扩展到复数空间，即：</p><script type="math/tex; mode=display">w^* = (X^HX+\lambda I)^{-1}X^Hy</script><p>$X^H$是$X$的复共轭矩阵。</p><h4 id="循环矩阵"><a href="#循环矩阵" class="headerlink" title="循环矩阵"></a>循环矩阵</h4><p>KCF中 训练样本是由目标样本经过循环位移得到。向量的循环可以使用排列矩阵获得, 下式中$P, Q$分别表示循环右移和左移矩阵：</p><script type="math/tex; mode=display">x_i = [x_{i1}, x_{i2},\dots x_{in}]^T\\P = \left[\begin{array}{ccccc}0, 0, \cdots, 0, 1\\1, 0, \cdots, 0, 0\\0, 1, \cdots, 0, 0\\...\\0, 0, \cdots, 1, 0\end{array}\right],~~~~Q= P^T\\Px_i = [x_{in}, x_{i1},\dots x_{i(n-1)}]^T\\x_iQ = [ x_{i2},\dots x_{in}, x_{i1}]^T</script><p>因此样本向量经过不断的左乘或者右乘排列矩阵，就可以实现循环位移，多个循环结果按序stack就成为循环矩阵$C(x)$如下图：</p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/1D xunhuan.png" alt="1D循环矩阵"></p><p>​                                                                        1D 循环矩阵示意    </p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/2D xunhuan.png" alt="2d xunhuan"></p><p>​                                                                        2D 图像循环示例</p><h4 id="循环矩阵傅氏空间对角化"><a href="#循环矩阵傅氏空间对角化" class="headerlink" title="循环矩阵傅氏空间对角化"></a>循环矩阵傅氏空间对角化</h4><p>所有循环矩阵$C(x)$都能够在傅氏空间中使用离散傅里叶变换进行对角化</p><script type="math/tex; mode=display">X = C(x) = F \cdot \text{diag}(\hat{x}) \cdot F^H</script><p>x是循环矩阵的生成向量，即对应于$C(x)$的第一行。 $\hat{x} = \mathcal{F}(x) =\sqrt{n}Fx$是$x$的傅里叶变换， $F$是傅里叶变换矩阵，常量：</p><script type="math/tex; mode=display">F=\frac{1}{\sqrt{n}}\left[\begin{array}{lllll}1,~~~~1,~~~\cdots,~~~~~~~1, ~~~~1\\1,~~~ w, ~~~\cdots, ~~~~~w^{n-2}, w^{n-1}\\~~~~~~~~~~~~\cdots \\1, w^{n-1}, \cdots, w^{(n-1)(n-2)}, w^{(n-1)^2}\end{array}\right]</script><p>关于矩阵傅里叶对角化请参考<a href="http://blog.csdn.net/shenxiaolu1984/article/details/50884830" target="_blank" rel="noopener">循环矩阵傅里叶对角化</a>., $F$是酉矩阵， 即$FF^H=F^HF=I$</p><h4 id="傅氏对角化简化的脊回归"><a href="#傅氏对角化简化的脊回归" class="headerlink" title="傅氏对角化简化的脊回归"></a>傅氏对角化简化的脊回归</h4><p>将$X=F\text{diag}(\hat{x})F^H$代入公式(4).</p><script type="math/tex; mode=display">\begin{align}w & = (F\cdot\text{diag}(\hat{x}^*)\cdot F^HF\cdot \text{diag}(\hat{x})\cdot F^H + \lambda F^HF)^{-1}F\cdot \text{diag}(\hat{x}^*)\cdot F^Hy \\&= (F\cdot\text{diag}(\hat{x}^*\odot\hat{x}+\lambda)\cdot F^H)^{-1}F\cdot\text{diag}(\hat{x}^*)F^Hy\\&= F\cdot \text{diag}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda})\cdot F^Hy\end{align}</script><p>有公式(6)可以进一步得到：</p><script type="math/tex; mode=display">\begin{align}w &= C(F^{-1}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda}))y \\\hat{w}&=\mathcal{F}^*(F^{-1}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda})) \odot \mathcal{F}(y)\\&= \frac{\hat{x}\odot \hat{y}}{\hat{x}^*\odot\hat{x}+\lambda}\\w &= \mathcal{F}^{-1}(\hat{w})\end{align}</script><p>这是因为$\mathcal{F}(C(x)y) = \mathcal{F}^*(x)\odot \mathcal{F}(y)$</p><p>于是可以使用向量电机代替矩阵运算，特别是牵涉到矩阵求逆运算。</p><h4 id="核空间的脊回归"><a href="#核空间的脊回归" class="headerlink" title="核空间的脊回归"></a>核空间的脊回归</h4><p>我们希望将样本数据映射到非线性空间内再进行回归，以期待新的空间内可分。于是新空间内回归函数为$f(x_i)=w^T\phi(x)$, 其中$\phi(\cdot)$是核函数。于是 $w = \min_w \Vert(\phi(X)w - y)\Vert^2 +\lambda \Vert w\Vert^2$</p><p>理想情形$w$是$\phi(X)w - y$零空间的向量，所以$w$可以由$\phi(X)=[\phi(x_1, x_2, \cdots, x_n)]^T$线性表示，于是$w=\sum_i \alpha_i\phi(x_i)=\phi(X)^T\alpha$, 于是</p><script type="math/tex; mode=display">\alpha = \min_a \Vert \phi(X)\phi(X)^T\alpha - y\Vert^2 + \lambda\Vert \phi(X)^T\alpha\Vert^2</script><p>该问题是$w$的对偶问题。</p><p>对$\alpha$计算导数并令其为0， 得到</p><script type="math/tex; mode=display">\begin{align}J(\alpha) &=  \alpha^T \phi(X)\phi(X)^T\phi(X)\phi(X)^T\alpha - 2y^T\phi(X)\phi(X)^T\alpha +C_{constant} + \lambda \alpha^T\phi(X)\phi(X)^T\alpha \\&= 2\phi(X)\phi(X)^T\phi(X)\phi(X)^T\alpha + 2\lambda \phi(X)\phi(X)^T\alpha - 2\phi(X)\phi(X)^Ty = 0\\\bar{\alpha} &= (\phi(X)\phi(X)^T+\lambda I)^{-1}y\end{align}</script><p>此处推导用到$\phi(X)\phi(X)^T$是协方差矩阵一定可逆的知识。</p><p>核方法中我们不知道核函数$\phi(\cdot)$的具体形式，但可以构造核矩阵$K=\phi(X)\phi(X)^T$, 于是$\bar{\alpha} = (K+\lambda I)^{-1}y$</p><script type="math/tex; mode=display">f(z) = w^T\phi(z) = \alpha^T\phi(X)\phi(z)</script><p>如果希望计算$\alpha $也可以利用傅氏空间对角化知识，那么$K$是一个循环位移矩阵。</p><blockquote><p><strong>Theorem 1.</strong> Given circulant data $C(x)$, the corresponding kernel matrix $K$ is circulatant if the kernel function satisfies $K(x, x’) = K(Mx, Mx’)$,for any permutation matrix $M$.<br>即核矩阵是循环矩阵应该满足两个条件：第一个样本和第二个样本都是由生成样本循环移位产生的，可以不是由同一个样本生成；满足$K(x, x’) = K(Mx, Mx’)$,其中$M$是排列矩阵。</p></blockquote><p>证明： 设$x\in R^n$, 则其生成向量$x’ = P^ix, \forall x’\in C(x)$, 于是：</p><script type="math/tex; mode=display">\begin{align}K_{ij} &= \phi(x_i)^T\phi(x_j)\\&= K(x_i, x_j)\\&= K(P^ix, P^jx) \\&= K(P^{-i}P^ix, P^{-i}P^{j}x), ~~\text{cause}~~ K(x, x')=K(Mx, Mx')\\&= K(x, P^{j-i}x) \\&= \phi(x)^T\phi(P^{j-i}x) \\&= \phi(x)^T\phi(x_{j-i})\end{align}</script><p>从而</p><script type="math/tex; mode=display">K_{0\cdot} = [\phi(x_0)^T\phi(x_0), \phi(x_0)^T\phi(x_1), ..., \phi(x_0)^T\phi(x_{n-1})]\\K_{1\cdot} = [\phi(x_0)^T\phi(x_{n-1}), \phi(x_0)^T\phi(x_0), ..., \phi(x_0)^T\phi(x_{n-2})]</script><p>这里，$j-i$表示位移数目， 负数表示左移数目。</p><p>因此可以证明Theorem1。</p><p>所以，只要选取核函数满足$K(x,x’)=K(Mx, Mx’)$， 那么核矩阵就是循环矩阵。当核矩阵是循环矩阵时，</p><script type="math/tex; mode=display">\begin{align}\alpha &= F\cdot \text{diag}(\hat{K}^{xx}+\lambda)^-1\cdot F y\\\hat{\alpha} &= \frac{\hat{y}}{(\hat{K}^{xx}+\lambda)^*}\\&= \frac{\hat{y}}{(\hat{K}^{xx}+\lambda)}\end{align}</script><p>其中$K^{xx}=\phi(x)^T\phi(X)^T$是核矩阵的生成向量，也是矩阵的第一行。</p><p><strong>注意，核函数一般满足对称性，即$K(x,y)=K(y,x)$, 于是可以证明$K^{xx}$是对称向量，即$K^{xx}_{0i} = K^{xx}_{0(n-i)}$, 对称向量的傅里叶变换为实数，因此其共轭可以忽略</strong></p><p>满足上述性质的核函数包括：</p><ul><li>Radial Basis Function kernels -e.g. Gaussian</li><li>Dot-Product kernels -e.g. linear, polynomial</li><li>Additive kernels - e.g. intersection, $\chi^2$ and Hellinger kernels</li><li>Exponentiated additive kernels.</li></ul><h4 id="快速检测"><a href="#快速检测" class="headerlink" title="快速检测"></a>快速检测</h4><p>首先由训练样本和标签训练检测器，其中训练集是由目标区域和由其移位得到的若干样本组成，对应的标签是根据距离越近正样本可能性越大的准则赋值的，于是得到训练样本集和对应的标签，采用上一节的核空间回归，可以计算得到$\alpha$</p><p>在检测样本时，认为待检测样本是target位移得到的，即$z_j = P^jz$, 于是其对应的回归值为$f(z_j) = \alpha^T\phi(X)\phi(z_j)$, 回归值最大则表明最可能是目标。</p><p>定义核矩阵$K^z= \phi(X)\phi(Z)^T$, 即 $K_{ij}^z = \phi(z_i)^T\phi(x_j)$, 由定理1可确定$K^z$是循环矩阵。</p><p> 于是在检测时，每个测试样本的回归值如下计算：</p><script type="math/tex; mode=display">\begin{align}f(z) &= (\alpha^T\phi(X)\phi(Z)^T)^T\\&= (K^z)^T\alpha\\&= F\cdot \text{diag}(\hat{x}^{xz})\cdot F^H\alpha\\\hat{f}(z) &= (\hat{K}^{xz})^*\hat{\alpha}\end{align}</script><p>$K^{zx}$是$K^z$的生成向量。</p><h4 id="核矩阵的快速计算"><a href="#核矩阵的快速计算" class="headerlink" title="核矩阵的快速计算"></a>核矩阵的快速计算</h4><p>目前计算瓶颈还有核矩阵的生成向量的计算。</p><h5 id="内积和多项式核"><a href="#内积和多项式核" class="headerlink" title="内积和多项式核"></a>内积和多项式核</h5><p>即$K(x_i, x_j’) = g(x_i^Tx_j’)$于是</p><script type="math/tex; mode=display">\begin{align}K^{xx'} &= g(C(x)x')^T \\&= g(\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}''))^T\end{align}</script><p>因此对于多项式核$K(x_i, x_j) = (x_i^Tx_j + a)^b$有</p><script type="math/tex; mode=display">K^{xx'} =((\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}''))^T+a)^b</script><h5 id="径向基核函数"><a href="#径向基核函数" class="headerlink" title="径向基核函数"></a>径向基核函数</h5><p>这类核函数重点在于计算$\Vert x_i-x_j\Vert^2$</p><p>$\Vert x_i-x_j\Vert^2 = \Vert x_i\Vert^2 + \Vert x_j\Vert^2 - 2 x_i^Tx_j$</p><p>所以</p><script type="math/tex; mode=display">K^{xx'} = h(\Vert x\Vert^2 + \Vert x' \Vert^2 -2\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}'))^T</script><p>对于高斯核有：</p><script type="math/tex; mode=display">K^{xx'} = exp(\frac{1}{\sigma^2}\Vert x\Vert^2 + \Vert x' \Vert^2 -2\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}'))^T</script><h4 id="1D-to-2D"><a href="#1D-to-2D" class="headerlink" title="1D to 2D"></a>1D to 2D</h4><p>2D数据时可以直接在核空间使用傅里叶变换对角化循环矩阵的特征，加速运算。</p><p>现有一个核函数$\varphi(\cdot)$, 自变量$X_i\in R^{m\times n}$, 于是$\varphi(X_i)\in R^k$, 于是核空间采用脊回归公式</p><script type="math/tex; mode=display">\alpha = (K+\lambda I)^{-1}y</script><blockquote><p>Theorem 2. The block matrix $K$ with elements $K_{(ii’)(jj’)} = \mathcal{K}(P^iXP^{i’}, P^jXP^{j’})$ is a Block-Circulant Matrix if $\mathcal{K}$ is a unitary invariant kernel.</p><p>Here, unitary invariant kernel satisfies $\mathcal{K}(X, X’) = \mathcal{K}(P^iXP^{i’}, P^iX’P^{i’})$</p></blockquote><p>块循环矩阵可以使用2D傅里叶变换实现矩阵对角化</p><script type="math/tex; mode=display">K = F_2\cdot \text{diag}(\hat{K}')\cdot F_2^H</script><p>其中$F_2$是2D傅里叶变换矩阵， $K’$是生成块循环矩阵的生成矩阵， $\hat{K}$表示对矩阵$K$进行2D傅里叶变换。</p><p>于是</p><script type="math/tex; mode=display">\alpha = F_2\cdot (\text{diag}(\hat{K}'+\lambda 1^m(1^n)^T))\cdot F_2^Hy</script><p>其中$1^m, 1^n$表示全1向量。</p><p>由公式6可得：</p><script type="math/tex; mode=display">\hat{\alpha}_M = (\hat{K}'+\lambda 1^m(1^n)^T)*\odot \hat{y}_M</script><p>$\alpha_M, y_M$分别表示矩阵形式的系数和标签。</p><p>于是回归值为</p><script type="math/tex; mode=display">\hat{f}_M = \hat{K}^{XZ}\odot \hat\alpha_M</script><p>其中$K^{XZ}$表示块循环矩阵的生成矩阵。</p><p>再往后的检测部分与1D类似，不作赘述。</p><h4 id="多通道问题"><a href="#多通道问题" class="headerlink" title="多通道问题"></a>多通道问题</h4><p>论文中在提取目标区域的特征时可以是灰度特征，但是使用Hog特征能够取得更好的效果，那么Hog特征该如何加入前面提到的模型呢？</p><p>Hog特征是将图像划分成较小的局部块，称为cell，在cell里提取梯度信息，绘制梯度方向直方图，然后为了减小光照影响，将几个cell的方向直方图串在一起进行block归一化，最终将所有的cell直方图串联起来就是图像的特征啦。</p><p>那么，按照传统的方式一张图像就提取出一个向量，但是这个向量怎么用啊？我们又不能通过该向量的移位来获得采样样本，因为，你想啊，把直方图的一个bin循环移位有什么意义啊？</p><p>所以论文中Hog特征的提取是将sample区域划分成若干的区域，然后再每个区域提取特征，代码中是在每个区域提取了32维特征，即划分9个梯度方向， 每个防线提取3个特征，然后再加上4个表观纹理特征和一个0元素表示阶段特征，详情参考<a href="http://www.cs.berkeley.edu/~rbg/latent/index.html" target="_blank" rel="noopener">FHOG</a>. 共 $3x9+5=32$维特征，于是可以使用$m\times n \times 32$表示图像，其中$m,n$是划分区域的网格数。那么这么操作之后矩阵为位移可以近似于patch块的位移，也就是网格的位移，于是矩阵的循环位移就是网格特征$m\times n \times 31$的每一通道上的循环位移，这里最后一维特征全0不考虑。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul><li><p>KCF相对于以前的单目标跟踪方法速度提升明显，性能也较好，思路简单。</p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/KCF example.png" alt="KCF example"></p><p>​                                                                            KCF example</p></li></ul><p>借上图来总结下KCF的过程，左图是刚开始我们使用红色虚线框框定了目标，然后红色实线框就是使用的padding了，其他的框就是将padding循环移位之后对齐目标得到的样本，由这些样本就可以训练出一个分类器，当分类器设计好之后，来到了下一帧图像，也就是右图，这时候我们首先在预测区域也就是红色实线框区域采样，然后对该采样进行循环移位，对齐目标后就像图中显示的那个样子 了，（这是为了理解，实际中不用对齐。。。），就是周围那些框框啦，使用分类器对这些框框计算响应，显然这时候白色框响应最大，因为他和之前一帧红色框一样，那我们通过白色框的相对移位就能推测目标的位移了。然后继续，再训练再检测。。。。</p><p>论文中还提到几点：</p><ol><li>对特征图像进行cosine window加权，这主要是为了减轻由于边界移位导致图像不光滑。</li><li>padding的size是目标框的2.5倍，肯定要使用padding窗口，要不然移位一次目标就被分解重组合了。。。效果能好哪去。。</li><li>对于标签使用了高斯加权。 这点尤其重要。</li><li>对$\alpha$前后帧结果进行了线性插值，为了让他长记性，不至于模型剧烈变化。</li></ol><ul><li>KCF的不足点<ol><li>依赖循环矩阵，对于多尺度的目标跟踪效果并不理想。当然可以通过设置多个size，在每个size上进行KCF运算，但这样的话很难确定应预先设置多少size，什么样的size，而且对size的遍历必将影响算法的速度。KCF最大的优势就是速度。</li><li>初始化矩阵不能自适应改变，其实这个问题和上一个缺点类似，这里强调的是非刚体运动，比如跳水运动员，刚开始选定区域肯定是个瘦长的矩形框，但当运动员开始屈体的时候显然这个预选定框就很大误差了。</li><li>难处理高速运动的目标， 高速运动目标很容易超出候选区域。</li><li>难处理低帧率中目标，这个和3类似，都是说相邻帧间目标位移过大。</li></ol></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> correlation filter </tag>
            
            <tag> kernelized correlation filter </tag>
            
            <tag> tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Frame-wise Motion and Appearance for Real-time Multiple Object Tracking</title>
      <link href="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/"/>
      <url>/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>文章认为MOT当前的主要挑战在于“不同帧中目标个数不定带来算法效率下降”。 </p><ul><li>LSTM 只处理了single object。(也有处理多目标的)</li><li><p>Re-ID exhausitively 匹配目标表观。（这个不客观，大多数方法都会通过先验条件约束范围）</p></li><li><p>单个box处理耗时严重，因为需要crop+resize+extract features</p></li></ul></li><li><p>文章针对该问题提出一种同时 关联不定数目目标的Deep Neural Network (DNN)</p><ul><li>Frame-wise Motion Fields (FMF) 估计目标运动位置，进行初步匹配。</li><li>Frame-wise Apearance Features(FAF)针对于FMF失败情形，采用表观再次匹配</li></ul></li><li><p>在MOT17 benchmark上保持SOTA性能同时，速度大幅提升。</p></li></ul><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>主要贡献点：</p><blockquote><p>1) Frame-wise Motion Fields (FMF) to represent the association among indefinite number of objects between frames.</p><p>2) Frame-wise Apearance Feature (FAF) to provide Re-ID features to assist FMF-based object association.</p><p>3) A simple yet effective inference algorithm to link the objects according to FMFs, and to fix a few uncertain associations using FAFs. </p><p>4)  Experiments on the challenging MOT17 benchmark show that our method achieves real-time MOT with competitive performance as the state-of-the-art approaches.</p></blockquote><hr><h3 id="Proposed-Method-FMA"><a href="#Proposed-Method-FMA" class="headerlink" title="Proposed Method (FMA)"></a>Proposed Method (FMA)</h3><p>FMA方法网络结构如下图：</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/architecture.png" alt="architecture"></p><h4 id="Frame-wise-Motion-Fields"><a href="#Frame-wise-Motion-Fields" class="headerlink" title="Frame-wise Motion Fields"></a>Frame-wise Motion Fields</h4><p>使用$I_1, I_2\in R^{w\times h\times 3}$分别表示时间先后的两帧图像， $b_i$表示第$i$个关联目标的bounding box， 两帧中$b_i$的$x,y$坐标分别表示为$\mathcal{X}_1(b_i), \mathcal{Y}_1(b_i), \mathcal{X}_2(b_i), \mathcal{Y}_2(b_i)$ , FMFs计算了两帧图像$I_1, I_2$之间的四个运动场信息：</p><script type="math/tex; mode=display">\begin{align}F_x^1(b_i) &= \mathcal{X}_2(b_i) - \mathcal{X}_1(b_i),\\F_y^1(b_i) &= \mathcal{Y}_2(b_i) - \mathcal(Y)_1(b_i)\\F_x^2(b_i) &= -F_x^1(b_i),\\F_y^2(b_i) &= -F_y^1(b_i)\end{align}</script><p>这里预测了目标的双向motion向量，主要是为了让motion向量更加鲁棒。两方面考虑： 1）前后两帧的表观和上下文信息可能差异较大，拆开考虑更鲁棒；2）可以解决部分遮挡问题，前后分别预测。示例如下图所示</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/FMFs.png" alt="FMFs"></p><p>注意上图(b)(d)中黑色十字标注的中心点.</p><p>损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_{MSE} = \sum_{k=1}^2\sum_{b_i\in B}\Vert H_x^k(b_i)-F_x^k(b_i)\Vert_F^2 +  \Vert H_y^k(b_i)-F_y^k(b_i)\Vert_F^2</script><p>$B$是关联的目标的bounding box的集合。</p><h4 id="Frame-wise-Appearance-Features"><a href="#Frame-wise-Appearance-Features" class="headerlink" title="Frame-wise Appearance Features"></a>Frame-wise Appearance Features</h4><p>FMF可以处理简单的情况，但是碰到拥挤或者干扰太多的时候就会预测失败，这时候希望通过FAF模块采用ReID实现匹配。</p><p>传统的ReID方法将每一个目标crop出来进行resize之后放到网络中抽取特征，耗时严重。该方法中利用FAF模块直接从整张图像中抽取每个目标的特征，同时FAF和FMF模块可以共享部分网络层。从而前向推理速度加快的同时也能够规避目标个数不同带来的问题。</p><p>给定同一个目标在两帧图像中的bounding boxes， FAF模块从FMF抽取的特征中crop patches，然后计算相似度。 在训练过程中同一个目标的不同特征concat一起作为正样本， 不同样本特征concat一起作为负样本。正负样本比例控制在$1:4$, 损失采用交叉熵损失</p><script type="math/tex; mode=display">\mathcal{L}_{BCE} -\frac{1}{N}\sum_{j=1}^N\mathcal{S}_j\log h(p_j) + (1-\mathcal{S_j})\log (1-h(p_j))</script><p>$p_j, \mathcal{S_j}$表示样本和对应label。</p><h4 id="Inference-Algorithm"><a href="#Inference-Algorithm" class="headerlink" title="Inference Algorithm"></a>Inference Algorithm</h4><p>$D=\{D_1, \cdots, D_N\}, T=\{T^1, \cdots, T^M\}$分别表示当前时刻的检测结果和已经存在的跟踪轨迹。 $IOU(\cdot)$表示IOU算子， $SIM(\cdot)$表示表观相似度。 Inference Algorithm分为3步：</p><ul><li>从former 到latter进行关联</li><li>从latter 到former进行关联</li><li>关联失败剩下的tracks和detections，利用FAFs进行关联。</li></ul><p>IOU和ReID的阈值分别为$\tau_1, \tau_2$</p><hr><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>Datasets: MOT17 benchmark</p><p>Setting: </p><p>​    训练样本对的选择： 两种时间间隔 每一帧和每4zhen</p><p>​    batchsize=4； lr=0.001, 70个epoch之后变为0.0001。</p><p>​    测试时样本对：连续两帧</p><p>​    $\tau_1=0.45, \tau_2=0.5$</p><h5 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h5><ol><li><p>MOT result of each component</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table2.png" alt="tab2"></p><p>训练的使用同时训练FAF和FMF模块，然后使用每个模块进行跟踪。</p><p>性能差别不是很大，但速度差异明显。 作者认为FAF需要对图像进行crop因此速度较慢。</p></li><li><p>Effectiveness of FMFs</p><p>根据FMFs, 利用IOU初步匹配效果如下：这张图分辨率太低，根本得不到什么结论。</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/FMFs_experiments.png" alt="FMFs"></p></li></ol><h5 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h5><ol><li><p>性能对比</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table3.png" alt="table3"></p><p>结论： a) 基于深度特征的方法性能优于传统特征； b）本文方法性能和MOTDT方法性能类似，但速度更快。</p><p>分析原因：a)，训练数据太少，只在MOT17训练集上训练，b)该方法比较依赖于准确的检测结果</p></li><li><p>检测器对比</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table4.png" alt="table4"></p><p>结论： 本文方法更适合于准确地检测器。</p></li></ol><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><blockquote><p>Practical and real-time MOT has to scale well with indefinite number of objects. This paper addresses this problem with frame-wise representations of object motion and appearance. In particular, FMFs simultaneously handle forward and backward motions of all bounding boxes in two input frames, which is the key to achieve real-time MOT inference. FAFs helps FMFs in handling some hard cases without significantly compromising the speed. The FMFs and FAFs are efficiently used in our inference algorithm, and achieved faster and more competitive results on the MOT17 benchmark. Our frame-wise representations are very efficient and general, making it possible to achieve real-time inference on more computationally expensive tracking tasks, such as instance segmentation tracking and scene mapping.</p></blockquote><hr><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><p>或许是因为preprint版本的原因，文章存在一些问题没有阐述明白。</p><ol><li>计算FMF的时候样本点的个数太少，过于稀疏是如何训练的？</li><li>训练细节没有提供多少epoch等</li><li>FAF进行crop时是从那一层crop的？</li><li>一般而言训练reid网络时，单独采用交叉熵损失似乎都不能取得较好效果。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Motion and Appearance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-MOT16:A Benchmark for Multi-Object Tracking</title>
      <link href="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/"/>
      <url>/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/</url>
      
        <content type="html"><![CDATA[<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>这篇文章主要介绍了MOT的2016 benchmark库。相对于MOT15的benchmark而言，MOT16 benchmark视频数据标注更加规范严格，除了标注pedestrian之外，还标注了其他部分类别，同时给出了每个标注的可见度。新的benchmark数据也更加多样。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>MOT15 benchmark存在的不足：</p><ul><li>数据来源不同，标注协议也不完全相同。</li><li>测试集和训练集中人群密度分布不均衡。</li><li>部分训练集过于简单，不利用实际训练。如PETS09-S2L1.</li><li>benchmark中提供的检测结果太差，导致跟踪IDs较高。</li></ul></li><li><p>MOT16 benchmark的改进点：</p><ul><li>14段视频数据，分别取自crowed scenarios, different viewpoints, camera motions 和 weather conditions， 训练数据足够复杂。</li><li>所有的序列均采用相同的标注协议进行标注。</li><li>除了pedestrian之外还标注了一些其他类别，提供训练。</li></ul></li><li><p>MOT16 benchmark网站：</p><p>​    <a href="http://www.motchallenge.net" target="_blank" rel="noopener">MOTChallenge</a></p></li></ul><h4 id="MOT16-datasets"><a href="#MOT16-datasets" class="headerlink" title="MOT16 datasets"></a>MOT16 datasets</h4><ul><li><p>数据集overview</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/datasets_overview.png" alt="dataset overview"></p><p>MOT16 benchmark 数据集示意。上面一行是训练集，下面一行是测试集。</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/overview_table.png" alt="overview_table"></p></li><li><p>MOT16 benchmark 提供的检测结果</p><p>DPM检测结果。（另外，当前网站提供了FRCNN， SDP等多种检测结果）</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/detections statistics.png" alt="detection box statistics"></p><p><strong>detection 存储格式</strong></p><p>​    示例</p>   <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1, -1, 794.2, 47.5, 71.2, 174.8, 67.5, -1, -1</span><br><span class="line">1, -1, 164.1, 19.6, 66.5, 163.2, 29.4, -1, -1</span><br><span class="line">1, -1, 875.4, 39.9, 25.3, 145.0, 19.6, -1, -1</span><br><span class="line">2, -1, 781.7, 25.1, 69.2, 170.2, 58.1, -1, -1</span><br></pre></td></tr></table></figure><p><code>det.txt</code>文件的每一行存储一个box信息， 使用逗号隔开的9(实际是10)个数值表示。第一个数值表示box所在的帧号，第二个数值表示当前box对应的target id，对于detection而言，target id未知，都标注为-1；第三到六个值表示box的坐标，即<code>(topleft_x, topleft_y, width, height)</code>,<strong>标注从1开始</strong>；第七个值表示detections的置信度， 剩下三个值占位，全为 $-1$ 。</p></li></ul><h4 id="MOT16-annotation"><a href="#MOT16-annotation" class="headerlink" title="MOT16 annotation"></a>MOT16 annotation</h4><ul><li><p>target class</p><p>MOT16 benchmark标注可以分为三类</p><blockquote><p>(i) moving or standing pedestrians;<br>(ii) people that are not in an upright position or artificial representations of humans; and<br>(iii) vehicles and occluders.</p></blockquote><p>第一类的目标是希望跟踪的目标，包括所有的moving或者upright standing的行人，以及骑车或者滑板的也算。另外弯腰捡东西或者弯腰和孩童谈话的目标在评估时也会纳入考虑。</p><p>第二类目标没办法确切分类。比如非upright姿态的人，以及海报，倒影，镜像等，还用在玻璃之后的人都属于distractors。这类目标在评估时既不惩罚也不奖励，也就是说跟踪算法对这类数据的处理效果不影响最终评估性能。</p><p>第三类目标是各种车辆等遮挡物，这类目标在评估时也不考虑，标注的主要目的是提供额外信息用于估计遮挡程度等。</p></li><li><p>bounding box alignment</p><ul><li>box是目标的外接矩形框，所有的目标区域都在box内，所以对于行人而言，其宽度变化较剧烈。</li><li>对于遮挡目标，或者超出视野的目标，根据可见部分，估计整个目标的box。</li><li>如果遮挡的目标没法用一个box准确标注，那么可能用多个box标注。这主要针对于tree的大目标。</li><li>运输工具上的行人，只有充分可见时才会标注。比如汽车中的人不标注，自行车或者摩托上的人标注。</li></ul></li><li><p>start and end of trajectories</p><ul><li>轨迹尽可能的长。the annotation starts as early and ends as late as possible such that the accuracy is not forfeited</li><li>离开视野的目标再次出现时被赋予新的轨迹编号。</li></ul></li><li><p>minimal size 和 occlusions</p><ul><li>标注时对于size没有限制，多小的目标都会标注。</li><li>遮挡程度是计算出来的，不是显式指定的。</li><li>当目标被完全遮挡且不再可见时，认为该目标终止。</li><li>如果一个目标长时间遮挡后再次出现，但是位置变化很大，那么赋予新的轨迹id。</li></ul></li><li><p><code>gt.txt</code>存储格式</p><p>和<code>det.txt</code>类似，每一行存储10个数值表示当前box的跟踪结果。每个值的意义对应于下表</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/data_format.png" alt="data_format"></p><p>target所属类别编号如下：</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/class_map.png" alt="class map"></p></li></ul><h4 id="MOT16-evaluation"><a href="#MOT16-evaluation" class="headerlink" title="MOT16 evaluation"></a>MOT16 evaluation</h4><ul><li><p>tracker-to-target assignment</p><p>这部分主要包括指标FN, FP, FAF,用来评估检测目标和真实目标的匹配程度，<strong>注意，只是box的匹配程度，而没有考虑label是否一致</strong> </p><p>FN: False Negative,  所有的gt中没有匹配到检测的个数</p><p>FP: False Positive, 所有检测中没有匹配到gt的个数</p><p>FAF: the number of false alarms per frame, 也称为 FPPI: false positives per image</p></li><li><p>匹配的一致性</p><p>使用IDS指标，评估跟踪的一致性。IDS表示一条跟踪轨迹和真实轨迹在第$i$帧关联成功，但之前最近的时刻没有关联，则认为发生了一次ID switch。</p><p><strong>值得注意的是， 检测和gt是否关联取决于两者之间的距离，比如IoU等，而当多个IoU符合条件时选择最大的IoU关联。但是在MOT中为了尽可能保持轨迹跟踪的一致性，如果$t-1$时刻truth object $i$ 和假设$j$ 匹配，但是$t$时刻虽然匹配，但不是最优匹配，这时候我们仍然选择$i,j$匹配。</strong></p><p>为了让IDS与recovered targets无关，最终选择IDS/Recall度量一致性。</p></li><li><p>Distance measure</p><blockquote><p>the intersection over union (a.k.a. the Jaccard index) is usually employed as the similarity criterion, while the threshold td is set to 0.5 or 50%.</p></blockquote></li><li><p>Target-like annotations</p><p>计算evaluations之前，关联假设与gt的步骤：</p><blockquote><p>1) At each frame, all bounding boxes of the result file are matched to the ground truth via the Hungarian algorithm. (使用匈牙利算法根据IoU关联）<br>2) All result boxes that overlap &gt; 50% with one of these classes (distractor, static person, reflection, person on vehicle) are removed from the solution.（类似于NMS，先把distractor匹配的假设删除）<br>3) During the final evaluation, only those boxes that are annotated as pedestrians are used. (剩下的在计算性能)</p></blockquote><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/assignment_examples.png" alt="annotations_examples"></p></li><li><p><strong>Multiple Object Tracking Accuracy</strong></p><script type="math/tex; mode=display">MOTA = 1- \frac{\sum_t(FN_t+FP_t+IDS_t)}{\sum_t GT_t}</script><p>MOTA取值范围为$(-\infty, 100)$</p></li><li><p>Multiple Object Tracking Precision</p><script type="math/tex; mode=display">MOTP = \frac{\sum_{t,i} d_{t,i}}{\sum_{t}c_t}</script><p>MOTP取值范围$(50, 100)$。这里只计算匹配成功的平均匹配度，而匹配成功的阈值是$50$.</p></li><li><p>Track quality measures</p><ul><li>mostly tracked (MT): 至少$80\%$ 关联跟踪</li><li>mostly lost (ML): 至少$80\%$关联失败</li><li>partially tracked (PT): 少于$20\%$关联成功</li></ul><p><strong>注意我这里说关联，而没说跟踪是因为不论跟踪的trackid是否相同，只要gt匹配到假设就认为关联成功</strong></p><p>MT， ML最终计算的是相对于ground truth trajectories的总数。</p><ul><li>track fragmentation(FM): 表示gt trajectory中的某个时刻没有关联成功，但前面存在关联成功，后续也关联成功，则认为是一次Frag，如上图中(b),(d)示意。与IDS类似，FM最终计算为FM/Recall</li></ul></li></ul><h4 id="Baseline-methods"><a href="#Baseline-methods" class="headerlink" title="Baseline methods"></a>Baseline methods</h4><p>这部分提供的baseline都太陈旧，性能很容易超越，所以需要新的baseline。</p><ul><li>DP_NMS</li><li>CEM</li><li>SMOT</li><li>TBD</li><li>JPDA_M</li></ul><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><blockquote><p>We have presented a new challenging set of sequences within the MOTChallenge benchmark. The 2016 sequences contain 3 times more targets to be tracked when compared to the initial 2015 version. Furthermore, more accurate annotations were carried out following a strict protocol, and extra classes such as vehicles, sitting people, reflections or distractors were also annotated to provide further information to the community. We believe that the MOT16 release within the already established MOTChallenge benchmark provides a fairer comparison of state-of-the-art tracking methods, and challenges researchers to develop more generic methods that perform well in unconstrained environments and on unseen data. In the future, we plan to continue our workshops and challenges series, and also introduce various other (sub-)benchmarks for targeted applications, e.g. sport analysis, or biomedical cell tracking.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Benchmark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello world</title>
      <link href="/2019/05/17/hello-world/"/>
      <url>/2019/05/17/hello-world/</url>
      
        <content type="html"><![CDATA[<h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><p>使用Hexo + GithubPage创建个人网站总结与验证。</p><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ul><li><a href="https://www.simon96.online/2018/10/12/hexo-tutorial/" target="_blank" rel="noopener">最全Hexo博客搭建</a></li><li><a href="https://www.jianshu.com/p/8d28027fec76" target="_blank" rel="noopener">hexo+github上传图片到博客</a></li><li><a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo|Themes</a></li><li><a href="https://zongweizhou1.github.io/2019/05/17/hello-world/" target="_blank" rel="noopener">Hexo博客添加Latex</a></li></ul><h4 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h4><ol><li><p>创建新的md文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo n <span class="string">"new file"</span></span><br></pre></td></tr></table></figure></li><li><p>清除缓存</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure></li><li><p>打开本地服务，查看网页生成是否达到预期</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>然后浏览器输入<code>localhost:4000</code></p></li><li><p>上传到网站</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>此时便可登录网站查看生成的blog</p></li></ol><h4 id="主题选择"><a href="#主题选择" class="headerlink" title="主题选择"></a>主题选择</h4><p>主题的选择看个人需求，比如我博客内容更多的关于科研文章的解读，需要频繁的输入公式和贴图，论文的截图一般都是白色背景，所以我选择主题需要支持<code>mathjax</code>, 以及白色背景最好， 最终选择模板<a href="https://github.com/dongyuanxin/theme-bmw" target="_blank" rel="noopener">BMW</a></p><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><h5 id="测试公式是否能显示正常"><a href="#测试公式是否能显示正常" class="headerlink" title="测试公式是否能显示正常"></a>测试公式是否能显示正常</h5><p>行内公式： $y=\frac{\alpha^2+\sum_{i=1}^\beta\Phi(\hat{x}_i)}{\text{exp}(x^2)}$</p><p>行间公式：</p><script type="math/tex; mode=display">y=\begin{cases}x^2+z^2, \text{   if} x>z, \\(z-x)^2, \text{else}\end{cases}.</script><h5 id="测试图片是否能够正常显示"><a href="#测试图片是否能够正常显示" class="headerlink" title="测试图片是否能够正常显示"></a>测试图片是否能够正常显示</h5><p><img src="/2019/05/17/hello-world/girl.png" alt="girl"></p>]]></content>
      
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>Project</title>
      <link href="/project/index.html"/>
      <url>/project/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>About</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<p>Teis is a test</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
