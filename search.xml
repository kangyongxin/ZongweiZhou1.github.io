<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>工具使用-pycharm+github push/pull/branch/merge操作</title>
      <link href="/2019/05/24/pycharm-github/"/>
      <url>/2019/05/24/pycharm-github/</url>
      
        <content type="html"><![CDATA[<p>一直记不住使用git上传下载github代码的指令，不过幸好现在使用pycharm管理项目代码，pycharm对guihub的版本控制我觉得还是挺适合我的。所以这里记录下如何使用pycharm + github进行项目的版本控制。</p><a id="more"></a><h3 id="PyCharm-添加-GitHub账户"><a href="#PyCharm-添加-GitHub账户" class="headerlink" title="PyCharm 添加 GitHub账户"></a>PyCharm 添加 GitHub账户</h3><ul><li><p>为了捋顺整条线，我们先使用pycharm创建新的项目，这里我的项目名称是<code>test_pg</code></p></li><li><p>File -&gt; settings -&gt; Version Control -&gt; Github 添加GitHub账号密码并测试连接是否成功。</p><p><img src="/2019/05/24/pycharm-github/setting.png" alt="setting"></p><p>在setting窗口设置版本控制version Control中GitHub的连接账号。按下图步骤依次填写。</p><p>login: 填写自己github的用户名</p><p>password: 自己GitHub的登陆密码。</p><p><img src="/2019/05/24/pycharm-github/github login.png" alt="github login"></p><p>然后第5步点击test按钮，测试与github账号的通信是否成功，成功如下图所示。</p><p><img src="/2019/05/24/pycharm-github/github login test.png" alt="github login test"></p></li></ul><h3 id="上传本地项目"><a href="#上传本地项目" class="headerlink" title="上传本地项目"></a>上传本地项目</h3><ul><li>将本地pycharm里的project分享到github上，并且在自己的github上创建一个对应的repository</li></ul><p><img src="/2019/05/24/pycharm-github/local to project.png" alt="local to github"></p><p>点击分享（Share Project on GitHub）之后 出现如下界面， 可以选择是否作为私有项目。然后输入对该项目的简短描述。</p><p><img src="/2019/05/24/pycharm-github/share.png" alt="share"></p><p>此时弹出<code>Add Files For Initial Commit</code>窗口，选择第一次要分享到github的文件，点击OK</p><p><img src="/2019/05/24/pycharm-github/init commit.png" alt="init commit"></p><p>分享成功的话，右下脚弹出窗口：</p><p><img src="/2019/05/24/pycharm-github/succefully shared.png" alt="share successfully"></p><p>此时github上就能够找到当前的repository</p><p><img src="/2019/05/24/pycharm-github/github examplar.png" alt="github examplar"></p><hr><h3 id="更新代码"><a href="#更新代码" class="headerlink" title="更新代码"></a>更新代码</h3><p>在本地修改，删除，或者添加代码之后，需要更新到Github对应的项目中， 比如我们这里新创建一个<code>README.md</code> 文档。</p><p><img src="/2019/05/24/pycharm-github/readme md.png" alt="readme md"></p><p>可以发现此时由于文档尚未加入到带分享目录中， 标题颜色是红色。右键文件，按如下方式add文件，文件名变为绿色。</p><p><img src="/2019/05/24/pycharm-github/add readme.png" alt="add readme"></p><p>然后再点击add上面的<code>commit File...</code> 选项。选择要commit的文件，填写commit内容。然后点击<code>commit</code> 这里其实是<code>Commit and Push</code>，即顺便将其push到github对应的repository上.</p><p><img src="/2019/05/24/pycharm-github/commit readme.png" alt="commit readme"></p><p>commit成功的话左下角<code>Version Control</code>窗口会显示提示：</p><p><img src="/2019/05/24/pycharm-github/commit result.png" alt="commit result"></p><p>上面是新创建的文件push到github上，对于修改的文件可以发现文件标题由绿色变为蓝色，这时候可以通过对比查看文件修改了什么</p><p><img src="/2019/05/24/pycharm-github/modify readme.png" alt="modify readme"></p><p><img src="/2019/05/24/pycharm-github/compare modify.png" alt="compare modify"></p><p><img src="/2019/05/24/pycharm-github/modify content.png" alt="modify content"></p><p>此时如果需要上传的话，先进行commit，然后再push</p><p><img src="/2019/05/24/pycharm-github/modify commit.png" alt="modify commit"></p><p><img src="/2019/05/24/pycharm-github/commit content.png" alt="commit content"></p><p><img src="/2019/05/24/pycharm-github/push modify.png" alt="push modify"></p><p><img src="/2019/05/24/pycharm-github/push successful.png" alt="push successful"></p><p>此时github中的repository出现对应的修改。</p><p><img src="/2019/05/24/pycharm-github/modify successful.png" alt="modify successful"></p><hr><h3 id="将github上的修改拉回本地"><a href="#将github上的修改拉回本地" class="headerlink" title="将github上的修改拉回本地"></a>将github上的修改拉回本地</h3><p><img src="/2019/05/24/pycharm-github/modify from github.png" alt="modify from github"></p><p><img src="/2019/05/24/pycharm-github/modify from github commit.png" alt="modify from github commit"></p><p>​    然后可以发现github上的readme文件多了我们添加的一句话。现在将修改pull到本地。</p><p>​    <img src="/2019/05/24/pycharm-github/pull github.png" alt="pull github"></p><p><img src="/2019/05/24/pycharm-github/pull info.png" alt="pull info"></p><p>成功后可以发现本地的readme文件也发生了对应的修改。</p><hr><h3 id="在远端创建新分支"><a href="#在远端创建新分支" class="headerlink" title="在远端创建新分支"></a>在远端创建新分支</h3><p>在上面所有操作中，我们都是在默认的master分支上进行操作的，又是我们需要创建新的分支，在新的分支上验证了修改代码的正确性，然后再merge到master上。</p><ul><li>点击IDE右下角 Git: master按钮， 选择 <code>New Branch</code></li></ul><p><img src="/2019/05/24/pycharm-github/new branch.png" alt="new branch"></p><ul><li><p>输入分支名称， 点击OK</p><p><img src="/2019/05/24/pycharm-github/branch name.png" alt="branch name"></p></li><li><p>现在再打开git:master 可以发现local 分支多了个branch分支， branch分支和master分支一模一样。</p><p><img src="/2019/05/24/pycharm-github/view branch.png" alt="new branch"></p></li><li><p>删除本地master分支，然后再branch分支上进行操作.这是可以发现local分支列表中只有branch分支了。</p></li></ul><p><img src="/2019/05/24/pycharm-github/delete master.png" alt="delete master"></p><ul><li><p>在branch分支中修改readme 内容，然后push到github上。</p><p><img src="/2019/05/24/pycharm-github/push branch.png" alt="push branch"></p><p>注意这里是由branch-&gt; origin:branch, 之前是master-&gt;origin:master</p><p>此时可以发现</p><p><img src="/2019/05/24/pycharm-github/github branch.png" alt="github branch"></p><hr><h3 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h3><p>后面用到merge的时候再详细补充。</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> pycharm </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Online Multi-Target Tracking Using Recurrent Neural Networks</title>
      <link href="/2019/05/23/MOT-RNN/"/>
      <url>/2019/05/23/MOT-RNN/</url>
      
        <content type="html"><![CDATA[<p>本文第一次提出利用深度网络端到端的实现多目标跟踪。</p><p><img src="/2019/05/23/MOT-RNN/architecture.png" alt="architecture"></p><p>真实环境中处理多目标跟踪任务具有一些难点。首先轨迹的个数不定，轨迹的起点和重点不定； 其次轨迹中目标的状态是连续变量，比如位置，尺寸，置信度等，最后一般解决多目标跟踪任务最终转化为组合优化问题，而组合优化问题是离散空间求解问题。</p><p>这篇文章和之前的两篇笔记一样，都是利用网络去解决<strong>数据关联</strong>问题。</p><p>项目地址： <a href="https://bitbucket.org/amilan/rnntracking" target="_blank" rel="noopener">https://bitbucket.org/amilan/rnntracking</a></p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>本文第一次提出利用深度网络端到端的实现多目标跟踪。</p><p><img src="/2019/05/23/MOT-RNN/architecture.png" alt="architecture"></p><p>真实环境中处理多目标跟踪任务具有一些难点。首先轨迹的个数不定，轨迹的起点和重点不定； 其次轨迹中目标的状态是连续变量，比如位置，尺寸，置信度等，最后一般解决多目标跟踪任务最终转化为组合优化问题，而组合优化问题是离散空间求解问题。</p><p>这篇文章和之前的两篇笔记一样，都是利用网络去解决<strong>数据关联</strong>问题。</p><p>项目地址： <a href="https://bitbucket.org/amilan/rnntracking" target="_blank" rel="noopener">https://bitbucket.org/amilan/rnntracking</a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="为什么不能直接将用于NLP的RNN用在MOT任务上？"><a href="#为什么不能直接将用于NLP的RNN用在MOT任务上？" class="headerlink" title="为什么不能直接将用于NLP的RNN用在MOT任务上？"></a>为什么不能直接将用于NLP的RNN用在MOT任务上？</h4><ul><li>NLP中每次预测的是一个词或者字符，但是MOT任务中每次预测需要预测多个目标，即状态空间是多维的。</li><li>NLP中预测的单词或者词汇可以用one-hot等方法转化为离散变量，但是MOT任务中目标状态既包括离散值又包括连续值。比如连续的位置，大小等，而数据关联则是离散空间内的求解。</li><li>每一个时刻目标个数不同就导致输出个数随着时间而变化。</li></ul><h4 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h4><p>RNN主要用于处理序列化数据， 每个时刻的输入包含当前时刻的数据以及前一时刻的状态。一般而言，每一时刻的处理可以使用多层网络实现， 在$t$时刻的多层隐状态可以使用$h_t^l, l=0, …, L$表示， $h_t^0$表示当前时刻的输入， $h_t^L$表示最终用于生成输出值$y_t$的embedding 特征。每一层形式化表示为$h_t^l = \tanh W^l(h_t^{l-1}, h_{t-1}^l)$ .</p><p>RNN在运动估计、状态更新方面效果较好，但是其并不能很好的处理数据关联这种组合任务。因此使用LSTM单元处理这类问题。 LSTM单元相对于RNN单元多出了一个记忆状态量$c$, 然后经过多个门函数对状态信息进行遗忘，更新和输出等，其形式化表示为$h_t^l = o\odot \tanh(c_t^l), c_t^l = f\odot c_{t-1}^l + i\odot g$, $\odot$表示元素乘法。</p><script type="math/tex; mode=display">i, o, f = \sigma[W^l(h_t^{l-1}, h_{t-1}^l)]</script><h4 id="Bayesian-Filtering"><a href="#Bayesian-Filtering" class="headerlink" title="Bayesian Filtering"></a>Bayesian Filtering</h4><p>假设真实状态$x$, 观测量$z$, 利用马尔科夫假设，当前时刻的状态分布计算为</p><script type="math/tex; mode=display">p(z_t|z_{1:t}) \propto p(z_t|x_t)\int p(x_t|x_{t-1})p(x_{t-1}|z_{1:t-1})dx_{t-1}</script><p>$p(z_t|x_t), p(x_t|x_{t-1}) $分别表示观测概率和转移概率。一般分为预测和状态更新两步。典型的模型有kalman滤波和粒子滤波。</p><p>处理多目标任务时还会遇到两个额外的挑战：</p><ul><li>状态更新前需要先利用数据关联确定目标和检测的对应关系。</li><li>应该有机制能够处理新的轨迹以及移除终止的轨迹。</li></ul><hr><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><h4 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h4><p>$x_t\in R^{N\cdot D}$               特定时刻所有目标的状态， 本文$D=4$， 包含$(x, y, w, h)$, $N$是目标个数。</p><p>$x_t^i$                               表示第$i$个目标的状态</p><p>$z_t\in R^{M\cdot D}$                 表示当前时刻检测的状态。</p><p>$A\in [0, 1]^{N \times (M+1)}$    关联矩阵， $\forall i:  \sum_j A_{ij}=1$, $M+1$考虑了一些目标当前出现漏检的情形。</p><p>$\varepsilon \in [0, 1]^N$                 表示目标在当前帧依然存在的概率。</p><p>$\sim$                                 该符号表示对应变量的groundtruth</p><h4 id="MTT-with-RNNs"><a href="#MTT-with-RNNs" class="headerlink" title="MTT with RNNs"></a>MTT with RNNs</h4><p>方法将MOT任务分解成两部分。一部分包含状态预测、更新和track管理，另一部分包含数据关联。</p><p>好处：</p><ul><li>每个模块可以分开调试</li><li>模块化之后可以简单的通过替换不同模块进行测试</li><li>分成模块单独训练可以加快收敛</li></ul><h4 id="Target-Motion"><a href="#Target-Motion" class="headerlink" title="Target Motion"></a>Target Motion</h4><p>如Fig2的左图所示， RNN模块的输入包含上一时刻的隐变量$h_t$, 状态量$x_t$, 当前时刻的观测量$z_t$, 关联矩阵$A_{t+1}$, 以前上一时刻目标存在与否的概率向量$\varepsilon_t$. 输出五个量： 隐变量$h_{t+1}$, 当前预测状态和更新后状态$x_{t+1}^<em>, x_{t+1}$, 当前每条轨迹依然存在的概率$\varepsilon_{t+1}$以及轨迹存活概率的绝对变化差值$\varepsilon_{t+1}^</em>$</p><p>三个模块分别对应：</p><ul><li>predict： 学习复杂的动态模型用于预测目标状态</li><li>Update： 利用当前观测去更新目标状态</li><li>Birth/Death: 根据目标状态来判别当前时刻检测是目标轨迹的起点还是终点</li></ul><p><strong>值得注意的是， 这里的</strong>$x_{t+1}^<em>, z_{t+1}$<em>*concat时利用的是关联矩阵$A_{t+1}$的信息</em></em></p><h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><script type="math/tex; mode=display">\mathcal{L}(x^*, x, \varepsilon, \tilde{x}, \tilde{\varepsilon}) = \frac{\lambda}{ND}\sum\Vert x^* - \tilde{x}\Vert^2 + \frac{k}{ND}\Vert x-\tilde{x}\Vert^2 + v\mathcal{L}_{\varepsilon}+\xi\varepsilon^*</script><p>等式右边三项分别表示预测误差，更新误差以及birth/death+reg误差。</p><script type="math/tex; mode=display">\mathcal{L}_{\varepsilon}(\varepsilon, \tilde{\varepsilon}) = \tilde{\varepsilon}\log \varepsilon + (1-\tilde{\varepsilon})\log (1-\varepsilon)</script><p>二值交叉熵损失。</p><p>由于$\tilde{\varepsilon}$真实值是一个相对于时间的矩形窗函数，如果单独采用二值交叉熵损失，特别容易导致漏检引起轨迹终止的问题，于是加了一个正则化项用于平滑损失。如下如所示：</p><p><img src="/2019/05/23/MOT-RNN/smoothness.png" alt="smoothness"></p><p>最左边是真实值，中间是没有正则项的结果，最右是正则化项辅助下的记过，黑色点和线表示轨迹，中间点没有均匀分布表示出现漏检，而中间图中漏检对应的地方其$\varepsilon$都很低，表示轨迹结束。</p><h4 id="Data-Association-with-LSTMs"><a href="#Data-Association-with-LSTMs" class="headerlink" title="Data Association with LSTMs"></a>Data Association with LSTMs</h4><p>LSTM的输入是$C|{ij} = \Vert x^i - z^j\Vert_2$矩阵，即欧氏距离矩阵，输出是匹配概率矩阵$A$, $A$的每一行执行softmax归一化，保证每一个轨迹只关联一个检测。</p><h4 id="Loss-1"><a href="#Loss-1" class="headerlink" title="Loss"></a>Loss</h4><p>负对数似然损失</p><script type="math/tex; mode=display">\mathcal{L}(A^i, \tilde{a}) = -\log (A_{i\tilde{a}})</script><p> $\tilde{a}$是正确的关联关系。</p><h4 id="Training-Data"><a href="#Training-Data" class="headerlink" title="Training Data"></a>Training Data</h4><p>由于视频数据标注代价太大，文章使用了大规模的合成数据。具体而言，首先从已知的轨迹中构建一个运动模型（估计轨迹起点位置以及平均速度的均值和方差）然后对于每一个训练样本就可以通过采样的方式生成$N$个训练样本。</p><h4 id="Network-size"><a href="#Network-size" class="headerlink" title="Network size"></a>Network size</h4><p>RNN单元只有一层300个隐节点， LSTM单元两层500个隐节点。</p><h4 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h4><p>RNN训练数据： $100K$ 长度为20帧的长序列。 mini-batches的大小为每个10， 图像中目标数据归一化为[-0.5, 0.5]。</p><hr><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Simulated-data"><a href="#Simulated-data" class="headerlink" title="Simulated data"></a>Simulated data</h4><p><img src="/2019/05/23/MOT-RNN/simulated data.png" alt="simulated data"></p><p>分布的点表示每一时刻目标的位置，groundtruth或者detections， 没有形成轨迹的点是一些噪声点。最先面是用来判断轨迹终止的概率，可以发现在合成数据上效果还是不错的。</p><h4 id="MOT15"><a href="#MOT15" class="headerlink" title="MOT15"></a>MOT15</h4><p><img src="/2019/05/23/MOT-RNN/MOT15 training.png" alt="MOT15 training"></p><p><img src="/2019/05/23/MOT-RNN/MOT15 test.png" alt="MOT15 test"></p><p>实验可以发现这篇文章主要还是在RNN做数据关联方面一个全新的尝试，其效果并不理想，尤其是在test集上，这可能是因为训练数据集太少。另外在训练集上我们也发现RNN_HA的效果比RNN_LSTM效果要好，这说明LSTM实现data association的效果目前还比不了匈牙利算法。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>加上前面的两篇笔记，总共三篇文章都是在尝试用深度网络直接实现数据关联。总的来说Milan大神的这篇文章出现的最早，但由于技术的原因吧性能不好，所以后续研究不多。目前随着数据量的增大以及检测精度的增加，基于CNN做数据关联的工作开始逐渐崭露头角。</p><p>我觉得用深度网络实现数据关联一定会是一个发展趋势，只有数据关联部分使用网络实现了，才能真正意义上说是用深度学习解决多目标跟踪任务。</p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> RNN </tag>
            
            <tag> data assocition using network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Deep Affinity Network for Multiple Object Tracking</title>
      <link href="/2019/05/22/SST-DAN/"/>
      <url>/2019/05/22/SST-DAN/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/05/22/SST-DAN/DAN.png" alt="DAN"></p><p>MOT方法一般包含两个步骤:目标检测和数据关联。 目标检测这两年随着深度学习的发展而迅速发展，但是数据关联绝大多数还是采用hand crafted的方式将表观特征，运动信息，空间关系，group关系等进行结合。 这篇文章则是利用深度网络实现端到端的表观特征抽取和数据关联。 Deep Affinity Network(DAN)还实现了轨迹的初始化和终止等操作。在MOT15和MOT17，以及UA-DETRAC数据集上验证了有效性。这篇文章和上篇笔记FANTrack的出发点类似。</p><p>项目地址： <a href="https://github.com/shijieS/SST.git" target="_blank" rel="noopener">https://github.com/shijieS/SST.git</a></p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p><img src="/2019/05/22/SST-DAN/DAN.png" alt="DAN"></p><p>MOT方法一般包含两个步骤:目标检测和数据关联。 目标检测这两年随着深度学习的发展而迅速发展，但是数据关联绝大多数还是采用hand crafted的方式将表观特征，运动信息，空间关系，group关系等进行结合。 这篇文章则是利用深度网络实现端到端的表观特征抽取和数据关联。 Deep Affinity Network(DAN)还实现了轨迹的初始化和终止等操作。在MOT15和MOT17，以及UA-DETRAC数据集上验证了有效性。这篇文章和上篇笔记FANTrack的出发点类似。</p><p>项目地址： <a href="https://github.com/shijieS/SST.git" target="_blank" rel="noopener">https://github.com/shijieS/SST.git</a></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul><li><p><strong>符号定义：</strong></p><ul><li>$I_t$ 视频的第$t$帧</li><li>$t-n:t$ 从$t-n$ 到$t$的时间间隔</li><li>下标 $t-n, t$ 表示元素是对$I_{t-n}, I_t$计算得到</li><li>$\mathcal{C}_t$ 第$t$帧中目标的中心坐标的集合， $\mathcal{C}_t^i$表示集合的第$i$个元素</li><li>$F_t$第$t$中目标的特征集合， $F_t^i$表示$t$帧中第$i$个目标的特征</li><li>$\Psi_{t-n,t}$是一个$N\times N\times 2M$的tensor，其中N表示目标的个数， $M$表示每个目标特征向量的维度，$N\times N$表示的任意两个来自于不同帧的目标配对，相当于global association。</li><li>$L_{t-n, t}$二值关联矩阵， 1表示关联， 0表示不关联， 表示来自于两张图片中目标的关联矩阵。这里包含了终止和起始状态。</li><li>$A_{t-n, t}$相似度矩阵， 由$\Psi$到$A$进而得到$L$</li><li>$\Tau_t$ 直至第$t$帧时的轨迹的集合。轨迹使用二元组的集合表示,$\Tau_t^i=\{(0,1),(1,2)\}$表示轨迹中的关联是第0帧的第一个目标和第1帧的第2个目标。</li><li>$\mathcal{Z}(\cdot)$算子用来计算集合或者矩阵中的元素</li><li>$\Lambda_t\in R^{\mathcal{Z}(\Tau_{t-1})\times(\mathcal{Z}(\mathcal{C_t})+1)}$ 是一个accumulator matrix，其元素表示目标与轨迹的相似度</li><li>$N_m$表示每一帧中最大允许的目标个数， $B$是batchsize</li></ul></li><li><p><strong>Deep Affinity Network （DAN）</strong></p><p>如图1所示， DAN包含了两部分：特征抽取模块(feature extractor)和相似度估计模块(Affinity estimator), 整体的训练是end-to-end方式的。</p><p>DAN的输入是两幅图像$I_t, I_{t-n}$以及每幅图像中对应目标的中心点坐标$\mathcal{C}_t, \mathcal{C}_{t-n}$, 这里不要求两幅图像连续， 时间间隔$N\in\mathbb{N}^{rand}\sim [1, N_V]$. </p><p>DAN的输出应该是两幅图像中目标的关联矩阵$L_{t-n, t}$</p><p>其主要过程：</p><ul><li><p>输入时序上的两帧图像$I_{t-n}, I_t$(非必须连续)及其对应检测的中心点集$C_{t-n}, C_t$， 将两幅图像送入VGG-like的共享参数Siamese网络，从网络中选择了9层feature maps， 然后从其中利用中心点集和$1\times 1$的卷积核对每个目标构建了$520$特征向量集合$F_t, F_{t-n}$。</p></li><li><p>来自于$F_{t-n}, F_t$的特征进行完全的两两匹配，形成张量$\Psi_{t-n,t}\in R^{1024\times N_m\times N_m}$, 该3D张量经过5层卷积将通道数降维1， $M\in R^{N_m \times N_m}$， 该矩阵其实表示的任意两个目标之间关联的可能性。</p></li><li><p>同时为了刻画轨迹的起点和终点，在矩阵$M$上分别添加augmented行和列， 然后通过按行或者列softmax或者关联矩阵$A, A_1, A_2$计算损失函数进行训练。</p></li></ul></li><li><p><strong>Data Preparation</strong></p><ul><li>Photometric distortions: 像素值尺度变化$[0.7, 1.5]$, 转换成HSV空间， 饱和度的尺度变化$[0.7, 1.5]$, 然后再转回到RGB中，同样尺度[0.7, 1.5]的变化</li><li>使用图像像素均值expand frame， 尺度变化范围$[1, 1.2]$</li><li>图像裁剪， 尺度范围$[0.8, 1]$， 同时需要包含所有检测目标的中心点</li></ul><p>以0.3的概率按上述方法扩展样本对。然后图像固定大小$H\times W\times 3$, 图像水平翻转的概率$0.5$</p></li><li><p>MOT中每一帧存活的轨迹个数不是固定的，给网络的训练带来难度。因此文章给出了一个轨迹个数的上限值$N_m=80$ , Fig.2 给出了关联矩阵的构建示意图。这里为了清晰演示，取$N_m=5$. 第1帧和第30帧图像各检测到4个目标， 总共有$5$个人。 Fig.2c展示了包含dummy row和column的中间矩阵$L_{1, 30}’$的构建, dummy row或者column用来表示dummy bounding box，从而让关联矩阵大小一致，不随目标个数变化而变化。 Fig.2d中关联矩阵进一步augmented引入新的row和column，表示新出现的目标和终止的轨迹。最终使用Fig.2d的矩阵作为训练目标。最后一行或者列中包含多个1则表示多个目标终止或出现。</p><p><img src="/2019/05/22/SST-DAN/1552789627289.png" alt="association matrix"></p><p>这里dummy行或者列与augmented行列是不同的， dummy是用来保证总体的匹配个数相同，需要服从每一行或者列最多只能有1个非零元素，augmented则是用来刻画轨迹的终止和起始，对其非零元素的个数没有约束。允许多条轨迹同时结束， 多个目标同时作为多条新轨迹的起点。</p><h4 id="Feature-extractor"><a href="#Feature-extractor" class="headerlink" title="Feature extractor"></a>Feature extractor</h4><p>在VGG16上进行了修改， 将VGG16后面的全连接层和softmax层都换成了卷积层，这么做主要是卷积层能够更好地encoding目标的空间特征并且可以从空间上抽取每个目标的特征。另外这里的输入数据也更大$3\times 900 \times 900$, 从而修改后的VGG16最后一层的输出还有$56\times 56$大小。 VGG16最后输出的$56\times 56$的feature map采用表2上半部分的结构进一步缩小到$3\times 3$</p><p>特征抽取和估计affinity的网络参数如下表：</p><p><img src="/2019/05/22/SST-DAN/1552791780072.png" alt="architecture details"></p><p>修改后的feature extractor网络具有33层卷积层，然后从中选择了9层作为最终提取特征的层，其选择的层数索引，以及如何从该层降维获得表观特征参数如下</p><p><img src="/2019/05/22/SST-DAN/1552792173244.png" alt="feature dimension"></p><p>利用目标的中心坐标$\mathcal{C}_t$在channel上选择特征，降维采用的$1\times 1$的卷积核， 特征进行concatenate。最终每一个目标形成$520$维的特征向量。对于那些dummy的目标，其特征表示为全零向量。</p><ul><li><p><strong>Affinity estimator</strong></p><p>feature extractor之后每幅图像抽取的特征$F_t\in R^{N_m\times 520}$，然后将两幅图像目标的特征任意的组合就形成了tensor $\Psi\in R^{(520\times 2)\times N_m\times N_m}$, 然后在利用5层卷积核为$1\times 1$的卷积层压缩维度形成相似度矩阵$M\in R^{N_m\times N_m}$,压缩网络的模型如Table 2的下半部分所示。</p><p>这里的M对应于真实的Label，即两个目标是否关联。但是没有解释新出现的轨迹和离开的轨迹。因此文章对相似度矩阵$M$进行了扩充。为了让模型更好训练以及loss更好定义，这里$M$分开进行行列扩充，扩充的向量$\textbf{v}=\gamma \textbf{1}$ , $\gamma$是超参。</p></li><li><p><strong>Network Loss</strong></p><p>$M_1\in R^{N_m\times(N_m+1)}$列扩展矩阵每一行表示第$m$个目标关联情况或者是否结束。 $M_2\in R^{(N_m+1)\times N_m}$行扩展矩阵对应了反向匹配时第二幅图像中目标的关联情况或者是否是新出现的轨迹。对行，列扩展矩阵$M_1, M_2$分别进行行，列softmax得到$A_1, A_2$表示的是概率。</p><p>因此DAN网络损失包含4部分：前向损失，后向损失，一致性损失和组合损失</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_f(L_1, A_1) &= \frac{\sum(L_1\odot(-\log A_1))}{\sum L_1}\\\mathcal{L}_b(L_2, A_2) &= \frac{\sum(L_2\odot(-\log A_2))}{\sum L_2}\\\mathcal{L_c}(\widehat{A_1}, \widehat{A_2}) &= \Vert\widehat{A_1}-\widehat{A_2} \Vert_1\\\mathcal{L_a}(L_3, \widehat{A_1}, \widehat{A_2}) &= \frac{\sum (L_3\odot (-\log(max(\widehat{A_1}, \widehat{A_2}))))}{\sum (L_3)}\\\mathcal{L} &= \frac{\mathcal{L_f}+\mathcal{L_b}+\mathcal{L_a}+\mathcal{L_c}}{4}\end{aligned}</script><p>其中$L_1, L_2$都是与$M_1, M_2$对应的$L_{t-n, t}$的trimmed 版本， $\widehat{A_1}, \widehat{A_2}$是trimmed成$N_m\times N_m$的版本， $L_3$则是同时去掉了最后一行和最后一列。</p><p>四种Loss损失：</p><ul><li>对于前向和后向损失，文章并没有直接使用输出$A_q(q=1,2)$与目标$L_q(q=1,2)$的距离作为损失， 而是采用$A_q$在真实关联位置的相对系数，也就是$-\log(A_q)$作为优化目标，这种方法其实是让关联位置的输出概率更快的接近于1， 而对于其他非真实关联的位置，由于$A_q$采用了行或者softmax，所以就非常接近于0.</li><li>截断后的关联矩阵，即不考虑augmented 行和列后的部分应该是相同的，所以文章使用$L_1$范数约束距离， $L_1$比$L_2$在较小范围内更加紧致。</li><li>$L_a$和$L_q(q=1,2)$类似， 不同的是选择最大$\widehat{A_1}, \widehat{A_2}$作为最终的关联矩阵。</li></ul></li><li><p><strong>DAN deployment </strong></p><p>因为feature extractor部分是参数共享的，而在计算相似度矩阵的时候只是后面的affinity estimator牵涉到两两相互操作，因此，每张图片其实只进行一次feature extractor的操作，后面的affinity estimator是进行匹配。</p></li><li><p><strong>Deep track association</strong></p><p>这部分很关键。为了匹配当前帧中的目标， 将每一帧的特征矩阵$F$和对应的时间戳保存起来，然后可以计算历史frames与当前frame之间的相似度矩阵。</p><p>关联过程： 首先根据第1帧图像中的目标个数初始化轨迹集合$\Tau_0$，轨迹中每一个元素是一个二元组(时间戳， 轨迹编号)，使用hungrain algorithm对accumulator matrix $\Lambda$分解去grow对应的轨迹， $\Lambda$是当前帧目标与多个历史帧目标的相似度矩阵的累积求和，注意这里累积求和是相同轨迹编号进行求和。</p><p>注意这里有个问题，匈牙利算法是一对一约束，但相似度矩阵中添加的最后一行一列不满足这个要求，于是匈牙利算法只是对去除最后一行和列的矩阵进行分解，然后再把最后的行列加进来。</p><p><img src="/2019/05/22/SST-DAN/1552795008751.png" alt="DAN deployment"></p></li></ul></li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>实验部分还是很充分的，在MOT15和MOT17上进行了行人跟踪的实验， 在UA-DETRAC上进行了车辆的跟踪。</p><p><img src="/2019/05/22/SST-DAN/1552795108343.png" alt="MOT17"></p><p><img src="/2019/05/22/SST-DAN/1552795135893.png" alt="MOT17 table"></p><p>这里有个指标有问题$MT$应该是越大越好</p><p><img src="/2019/05/22/SST-DAN/1552795189925.png" alt="UA-DETRAC"></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>实验没有给出在MOT16上DPM检测器下的跟踪性能，我跑了下代码发现性能很差。在MOT17库上性能好是因为MOT17库上的检测相对准确。也就是说该方法其实对于检测的精度还是非常依赖的。因为网络的输入是目标的中心点坐标，并且在抽取特征阶段是利用中心点位置channel 作为特征，检测误差必定导致特征的不准确，从而影响性能。</p><p>另外值得一提的是，这篇文章也只利用的表观信息，如何更好地利用空间信息也是个研究方向。</p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Affinity Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-FANTrack：3DMulti-Object Tracking with Feature Association Network</title>
      <link href="/2019/05/22/FANTrack/"/>
      <url>/2019/05/22/FANTrack/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/05/22/FANTrack/siamese similarity.png" alt="siamese similarity"></p><p>目前大多数深度学习的方法主要基于特征的学习， 代价函数的设计，或者如何有效解决复杂的数据关联模型，很少有利用CNN网络端到端解决MOT的。本文提出使用CNN解决data association问题。该方案纯粹利用数据，从3D的角度实现全局的数据关联，同时处理noisy detections以及目标个数变化等问题。</p><p>文章提供代码： <a href="https://git.uwaterloo.ca/wise-lab/fantrack" target="_blank" rel="noopener">https://git.uwaterloo.ca/wise-lab/fantrack</a></p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p><img src="/2019/05/22/FANTrack/siamese similarity.png" alt="siamese similarity"></p><p>目前大多数深度学习的方法主要基于特征的学习， 代价函数的设计，或者如何有效解决复杂的数据关联模型，很少有利用CNN网络端到端解决MOT的。本文提出使用CNN解决data association问题。该方案纯粹利用数据，从3D的角度实现全局的数据关联，同时处理noisy detections以及目标个数变化等问题。</p><p>文章提供代码： <a href="https://git.uwaterloo.ca/wise-lab/fantrack" target="_blank" rel="noopener">https://git.uwaterloo.ca/wise-lab/fantrack</a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>tracking-by-detection: 分为两个步骤。首先使用一个检测器从帧图像中将前景目标检测出来或者定位出来；然后在时间域内利用离散组合优化问题将noisy detections关联起来形成轨迹。</p><p>current challenges： 先验未知，但数目变化的目标个数； incorrect或者missing的detections； 因为传感器，光照，视角等变化导致的表观变化； 频繁的遮挡或者运动中出现的激烈变化等。</p><p>Milan 是第一个提出端到端实现MOT任务的， 其利用RNN解决数据关联问题。</p><blockquote><p>A. Milan, S. H. Rezatofighi, A. R. Dick, K. Schindler, and I. D. Reid, “Online multi-target tracking using recurrent neural networks,” CoRR, vol. abs/1604.03635, 2016.</p></blockquote><p>但是RNN的训练过程比一般的CNN训练难度大的多。</p><p>本文提出了一种基于CNN的两步骤关联推断算法。如下图所示。首先利用目标的表观和3D特征学习相似度函数。然后从提出的特征配对中利用CNN网络预测一个离散的关联矩阵。</p><p><img src="/2019/05/22/FANTrack/architecture.png" alt="architecture"></p><p>在KITTI数据库上证明了算法的有效性：</p><ul><li>能够利用CNN网络解决多目标关联问题。</li><li>同时利用了表观和3Dbounding box线索，或得到代价函数更加鲁棒。</li><li>性能与当前SOTA方法相当。</li></ul><hr><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>假设任意帧中都有$N$个目标，$M$条已有轨迹和对应的label， 检测器使用的AVOD 3D检测器。</p><h4 id="Similarity-Network"><a href="#Similarity-Network" class="headerlink" title="Similarity Network"></a>Similarity Network</h4><p>文章提出的Similarity Network如Fig2所示的SimNet。该网络有两个输入，分别对应这轨迹和检测的表观特征对，以及3Dbounding box的参数对， 3Dbounding box使用7维向量表示， 图像特征使用$7\times 7\times 320$维向量， 输出是$N_{max}$层feature maps。每个feature map大小是$10\times 10$, 每一个像素对应真实环境中$0.5m$的分辨率。feature map输出的是每一个目标与其他所有目标的相似度， 该feature maps被用于数据关联。</p><p>SimNet有两个分支： bounding box分支和appearance 分支， 每一个分支都是siamese结构判断两个输入相似与否。这两个相似度最终按照importance 分支的重要性程度进行加权。最后有这些特征获得similarity map</p><ul><li><p><strong>bounding box 分支</strong></p><p><img src="/2019/05/22/FANTrack/bounding box.png" alt="bounding box"></p><p>3D bounding box 的特征$(x, y, z, l, w, h, \theta_z)$分别表示中心点位置， 坐标对齐的长度以及z轴旋转角度。</p><p>输入是$（N+M）\times 1\times 7$放入$1\times 1$的卷积核两层全连接层后得到embedding特征，然后进行归一化处理，最后得到的特征用于计算余弦相似度。</p></li><li><p><strong>Appearance分支</strong></p><p><img src="/2019/05/22/FANTrack/appearance.png" alt="appearance"></p><p>输入的是轨迹和检测concate的AVOD抽取的特征$(N+M)\times 7\times 7\times 320$, 这个特征是AVOD的第二和第四层的融合。在转成FC层之前引入了全局池化层以降低模型参数，最终归一化之后的特征用于计算余弦相似度。</p></li><li><p><strong>Importance 分支</strong></p><p>该分支主要用于计算bounding box分支和appearance分支的重要性程度</p><p><img src="/2019/05/22/FANTrack/importance.png" alt="importance"></p><p>将特征concat之后进行一个2维输出，分别表示两者权重$w_{bbox}, w_{appear}$, 这部分权重没有标签，相当于学习出attention。</p></li><li><p><strong>Similarity map</strong></p><p><img src="/2019/05/22/FANTrack/global similarity map.png" alt="global similarity map"></p><p>这张图表示的是一条轨迹与所有排列好的检测之间的相似度计算。将原始图像按照$0.5m$的距离划分成网格，然后将detections映射到对应的网格中，使用target的特征作为卷积核在网格上进行卷积得到global similarity map， 然后再从这张similarity map的target位置周围crop出$5\times 5 m^2$的区间，也就是$21\times 21$大小的map区域，于是$N$条轨迹最终获得$N\times 21\times 21$的similarity maps， 从similarity maps中直接学习匹配关系。</p></li><li><p><strong>Loss Function</strong></p><script type="math/tex; mode=display">L(\Theta_1) = \frac{1}{N^+}\sum_{i=1}^N w_{skew}^i \times w_{cost}^i\times (1-y^i\times \hat{y}^i(\Theta_1))</script><p>$\Theta_1$是待学习的参数， $N^+$表示非零权重的个数，$y^i\in \{-1, 1\}$是余弦相似度的label， $w_{skew}^i$有点类似于focal loss中$\gamma$的作用,用来平衡正负样本的影响， $w_{cost}^i$用来强调hard negative样本。 $\hat{y}^i(\Theta_1)$表示网络输出的最终余弦相似度</p><script type="math/tex; mode=display">\hat{y}^i(\Theta_1) = w_{bbox}(\Theta_1)^i \times \hat{y}_{bbox}^i + w_{appear}^i \times \hat{y}_{appear}^i(\Theta_1)</script></li><li><p><strong>SimNet 创建训练样本</strong></p><p>如果bounding box与某一个groundtruth 的IoU超过0.8则认为是正样本，另外为了增加样本多样性，如果proposals与当前已经确定类别的proposals的IoU大于0.95则抑制该proposals。</p><p>这里说的都是判断proposals与groundtruth的配对，也就是给proposals分配不同的ID，然后根据ID组成不同的正负样本对。</p></li></ul><h4 id="Data-Association-Network"><a href="#Data-Association-Network" class="headerlink" title="Data Association Network"></a>Data Association Network</h4><p><img src="/2019/05/22/FANTrack/data association.png" alt="data association"></p><ul><li>为了处理目标变化的情形， 设定了最大目标数$N_{max}$， 那么$N_{max}-N$称为dummy maps,有全0元素构成。</li><li>处理丢失的detection时，对每一个目标引入了一个额外的cell用于解释虚假检测。</li></ul><p>输入数据，即$N_{max}\times21\times 21$的similarity maps首先输入到3层空洞卷积中，空洞因子分别为$2, 4, 6$, 然后后面接一层$3\times 3$卷积层用于计算logits maps。</p><p>由于目标和轨迹的相似度位置是已经确定的，所以网络计算association时只计算非零区域，于是设计了对每一层局部相似度map的association masks。 在masks中可能是检测的位置上为0， 否则则置为最小负值。 然后mask与logits maps叠加，这时可能是检测的区域特征被保存下来，不可能是检测的区域特征几乎都是最小值。</p><p>接着叠加的特征分成了两个分支，一个分支是全连接层用于判断该轨迹是否对应漏检。另一个分支将logits maps拉成1D向量然后与第一个分支的值concat一起，形成$N_{max}\times (21\times21+1)$的tensor，然后使用softmax计算$21\times 21+1$为的概率向量。 最终经过reshape获得关联概率矩阵，然后获得关联关系。<strong>这里的softmax相当于在2D上进行softmax，所以也就只关联到1个detection，但并没有办法保证1个detection仅关联了1条轨迹</strong> 。</p><ul><li><strong>loss function</strong><script type="math/tex; mode=display">L(\Theta) = l(\Theta)_{assoc} + l(\Theta)_{reg}</script>前后两项分别表示二值交叉熵损失和正则化损失。<script type="math/tex; mode=display">q_{vec} = q_{assoc}^t(i,j) \times \log(min(\hat{q}_{assoc}^t(i,j;\Theta)+0.01, 1))\\p_{vec} = p_{assoc}^t(i,j) \times \log(min(\hat{p}_{assoc}^t(i,j;\Theta)+0.01, 1))\\l(\Theta)_{assoc} = \sum_t^N\sum_{i,j}^{21+1}(-q_{vec})+(-p_{vec})</script>这个式子是交叉熵损失函数。 $q_{assoc}^t(i,j) = 1-p_{assoc}^t(i,j)$</li></ul><h4 id="Track-Management"><a href="#Track-Management" class="headerlink" title="Track Management"></a>Track Management</h4><ul><li><p>利用kalman滤波预测运动目标的状态</p></li><li><p>利用概率值$P_e$创建贝叶斯估计模型初始化、更新和剪枝轨迹。</p><p><img src="/2019/05/22/FANTrack/algorithm.png" alt="algorithm"></p></li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>KITTI. 包含21段训练序列和29段测试序列。由于训练序列又按照难度、遮挡和模糊程度等划分不同的等级，作者选择每个层级的$20\%$用来验证。</p><h4 id="Benchmark-Results"><a href="#Benchmark-Results" class="headerlink" title="Benchmark Results"></a>Benchmark Results</h4><p><img src="/2019/05/22/FANTrack/results.png" alt="results"></p><p>benchmark上的对比实验来看性能并不好。作者认为本文方法inference是在3D上进行的，而evaluation仅评估了2D指标，所以对于本文方法是不全面的，另外对比的方法都不是基于深度学习的数据关联方法。还有本文只是利用了kalman滤波进行运动状态的估计和预测， 如果用其他更复杂的算法性能或许更好。</p><p>Ablation分析对比了不同的特征抽取模块和关联模块，<strong>这个实验做的并不是很充分， 比如没有分析其他的特征抽取方法和匈牙利算法的性能如何，这样才能更好地体现SimNet的性能</strong> </p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul><li>本文方法输入需要是每一个检测的表观特征和3Dbounding box信息，这里说是计算轨迹和检测之间的关联，其实轨迹部分的特征如何选择并没有细说。</li><li>SimNet抽取过程中2个分支分别计算表观和box的相似度，然后用importance branch计算每类相似度的权重，而这个权重的输入其实是concat的表观和box特征，感觉可能存在一些问题。</li><li>SimNet抽取出来的是每个target与所有检测的similarity， 经过AssocNet获得association 概率，其实这个模块我觉得重要的有两点，其一是处理漏检，其二是全局的spatial softmax。</li><li>我认为方法还存在的不足在于并没有约束一个检测只能对应一个target。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> KITTI </tag>
            
            <tag> Feature Association Network </tag>
            
            <tag> 3D </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Relation Network for Object Detection</title>
      <link href="/2019/05/22/relationNetwork/"/>
      <url>/2019/05/22/relationNetwork/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/05/22/relationNetwork/title.png" alt="title"></p><p>在目标检测领域，深度学习(RCNN系列，YOLO系列，SSD系列)方法取得了很大的成功，但是这些检测方法考虑的仅仅是目标的个体，而没有考虑一帧图像中目标之间的相互影响，所以本文提出一种<strong>object relation module</strong> ,通过考虑目标集合中个体的交互关系来辅助目标的检测。该模块参数很少能够很方便的嵌入到已有的网络中，提高目标检测性能，另外本文使用该模块实现duplicate removal（NMS实现的功能），从而能够实现网络的end-to-end学习。</p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>我们都知道在目标检测领域，深度学习(RCNN系列，YOLO系列，SSD系列)方法取得了很大的成功，但是这些检测方法考虑的仅仅是目标的个体，而没有考虑一帧图像中目标之间的相互影响，所以本文提出一种<strong>object relation module</strong> ,通过考虑目标集合中个体的交互关系来辅助目标的检测。该模块参数很少能够很方便的嵌入到已有的网络中，提高目标检测性能，另外本文使用该模块实现duplicate removal（NMS实现的功能），从而能够实现网络的end-to-end学习。</p><p><strong>思考_</strong> : 在多目标跟踪问题中，一般的计算两个目标之间是否匹配的策略还是类似于ReID的方法，仅考虑两者之间的相似度，但是在跟踪的时空图中，两个观测的关系不仅仅取决于两者之间的关系，还取决于同时存在的其他目标之间的关系，所以多目标的匹配策略同样可以使用类似的relation module。</p><h3 id="Object-Relation-Module-ORM"><a href="#Object-Relation-Module-ORM" class="headerlink" title="Object Relation Module (ORM)"></a>Object Relation Module (ORM)</h3><p><img src="/2019/05/22/relationNetwork/pipeline.png" alt="pipeline"></p><p>文章在处理object之间关系时主要借鉴和继承了google的关于attention的开山之作</p><blockquote><p>Attention is all your need</p></blockquote><p>具体而言，假设经过特征抽取网络，比如RPN层结果，获得了$N$个候选目标的表观特征$f_A$和几何特征$f_G$，也就是bounding box特征。 $N$个特征表示为$\{(f_A^n, f_G^n）\}_{n=1}^N$, 那么经过relation module之后的特征$f_R(n)$如下计算</p><script type="math/tex; mode=display">f_R(n) = \sum_mw^{mn}\cdot(W_v\cdot f_A^m)</script><p>可以看到$W_v$其实是对表观特征先进行一次embedding，然后使用一组与目标$n$有关的权重进行加权求和作为第$n$个样本新的特征，所以这是一种Attention的操作。</p><p>而权重的计算结合了空间几何特征和表观特征，其模块计算图如下：</p><p><img src="/2019/05/22/relationNetwork/relation module.png" alt="relation module"></p><p>具体公式如下：</p><script type="math/tex; mode=display">\begin{align}w_A^{mn}&=\frac{\text{dot}(W_kf_A^m, W_Qf_A^n)}{\sqrt{d_k}}\\w_G^{mn} &= \max \{0, W_G\cdot \Epsilon_G(f_G^m, f_G^n)\}\\w^{mn} &= \frac{w_G^{mn}\cdot \exp{(w_A^{mn})}}{\sum_k w_G^{kn}\cdot \exp{(w_A^{kn})}}\end{align}</script><p>其中$W_A, W_V, W_Q, W_k$都是需要学习的参数，$\Epsilon(\cdot,\cdot)$是特征融合函数，可以使用网络实现，论文中类似于’Attention is all your need’的做法，使用cosine和sine functions融合位置参数$f_G=(\log(\frac{|x_m-x_n|}{w_m}),\log(\frac{|y_m-y_n|}{h_m}),\log(\frac{w_n}{w_m}),\log(\frac{h_n}{m_m}))$</p><p>由于空间中目标个数也是不定的，所以其实就是在图上进行一种attention， 可以参考论文Graph Attention Network（GAT）, 同样的类似于GAT方法，文章中也使用了multi-head attention的一种方式，如Figure 2所示，每一个relation模块是一种attention， 多个attention 模块同时作用， 文中取16个这种模块。 另外attention之后的特征和原始特征进行相加，这类似于Graph Convolution Network中邻接矩阵的归一化加上单位阵的操作。</p><script type="math/tex; mode=display">f_A^n = f_A^n + \text{Concat}[f_R^1(n), \cdots , f_R^{N_r}(n)], \text{for all}\quad n</script><p>如果将所有attention的结果等长加在一起会导致计算量太大，所以这里将所有的attention结果concat在一起和原始的特征加在一起减小运算量，但文章没有实验对比性能有什么差异。<strong>注意</strong>每一个attention模块的输出大小应该特殊设置，以使得最后concat之后特征与原始特征等长从而能够进行相加操作。 </p><hr><h3 id="Relation-Networks-for-Object-Detection"><a href="#Relation-Networks-for-Object-Detection" class="headerlink" title="Relation Networks for Object Detection"></a>Relation Networks for Object Detection</h3><p>相对于已有的object detection框架而言，简单的增加了两个模块，如下图所示。(a)用于刻画Instance之间的关系，(b)用于去除重复的Instances，下面会介绍两个模块。</p><p><img src="/2019/05/22/relationNetwork/illustration.png" alt="illustration"></p><p>第一个模块是用于从每个目标所处的空间关系出发，融合目标的context信息从而提取更好地特征， 第二个模块是希望利用目标之间的位置关系将NMS的过程融合到网络里面从而避免阈值的选取。<strong>注意</strong>(b)图中$s_0, s_1$是乘法操作。</p><h4 id="Relation-for-Instance-Recognition"><a href="#Relation-for-Instance-Recognition" class="headerlink" title="Relation for Instance Recognition"></a>Relation for Instance Recognition</h4><p>以RCNN为basenet，RCNN在ROI pooling之后过程如下:</p><script type="math/tex; mode=display">\begin{align}ROI\_Feat_n &\xrightarrow{FC} \quad \quad 1024 \\&\xrightarrow{FC} \quad \quad 1024\\ &\xrightarrow{LINEAR}(score_n, bbox_n)\end{align}</script><p>加上relation module，即Figure 2模块在FC之后得到计算过程如下：</p><script type="math/tex; mode=display">\begin{align}\{ROI\_Feat_n\}_{n=1}^N &\xrightarrow{FC} \quad \quad 1024\cdot N \xrightarrow{\{RM\}^{r_1}} 1024 \cdot N\\&\xrightarrow{FC} \quad \quad 1024 \cdot N \xrightarrow{\{RM\}^{r_2}} 1024 \cdot N\\ &\xrightarrow{LINEAR}\{(score_n, bbox_n)\}_{n=1}^N\end{align}</script><p>这里$r_1, r_2$分别表示每个RM模块中head的个数，可以选择不同的数值。</p><h4 id="Relation-for-Duplicate-Removal"><a href="#Relation-for-Duplicate-Removal" class="headerlink" title="Relation for Duplicate Removal"></a>Relation for Duplicate Removal</h4><p>作者把duplicate removal归结成一个二分类问题，对于每一个gt box，只有一个detected box是正确的，其他的都是所谓duplicate。作者的duplicate removal network是接在classifier的输出后面。该模块的输入包括object proposal的 score vector（属于各个类别的概率）, bbox，以及proposal的特征（典型的1024维）。对于某一个object proposal的某一个类别，假设属于这个类别的概率为$score^n$，首先经过一个rank embed模块，即拿出其他object proposal属于该类别的score，进行排序，得到第n个object proposal在排序中的下标（rank），作者特别说明了，使用rank值而不是直接score的值非常重要。然后将rank值映射到128维向量，同时将该proposal的特征也映射到128维，将两种128维的特征相加之后作为新的appearance feature，然后和bbox作为relation module的输入，得到新的128维的输出，和$W_s$做内积之后通过sigmoid得到$s_1$，最终的correct的概率$s=s_0 * s_1$</p><p>我觉得这部分选择rank而不是score作为特征的原因在于rank是一种序列特征，其与score的具体数值无关。</p><h4 id="End-to-End-Object-Detection"><a href="#End-to-End-Object-Detection" class="headerlink" title="End-to-End Object Detection"></a>End-to-End Object Detection</h4><p>这一部分我觉得挺有意思的。</p><p>end2end training就是把两部分损失叠加起来，但是直观上而言会存在一些问题。</p><ul><li><p>首先， instance recognition step和duplicate removal step目标是相反的， instance recognition step是将所有可能是目标的区域都尽可能的获得较高的置信度，而duplicate removal step却要求部分可能是真值得候选区域只选一个其余的置信度要尽可能的低。这个问题实际上不冲突的关键就在于duplicate removal模块中$s_0, s_1$乘积作用， $s_0$可以保留instance recognition step的作用，$s_1$用来达到duplicate removal step的目标。</p></li><li><p>duplicate removal 模块并没有确切的标签，其标签是通过instance recognition 模块输出与真实标签的覆盖程度确定的，因此训练过程标签中是会发生变化的。这种变化会不会导致训练不稳定？文章指出实验中并没有出现不稳定的现象.</p><blockquote><p>While there is no theoretical evidence yet, our guess is that the duplicate removal network is relatively easy to train and the instable label may serve as a means of regularization.</p></blockquote></li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>文章中所有的实验都是在COCO数据集上完成</p><h4 id="Relation-for-Instance-Recognition-1"><a href="#Relation-for-Instance-Recognition-1" class="headerlink" title="Relation for Instance Recognition"></a>Relation for Instance Recognition</h4><p>对比试验中NMS的阈值设为0.6，(a)中none表示不使用几何信息,$w_g=1$, unary表示使用的是[1]中的embedding方法，(b)中是attention head的个数，(c)中是每个阶段使用relation module的个数</p><p><img src="/2019/05/22/relationNetwork/ablation.png" alt="ablation"></p><p>结论：</p><ul><li>relation attention 能够提升检测性能</li><li>几何特征能够提升检测性能</li><li>attention head的个数增多能够提升检测性能，文中实验在16时达到饱和</li><li>relation module的模块个数能够提升检测性能</li></ul><h4 id="Does-the-improvement-come-from-more-parameters-of-depths"><a href="#Does-the-improvement-come-from-more-parameters-of-depths" class="headerlink" title="Does the improvement come from more parameters of depths?"></a>Does the improvement come from more parameters of depths?</h4><p>我觉得这个对比实验做的很好，引入attention后相当于网络多加了一些参数和层，那么到底是attention机制有效还是参数更多了有效？</p><p><img src="/2019/05/22/relationNetwork/201806_003_07.png" alt="various heads"></p><p>table 2 表明相同深度下使用和不使用relation module的性能已经不同的heads的性能对比。</p><ul><li>A wider 2fc head (1432-d, b) only introduces small improvement (+0.1 mAP).</li><li>A deeper 3fc head (c) deteriorates the accuracy (-0.6 mAP), probably due to the difficulty of training. </li><li>To make the training easier, residual blocks are used5 (d), but only moderate improvement is observed (+0.3 mAP). </li><li>global context is used (e, 2048-d global average pooled features are concatenated with the second 1024-d instance feature before classification), no improvement is observed. </li><li>our approach (f) significantly   improves the accuracy (+2.3 mAP). </li><li>nother baseline which concatenates the original pooled features with the ones from a 2× larger RoI (g), the performance is improved from 29.6 to 30.4 mAP, indicating a better way of utilizing context cues.</li><li>we combine this new head with relation modules, that is, replacing the 2fc with {r1, r2}={1, 1} (h). We get 32.5 mAP, which is 0.6 better than setting (f) (31.9 mAP). This indicates that using a larger window context and relation modules are mostly complementary.</li><li>When more residual blocks are used and the head network becomes deeper (i), accuracy no longer increases. </li><li>accuracy is continually improved when more relation modules are used (j). </li></ul><p>对比实验从各个角度证明了文章提出的RM的有效性。</p><h4 id="Relation-for-duplicate-removal"><a href="#Relation-for-duplicate-removal" class="headerlink" title="Relation for duplicate removal"></a>Relation for duplicate removal</h4><p>backbone-network : Faster RCNN</p><p>前面我们提到在for duplicate removal模块是，输入特征是bbox和rank与appearance embedding之后的特征和。那么不同的输入会有什么影响。</p><p><img src="/2019/05/22/relationNetwork/201806_003_09.png" alt="ablatopn_study_dup_removal"></p><p>单独使用NMS， 阈值0.5的时候AP是29.6， 第二列正是算法最终使用的特征，即rank对应的特征，表观特征和bbox特征，第三列表示当不使用rank特征时，AP=26.6下降严重，说明rank特征有用。第三列的第二子列表示将rank特征用score特征取代，发现性能也会下降。第四列表示不使用表观特征，这时候可以发现只利用rank和box特征相对于NMS性能也是提升的。第五列的第一子列表示不使用bbox信息，第二子列表示计算bbox的attention 权重时方式不同，是将$f_G$映射到高维空间然后加到$f_A$上作为新的特征，发现这两种方式性能都下降了。</p><p><img src="/2019/05/22/relationNetwork/201806_003_08.png" alt="nms"></p><p>对比了不同阈值下NMS和本文relation module进行抑制duplicate的性能。NMS的阈值是IoU的阈值， 本文的阈值是二分类问题的阈值。最下面一行表示end-2-end训练，发现性能提升还是很明显的。</p><p><img src="/2019/05/22/relationNetwork/201806_003_10.png" alt="examples"></p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>大多数的目标检测算法都仅考虑了目标本身(最后的分类层不纳入考虑，因为分类层是指定类别，与proposal位置无关)， 本文则是考虑了目标之间的相互作用，目标之间的关系对于目标的检测还是有帮助的，比如行人检测时，两个行人交互时一定是存在特殊约束的，而这种约束则可以通过相互关系进行抽象。同时利用目标关系来实现NMS的功能是一个较大的创新点。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Relation Network </tag>
            
            <tag> Object Detection </tag>
            
            <tag> GCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Repulsion loss:Detecting Pedestrians in a Crowd</title>
      <link href="/2019/05/21/repulsion-loss/"/>
      <url>/2019/05/21/repulsion-loss/</url>
      
        <content type="html"><![CDATA[<p>尽管目标检测目前已经取得了非常好的性能，但是针对于特定领域内的跟踪方法还可以进一步探讨。本篇文章的研究重点在于更好的检测拥挤场景下的行人。</p><p>文章首先分析了拥挤场景下SOTA检测方法存在的问题，然后提出了一种针对于拥挤场景专门设计的回归损失。该损失函数的启发源主要有两点：预测框应该尽可能和目标接近；同时预测框应尽可能地与surrounding目标区分。</p><p>实验证明本文提出的损失函数能够在拥挤场景中很大提升SOTA方法的性能。</p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>尽管目标检测目前已经取得了非常好的性能，但是针对于特定领域内的跟踪方法还可以进一步探讨。本篇文章的研究重点在于更好的检测拥挤场景下的行人。</p><p>文章首先分析了拥挤场景下SOTA检测方法存在的问题，然后提出了一种针对于拥挤场景专门设计的回归损失。该损失函数的启发源主要有两点：预测框应该尽可能和目标接近；同时预测框应尽可能地与surrounding目标区分。</p><p>实验证明本文提出的损失函数能够在拥挤场景中很大提升SOTA方法的性能。</p><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>一般情形下，遮挡可以分为两种： 类间遮挡(inter-class occlusion)和类内遮挡(intra-class occlusion). 在行人检测任务上， inter-class occlusion例如行人被汽车，树木等他类遮挡；intra-class occlusion 如人与人之间的遮挡。</p><p>crowd occlusion对行人检测的影响主要体现在增加了pedestrian定位的难度。例如两个目标$T,B$发生了遮挡， 那么检测器就会因为这两个target overlap部分的特征相同而出现较大的误差。导致$T$的预测窗口可能更偏向于$B$ 或者相反。更糟糕的是，一般detection之后的结果需要进行NMS操作，NMS操作可能凭IOU将发生shift的预测框直接剔除，导致miss detection的产生。所以crowd occlusion环境对于NMS的阈值较敏感。 阈值较高剔除的框越少可能产生的false positive越多， 阈值较小剔除较多导致miss detections很多。</p><p>传统的检测方法使用box regression技术localized 目标，具体而言就是让proposals和ground-truth之间的距离尽可能地接近，距离度量可以使用$\text{Smooth}_{L1}$ 或者IoU等。但是这种约束只是让predict尽可能地和target接近，而并没有考虑到target周围的影响。如下图所示， 一般检测方法只越是红色虚线框与棕色框尽可能接近，而对于和蓝色框的关系不加约束。本文解决的正是这个问题。</p><p><img src="/2019/05/21/repulsion-loss/illustration.png" alt="illustration"></p><p>本文的贡献点：</p><ul><li>实验分析了crowd occlusion对于行人检测的影响。在CityPersons benchmark上定量的分析了因crowd occlusion导致的false positives和miss detections的变化。</li><li>提出了两种repulsion loss处理crowd occlusion 问题： RepGT Loss和RepBox Loss。 RepGT loss惩罚的是预测框偏向其他的ground truth的程度， RepBox是为了让预测的box之间尽可能地分开，以削弱对NMS阈值的依赖。</li><li>利用提出的repulsion losses，训练了一个crowd-robust的端到端网络，并在CityPerson和Caltech-USA上验证了模型性能。另外PASCAL VOC数据集表明提出的repulsion loss对于通用目标的检测同样有积极作用。</li></ul><hr><h3 id="拥挤遮挡分析"><a href="#拥挤遮挡分析" class="headerlink" title="拥挤遮挡分析"></a>拥挤遮挡分析</h3><p><strong>数据集合度量</strong>   CityPersons是语义分割数据集CityScapes上新创建的行人检测数据集。35000个人以及附加的13000左右的ignored regions， 提供了行人的bounding box和可见部分的标注。本文的实验都是在CityPerson的reasonable 的train/validation子集上进行的。评估使用FPPI(False Positives Per Image)的对数值.($MR^{-2}$) 该指标越小越好。</p><p><strong>Detector</strong> 检测器使用的还是Faster RCNN框架。不同点在于将backbone由VGG-16替换成ResNet-50.<strong>注意</strong> ResNet-50很少在行人检测中使用，主要是因为ResNet-50下采样的倍数太大，最终得到的特征太少。所以文章中使用了膨胀卷积(dilated convolution)使最后输出的特征大小事输入图像的1/8.ResNet-based 检测器在验证集上取得了$14.6 MR^{-2}$</p><p><strong>检测失败原因分析</strong> </p><ul><li><p>Miss detections.</p><p>数据集中样本的可见部分也使用box标注，所以样本的遮挡率可以得到  $occ = 1 - \frac{area(BBox_{visible})}{area(BBox)}$ </p><p>在ground-truth上定义不同的子集。</p><p>​        <em>occlusion case</em>:    $occ \ge 0.1$;</p><p>​        <em>crowd occlusion case</em>: $occ \ge 0.1$ 且存在至少一个gt box与其$IoU\ge 0.1$ .</p><p>按照上述定义， reasonable 验证集中1579个标注行人中，有$810(51.3)$的遮挡样本，称为reasonable-occ子集， $479(30.3%)$个crowd occlusion样本，称为reasonable-crowd子集。</p><p>下图分析了在reasonable, reasonable-occ, reasonable-crowd三种子集上miss detection的统计情况。</p><p><img src="/2019/05/21/repulsion-loss/miss_detection.png" alt="miss_detection"></p><p>横轴的label表示在指定false positive数目条条件下，miss detection的数目， 比如#Miss@20fp表示调节参数使得保留20个false positive下的miss detection数目。可以看到在所有的miss detections中，源于reasonable-crowd的大约占了60%，表明crowd occlusion是影响detector性能的主要因素。另外通过降低NMS的阈值，如false positive从100到500， 因crowd occlusion引起的miss detections比例从60.7%上升到了69.2%， 表明<strong>NMS降低阈值并不能解决因crowd occlusion引起的漏检问题</strong>。</p><p>在下图(a)中，给出了miss detection 随着检测器置信度阈值增大的变化曲线。可以发现baseline方法在置信度较高时会产生更多的漏检，这部分主要是由于detections之间的重叠导致的。同时该图表明了RepGT对于miss detection和False positive的作用</p><p><img src="/2019/05/21/repulsion-loss/miss detection_det_score.png" alt="miss_detections_det_score"><img src="/2019/05/21/repulsion-loss/proportion of crowed error.png" alt="proportion of crowed error"></p></li><li><p>False Positives</p><p>false positives可以分为三类：</p><ul><li>background error: 预测框和任意的target $IoU \le 0.1$; </li><li>localization error: 与一个且只有一个target的$IoU \ge 0.1$ ，注意这里是指没有匹配上，但依然与某些目标存在IOU; </li><li>Crowd error: predictions 与至少两个ground-truth的$IoU\ge 0.1$ </li></ul><p>上图中的(b)图表示crowed errors随着false positives的变化所占的比例。可以发现其几乎稳定在$20\%$左右。</p><p>也表明在相同的False positive 数中RepGT loss中Crowd error的占比一直低于baseline中产生的Crowd error占比。</p><p>下图一些crowd error的例子。</p><p><img src="/2019/05/21/repulsion-loss/crowd error1.png" style="zoom:10"><img src="/2019/05/21/repulsion-loss/crowd error2.png" style="zoom:10"></p><p>其中绿色表示正确的匹配，红色表示crowd error。错误一般是由于向临近的目标偏移导致的。</p></li><li><p>Conclusion</p><blockquote><p>The analysis on failure cases validates our observation: <strong>pedestrian detectors are surprisingly tainted by crowd occlusion, as it constitutes the majority of missed detections and results in more false positives by increasing the difficulty in localization</strong>.</p></blockquote></li></ul><hr><h3 id="Repulsion-Loss"><a href="#Repulsion-Loss" class="headerlink" title="Repulsion Loss"></a>Repulsion Loss</h3><p>总的损失 $L = L_{Attr} + \alpha <em> L_{RepGT} + \beta </em> L_{RepBox}$ </p><p>其中attraction term $L_{Attr}$表示预测框和真实框之间的相似度， $L_{RepGT}$表示的预测框与其它真实框尽可能地远离， $L_{RepBox}$表示预测框与其他target的预测框尽可能地远离。 </p><p>$P = (l_p, t_p, w_p, h_p), G=(l_G, t_G, w_G, h_G)$ 分别表示提取的框和真实的框， $P_+=\{P\}$  表示所有正样本组成的集合 (存在至少一个ground-truth背景 $IoU \ge 0.5$ ), $\mathcal{G} = \{G\}$表示所有的ground-truth </p><p><strong>Attraction Term</strong> </p><script type="math/tex; mode=display">L_{Attr} = \frac{\sum_{P\in P_+}Smooth_{L_1}(B^P, G_{Attr}^P)}{|P_+|}</script><p>其中 $G_{Attr}^P = arg max_{G\in \mathcal{G}} IoU(G, P)$ </p><p> <strong>Repulsion Term (RepGT) </strong></p><script type="math/tex; mode=display">G_{Rep}^P = arg max_{G\in\mathcal{G\setminus{G_{Attr}^P}}}IoU(G, P)</script><p>表示的是与预测框的IoU第二大的ground truth</p><script type="math/tex; mode=display">L_{Attr} = \frac{\sum_{P\in P_+}Smooth_{ln}(IoG(B^P, G_{Rep}^P))}{|P_+|}</script><p>其中$IoG(B, G) = \frac{area(B\and G)}{area(G)}$ </p><script type="math/tex; mode=display">Smooth_{ln} = \begin{cases}-ln(1-x), ~~~~~~~~~~x\le \sigma \\\frac{x-\sigma}{1-\sigma} - ln(1-\sigma), ~~x\gt \sigma\end{cases}</script><p>$\sigma \in [0, 1)$是光滑参数。</p><p><strong>RepulsionTerm（RepBox）</strong></p><p>为了让检测结果对NMS尽可能地鲁棒， RepBox Loss的目的是让predict与其他目标的target的尽可能地远离。将$P_+$ 根据IoU分配到$|\mathcal{G}|$ 中，$P_+ = P_1 \and P_2 \and \cdots \and P_{|\mathcal{G}|}$. 那么对于任意两个来自于不同子集的proposals，希望他们的重叠度尽可能的小</p><script type="math/tex; mode=display">L_{RepBox} = \frac{\sum_{i\neq j}Smooth_{ln}(IoU(B^{P_i}, B^{P_j}))}{\sum_{i\neq j}\bold{1}[IoU(B^{P_i}, B^{P_j}) > 0] + \epsilon}</script><p>其中$\bold{1}$ 表示identity 函数， $\epsilon$ 是很小的常量防止除零操作。</p><p><strong>Discussion</strong> </p><ul><li><p>Distance metric。 在计算repulsion term时， 使用IoG, IoU而不是$Smooth_{L1}$ 是由于 IoG, IoU 取值[0,1]，而SmoothL1无界， 使用SmoothL1会要求predicted box与repulsion ground-truth越远越好， 而IoG只是要求overlap越小越好，所以后者与我们设计思想更搭。另外在RepGT中使用IoG而不是IoU是因为，IoU-based loss可能导致enlarge bounding box增大union area来降低IoU，而IoG的降低则必须是overlap的降低.</p></li><li><p>Smooth Parameter $\sigma$ . 如下图. 所示$\sigma$ 可以调节repulsion loss对outliers的鲁棒性。</p><p><img src="/2019/05/21/repulsion-loss/sigma.png" alt="sigma"></p></li></ul><hr><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><strong>Dataset</strong></p><ul><li>CityPersons</li><li>Caltech-USA 2.5小时的视频，划分为训练集(42500帧)和测试集(4024帧)</li></ul><p><strong>Training Details</strong></p><ul><li>CityPersons: 80k iterations, init_lr = 0.016, 60k之后lr下降10倍</li><li>Caltech-USA：160k iterations, init_lr = 0.016, 120k 之后lr下降10倍</li><li>SGD</li><li>4 GPU</li><li>minibatch=4</li><li>weight decay=0.0001， momentum=0.9</li><li>No multi-scale training/testing set used</li><li>For caltech-USA, 10x set(~42k frames) is used for training</li><li>Online Hard Example Mining (OHEM) is used to accelerate covergence</li></ul><p><strong>Ablation Study</strong></p><ul><li><p>RepGT Loss</p><p>Table 1中比较了不同的$\sigma$ 对SmoothL1loss的影响， $\sigma=0$ 意味着直接将 $-ln(1-IoG)$相加。</p><p><img src="/2019/05/21/repulsion-loss/ablation.png" alt="ablation"></p><p><img src="/2019/05/21/repulsion-loss/parameters.png" alt="parameters"></p></li><li><p>RepBox Loss</p><p>由table1可以发现在$\sigma=0$的时候RepBox Loss的性能最好， $\sigma=0$时相当于对IoU进行求和。这个可能是因为 RepGT处理的是预测和ground truth之间的重叠，这种情形下outliers其实很少，所以对outliers的抑制较大， 而RepBox处理的框更多，显然outliers也更多，这时候因为都是predict所以不知道之间哪个更可靠，就惩罚的少一点。</p><p>下图左给出了NMS在FPPI=0.01时不同阈值下，对应Miss Rate的变化，发现RepBox Loss 相对于baseline 对NMS的阈值更加鲁棒。下图右给的例子中显示的是在Crowd中也很少有predict处于两个ground-truth box之间，这样对NMS的阈值依赖更低.</p><p><img src="/2019/05/21/repulsion-loss/RepBoxLoss.png" alt="repboxloss"><img src="/2019/05/21/repulsion-loss/visualize.png" alt></p></li><li><p>Balance of RepGT and RepBox</p><p>Table 2给出了不同的$\alpha, \beta$下RepGT和RepBox Loss的作用，实验表明$\alpha=0.5, \beta=0.5$时性能做好</p></li></ul><p><strong>Comparisons with state-of-the-art methods</strong></p><p><img src="/2019/05/21/repulsion-loss/table3.png" alt="table3"></p><p>其中Reasonable set： $occlusion\le 35\%$;   Partial set: $10\% \lt occlusion \le 35\%$; Bare set: $occlusion \le 10\%$ Heavy set: $occlusion\gt 35\%$  。图8给出了具体的MR和FPPI的曲线图， 右上角的label对应IoU=0.5下的MR</p><p><img src="/2019/05/21/repulsion-loss/table4.png" alt></p><p><img src="/2019/05/21/repulsion-loss/table5.png" alt></p><p><img src="/2019/05/21/repulsion-loss/fig8.png" alt> </p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>本文提出的loss其本质上创新性并不是很强，但作者针对于特定的问题提出了解决办法，并实验证明了算法有效。这点还是值得借鉴的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Pedestrian Detection </tag>
            
            <tag> Repulsion Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多目标跟踪总结(下)-资源汇总</title>
      <link href="/2019/05/21/MOT-overview-3rd/"/>
      <url>/2019/05/21/MOT-overview-3rd/</url>
      
        <content type="html"><![CDATA[<p>本文主要收集MOT领域的一些资源， 包括数据集，相关论文以及部分开源代码等。</p><a id="more"></a><h3 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h3><p>本文主要收集MOT领域的一些资源， 包括数据集，相关论文以及部分开源代码等。</p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><div class="table-container"><table><thead><tr><th>Dataset</th><th style="text-align:center">MultiView</th><th style="text-align:center">Groundtruth</th><th>Site</th></tr></thead><tbody><tr><td>PETS2006</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\times$</td><td><a href="www.cvg.rdg.ac.uk/PETS2006/data.html">www.cvg.rdg.ac.uk/PETS2006/data.html</a></td></tr><tr><td>PETS2007</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="http://www.cvg.reading.ac.uk/PETS2007/" target="_blank" rel="noopener">http://www.cvg.reading.ac.uk/PETS2007/</a></td></tr><tr><td>PETS2009</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.cvg.rdg.ac.uk/PETS2009/a.html">www.cvg.rdg.ac.uk/PETS2009/a.html</a></td></tr><tr><td>CAVIAR</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/" target="_blank" rel="noopener">http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/</a></td></tr><tr><td>TUD</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.d2.mpi-inf.mpg.de/datasets">www.d2.mpi-inf.mpg.de/datasets</a></td></tr><tr><td>TRECVID</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\times$</td><td><a href="http://www-nlpir-nist.gov/projects/" target="_blank" rel="noopener">http://www-nlpir-nist.gov/projects/</a></td></tr><tr><td>Caltech Pedestrian</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/">www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/</a></td></tr><tr><td>UBC Hockey</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="www.cs.ubc.ca/~okumak/research.html">www.cs.ubc.ca/~okumak/research.html</a></td></tr><tr><td>AVSS 2007</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.eecs.qmul.ac.uk/~andrea/avss2007_d.html">www.eecs.qmul.ac.uk/~andrea/avss2007_d.html</a></td></tr><tr><td>ETH pedestrian</td><td style="text-align:center">$\checkmark$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.vision.ee.ethz.ch/~aess/dataset/">www.vision.ee.ethz.ch/~aess/dataset/</a></td></tr><tr><td>ETHZ Central</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.vision.ee.ethz.ch/datasets">www.vision.ee.ethz.ch/datasets</a></td></tr><tr><td>Town Centre</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.htnl#datasets">www.robots.ox.ac.uk/ActiveVision/Research/Projects/2009bbenfold_headpose/project.htnl#datasets</a></td></tr><tr><td>Zara</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data" target="_blank" rel="noopener">https://graphics.cs.ucy.ac.cy/research/downloads/crowd-data</a></td></tr><tr><td>UCSD</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm" target="_blank" rel="noopener">http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm</a></td></tr><tr><td>UCF Crowds</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\times$</td><td><a href="www.crcv.ucf.edu/data/crowd.php">www.crcv.ucf.edu/data/crowd.php</a></td></tr><tr><td>KITTI</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">http://www.cvlibs.net/datasets/kitti/eval_tracking.php</a></td></tr><tr><td>MOT2015</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/2D_MOT_2015/" target="_blank" rel="noopener">https://motchallenge.net/data/2D_MOT_2015/</a></td></tr><tr><td>MOT2016</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/MOT16/" target="_blank" rel="noopener">https://motchallenge.net/data/MOT16/</a></td></tr><tr><td>MOT2017</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/MOT17/" target="_blank" rel="noopener">https://motchallenge.net/data/MOT17/</a></td></tr><tr><td>MOT2019</td><td style="text-align:center">$\times$</td><td style="text-align:center">$\checkmark$</td><td><a href="https://motchallenge.net/data/CVPR_2019_Tracking_Challenge/" target="_blank" rel="noopener">https://motchallenge.net/data/CVPR_2019_Tracking_Challenge/</a></td></tr></tbody></table></div><p>目前多目标跟踪主要使用的数据集是MOTChallenge数据集， 包括MOT15， MOT16， MOT17和MOT19.</p><h3 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h3><blockquote><p> <strong>Evaluation Metric</strong> </p></blockquote><p><strong>CLEAR MOT</strong> :       Bernardin, K. &amp; Stiefelhagen, R. “Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metric”  <a href="https://cvhci.anthropomatik.kit.edu/images/stories/msmmi/papers/eurasip2008.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>IDF1</strong>:                       Ristani, E., Solera, F., Zou, R., Cucchiara, R. &amp; Tomasi, C. “Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking” <a href="https://users.cs.duke.edu/~ristani/bmtt2016/ristani2016MTMC.pdf" target="_blank" rel="noopener">paper</a>       </p><p><strong>MTMCT</strong>                  Ristani, E., Solera, F., Zou, R. S., Cucchiara, R., &amp; Tomasi, C. (2016). Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking. <a href="doi.org/10.1007/978-3-319-48881-3_2">paper</a></p><p><strong>MOT15</strong>:                   Leal-Taixé L, Milan A, Reid I, et al. Motchallenge 2015: Towards a benchmark for multi-target tracking . <a href="https://arxiv.org/abs/1504.01942" target="_blank" rel="noopener">paper</a>                   </p><p><strong>MOT16</strong> :                  Milan A, Leal-Taixé L, Reid I, et al. MOT16: A benchmark for multi-object tracking. <a href="https://arxiv.org/pdf/1603.00831" target="_blank" rel="noopener">paper</a></p><p><strong>Evaluation Code</strong>: <a href="https://bitbucket.org/amilan/motchallenge-devkit/" target="_blank" rel="noopener">matlab</a>, <a href="https://github.com/cheind/py-motmetrics" target="_blank" rel="noopener">python</a></p><blockquote><p><strong>Overview</strong> </p></blockquote><p>Emami, P., Pardalos, P. M., Elefteriadou, L., &amp; Ranka, S. (2018). Machine Learning Methods for Solving Assignment Problems in Multi-Target Tracking, 1(1), 1–35. <a href="arxiv.org/abs/1802.06897">paper</a></p><p>Leal-Taixé, L., Milan, A., Schindler, K., Cremers, D., Reid, I., &amp; Roth, S. (2017). Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking. <a href="arxiv.org/abs/1704.0278">paper</a></p><p>Luo, W., Xing, J., Milan, A., Zhang, X., Liu, W., Zhao, X., &amp; Kim, T.-K. (2014). Multiple Object Tracking: A Literature Review, 1–18.  <a href="arxiv.org/abs/1409.7618">paper</a></p><p>Li, X., Hu, W., Shen, C., Zhang, Z., &amp; Dick, A. (2013). A Survey of Appearance Models in Visual Object Tracking, 1–42. <a href="arxiv.org/pdf/1303.4803">paper</a></p><p>Poore, A. B., &amp; Gadaleta, S. (2006). Some assignment problems arising from multiple target tracking, 43, 1074–1091. <a href="doi.org/10.1016/j.mcm.2">paper</a></p><p>Yilmaz, A., &amp; Javed, O. (2006). Object Tracking : A Survey, 38(4).  <a href="doi.org/10.1145/1177352">paper</a></p><p>A 101 slide . <a href="http://vision.stanford.edu/teaching/cs231b_spring1415/slides/greedy_fahim_albert.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p><strong>2019</strong> </p></blockquote><p><strong>NT</strong>:                 Longyin Wen<em>, Dawei Du</em>, Shengkun Li, Xiao Bian, Siwei Lyu Learning Non-Uniform Hypergraph for Multi-Object Tracking, In AAAI 2019.  <a href="http://www.cs.albany.edu/~lsw/papers/aaai19a.pdf  from  github.com/longyin880815" target="_blank" rel="noopener">paper</a></p><p><strong>FMA</strong>:           Zhang, J., Zhou, S., Wang, J., &amp; Huang, D. (2019). Frame-wise Motion and Appearance for Real-time Multiple Object Tracking, (1).  <a href="arxiv.org/abs/1905.02292">paper</a></p><p><strong>STRN</strong>:           Xu, J., Cao, Y., Zhang, Z., &amp; Hu, H. (2019). Spatial-Temporal Relation Networks for Multi-Object Tracking.  <a href="arxiv.org/abs/1904.11489">paper</a></p><p><strong>LSST</strong>:             Feng, W., Hu, Z., Wu, W., Yan, J., &amp; Ouyang, W. (2019). Multi-Object Tracking with Multiple Cues and Switcher-Aware Classification.  <a href="arxiv.org/abs/1901.06129">paper</a></p><p><strong>MOTS</strong>:            Voigtlaender, P., Krause, M., Osep, A., Luiten, J., Sekar, B. B. G., Geiger, A., &amp; Leibe, B. (2019). MOTS: Multi-Object Tracking and Segmentation.  <a href="arxiv.org/abs/1902.03604">paper</a></p><p><strong>FAMNet</strong>:     Chu, P., &amp; Ling, H. (2019). FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional Assignment for Online Multiple Object Tracking. <a href="arxiv.org/abs/1904.04989">paper</a></p><p><strong>FANTrack</strong>: Baser, E., Balasubramanian, V., Bhattacharyya, P., &amp; Czarnecki, K. (2019). FANTrack: 3D Multi-Object Tracking with Feature Association Network.  <a href="https://arxiv.org/abs/1905.02843" target="_blank" rel="noopener">paper</a>, <a href="https://git.uwaterloo.ca/wise-lab/fantrack" target="_blank" rel="noopener">code</a></p><p><strong>IATracker</strong>:   Chu, P., Fan, H., Tan, C. C., &amp; Ling, H. (2019). Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic Model Refreshment.  <a href="arxiv.org/abs/1902.08231">paper</a></p><blockquote><p><strong>2018</strong> </p></blockquote><p><strong>SST</strong>:                 Sun. S., Akhtar, N., Song, H., Mian A., &amp; Shah M. (2018). Deep Affinity Network for Multiple Object Tracking.  <a href="https://arxiv.org/abs/1810.11780" target="_blank" rel="noopener">paper</a>, <a href="https://github.com/shijieS/SST" target="_blank" rel="noopener">code</a></p><p><strong>CCC</strong>:                 Keuper, M., Tang, S., Andres, B., Brox, T., &amp; Schiele, B. (2018). Motion Segmentation &amp; Multiple Object Tracking by Correlation Co-Clustering. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>8828</em>(c), 1–13.   <a href="doi.org/10.1109/TPAMI.2018.2876253">paper</a></p><p><strong>HAF</strong>:                 Sheng, H., Zhang, Y., Chen, J., Xiong, Z., &amp; Zhang, J. (2018). Heterogeneous Association Graph Fusion for Target Association in Multiple Object Tracking. IEEE Transactions on Circuits and Systems for Video Technology. <a href="doi.org/10.1109/TCSVT.2018.2882192">paper</a></p><p><strong>TNT</strong>:               Wang, G., Wang, Y., Zhang, H., Gu, R., &amp; Hwang, J.-N. (2018). Exploit the Connectivity: Multi-Object Tracking with TrackletNet.  <a href="arxiv.org/abs/1811.07258">paper</a></p><p><strong>PHD</strong>:              Fang, K., Xiang, Y., Li, X., &amp; Savarese, S. (2018). Recurrent Autoregressive Networks for Online Multi-Object Tracking. <em>WACV</em>.   <a href="yuxng.github.io/fang_wacv18.pdf">paper</a></p><p><strong>DMAN</strong>:            Zhu, Ji and Yang, Hua and Liu, Nian and Kim, Minyoung and Zhang, Wenjun and Yang, Ming-Hsuan “Online Multi-Object Tracking with Dual Matching Attention Networks”   <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ji_Zhu_Online_Multi-Object_Tracking_ECCV_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>C-DRL</strong>:             Ren, Liangliang and Lu, Jiwen and Wang, Zifeng and Tian, Qi and Zhou, Jie “Collaborative Deep Reinforcement Learning for Multi-Object Tracking” <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Liangliang_Ren_Collaborative_Deep_Reinforcement_ECCV_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>SADF</strong>:             Yoon, Y., Boragule, A., Song, Y., Yoon, K., &amp; Jeon, M. (2018). Online Multi-Object Tracking with Historical Appearance Matching and Scene Adaptive Detection Filtering.  <a href="ieeexplore.ieee.org/document/8639078">paper</a></p><p><strong>MOTDT</strong>:           Long Chen, Haizhou Ai “Real-time Multiple People Tracking with Deeply Learned Candidate Selection and Person Re-identification” in ICME 2018.     <a href="https://www.researchgate.net/publication/326224594_Real-time_Multiple_People_Tracking_with_Deeply_Learned_Candidate_Selection_and_Person_Re-identification" target="_blank" rel="noopener">paper</a>, <a href="https://github.com/longcw/MOTDT" target="_blank" rel="noopener">code</a></p><p><strong>DeepCC</strong>:          Ristani and C. Tomasi “Features for Multi-Target Multi-Camera Tracking and Re-Identification” In CVPR 2018 <a href="https://arxiv.org/pdf/1803.10859.pdf" target="_blank" rel="noopener">paper</a>,  <a href="https://github.com/ergysr/DeepCC" target="_blank" rel="noopener">code</a></p><p><strong>THOPA-net</strong>:     Fabbri, M., Lanzi, F., Calderara, S., &amp; Vezzani, R. (2018). Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World. <a href="researchgate.net/publication/323957071_Learning_to_Detect_and_Track_Visible_and_Occluded_Body_Joints_in_a_Virtual_World">paper</a></p><p><strong>MHT-bLSTM</strong>:  Kim, Chanho and Li, Fuxin and Rehg, James M “Multi-object Tracking with Neural Gating Using Bilinear LSTM” .  <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Chanho_Kim_Multi-object_Tracking_with_ECCV_2018_paper.pdf" target="_blank" rel="noopener">paper</a></p><p><strong>Trajectory Factory</strong>: Cong Ma, Changshui Yang, Fan Yang, Yueqing Zhuang, Ziwei Zhang, Huizhu Jia, Xiaodong Xie “Trajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese Bi-GRU for Multiple Object Tracking” In ICME 2018.   <a href="https://arxiv.org/abs/1804.04555" target="_blank" rel="noopener">paper</a></p><p><strong>MOTBeyondPixels</strong>: Sarthak Sharma,  Junaid Ahmed Ansari,  J. Krishna Murthy,  and K. Madhava Krishna Beyond Pixels: Leveraging Geometry and Shape Cues for Online Multi-Object Tracking In ICRA 2018 <a href="https://arxiv.org/abs/1802.09298" target="_blank" rel="noopener">paper</a>,  <a href="https://github.com/JunaidCS032/MOTBeyondPixels" target="_blank" rel="noopener">code</a></p><p>Henschel, R., Leal-Taixe, L., Cremers, D., &amp; Rosenhahn, B. (2018). Fusion of head and full-body detectors for multi-object tracking. <em>IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</em>, <em>2018</em>–<em>June</em>, 1509–1518.   <a href="doi.org/10.1109/CVPRW.2018.00192">paper</a></p><p>Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes “Tracking by Prediction: A Deep Generative Model for Mutli-Person localisation and Tracking” In WACV 2018. <a href="https://arxiv.org/pdf/1803.03347.pdf" target="_blank" rel="noopener">paper</a></p><blockquote><p><strong>2017</strong></p></blockquote><p><strong>D2T</strong>:                  Feichtenhofer, C., Pinz, A., &amp; Zisserman, A. (2017). Detect to Track and Track to Detect. In ICCV2017.  <a href="doi.org/10.1109/ICCV.2017.330">paper</a>, <a href="github.com/feichtenhofer/Detect-Track">code</a></p><p><strong>IOU</strong>:                  Bochinski, E., Eiselein, V., &amp; Sikora, T. (2017). High-Speed tracking-by-detection without using image information. <em>AVSS 2017</em>. <a href="doi.org/10.1109/AVSS.2017.8078516">paper</a>, <a href="github.com/bochinski/iou-tracker/">code</a></p><p><strong>CIWT</strong>:                Aljosa Osep, Alexander Hermans Combined Image and World-Space Tracking in Traffic Scenes. In ICRA 2017.  <a href="vision.rwth-aachen.de/media/papers/paper_final_compressed.pdf">paper</a>,  <a href="github.com/aljosaosep/ciwt">code</a></p><p><strong>RCMSS</strong>:              Naiel, M. A., Ahmad, M. O., Swamy, M. N. S., Lim, J., &amp; Yang, M. H. (2017). Online multi-object tracking via robust collaborative model and sample selection. In CVIU 2017.  <a href="doi.org/10.1016/j.cviu.2016.07.003">paper</a>, <a href="users.encs.concordia.ca/~rcmss/">code</a></p><p><strong>EAMTT</strong>:              Tang, S., Andriluka, M., Andres, B., &amp; Schiele, B. (2017). Multiple people tracking by lifted multicut and person re-identification.  In CVPR 2017. <a href="doi.org/10.1109/CVPR.2017.394">paper</a></p><p><strong>STAM</strong>:                 Chu, Q., Ouyang, W., Li, H., Wang, X., Liu, B., &amp; Yu, N. (2017). Online Multi-object Tracking Using CNN-Based Single Object Tracker with Spatial-Temporal Attention Mechanism.  In ICCV2017. <a href="doi.org/10.1109/ICCV.2017.518">paper</a></p><p><strong>DeepSORT</strong>:         Wojke, N., Bewley, A., &amp; Paulus, D. (2017). Simple Online and Realtime Tracking with a Deep Association Metric.  In ICIP2017.  <a href="doi.org/10.1109/ICIP.2017.8296962">paper</a>, <a href="github.com/nwojke/deep_sort">code</a> </p><p><strong>Quad-CNN</strong>:         Son, J., Baek, M., Cho, M., &amp; Han, B. (2017). Multi-object tracking with quadruplet convolutional neural networks. In CVPR2017.  <a href="doi.org/10.1109/CVPR.2017.403">paper</a></p><p><strong>Art-Tracker</strong>:       Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, Bernt Schiele “Art Track: Articulated Multi-Person Tracking in the Wild”  In CVPR2017. <a href="https://arxiv.org/abs/1612.01465" target="_blank" rel="noopener">paper</a></p><p><strong>SOTforMOT</strong>:         He, Q., Wu, J., Yu, G., &amp; Zhang, C. (2017). SOT for MOT.  <a href="arxiv.org/abs/1712.01059">paper</a></p><p><strong>NMGC-MOT</strong>:         Maksai, A., Wang, X., Fleuret, F., &amp; Fua, P. (2017). Non-Markovian Globally Consistent Multi-Object Tracking.  In ICCV2017.  <a href="openaccess.thecvf.com/content_ICCV_2017/papers/Maksai_Non-Markovian_Globally_Consistent_ICCV_2017_paper.pdf">paper</a>  , <a href="github.com/maksay/ptrack_cpp">code</a></p><p><strong>RNN_LSTM</strong>:          Milan, A., Rezatofighi, S. H., Dick, A., Reid, I., &amp; Schindler, K. (2017). Online Multi-Target Tracking Using Recurrent Neural Networks.  AAAI 2017.  <a href="arxiv.org/abs/1604.03635">paper</a>,  <a href="bitbucket.org/amilan/rnntracking">code</a></p><p><strong>ReidTracking</strong>:               Beyer, L., Breuers, S., Kurin, V., &amp; Leibe, B. (2017). Towards a Principled Integration of Multi-Camera Re-Identification and Tracking through Optimal Bayes Filters.  <a href="arxiv.org/abs/1705.04608">paper</a>, <a href="github.com/VisualComputingInstitute/towards-reid-tracking">code</a></p><p><strong>DeepNetworkFlows</strong>:  Schulter, S., Vernaza, P., Choi, W., &amp; Chandraker, M. (2017). Deep network flow for multi-object tracking.  In CVPR 2017.  <a href="doi.org/10.1109/CVPR.2017.292">paper</a></p><p>Sadeghian, A., Alahi, A., &amp; Savarese, S. (2017). Tracking the Untrackable: Learning to Track Multiple Cues with Long-Term Dependencies.  In ICCV2017. <a href="doi.org/10.1109/ICCV.2017.41">paper</a></p><blockquote><p><strong>2016</strong></p></blockquote><p><strong>CPD</strong>:                       Lee, B., Erdenee, E., Jin, S., &amp; Rhee, P. K. (2016). Multi-Class Multi-Object Tracking using Changing Point Detection.  <a href="doi.org/10.1007/978-3-319-48881-3">paper</a></p><p><strong>POI</strong> :                        Yu, F., Li, W., Li, Q., Liu, Y., Shi, X., &amp; Yan, J. (2016). POI: Multiple Object Tracking with High Performance Detection and Appearance Feature.  In BMTT 2016.  <a href="https://arxiv.org/pdf/1610.06136.pdf" target="_blank" rel="noopener">paper</a>,  <a href="https://drive.google.com/open?id=0B5ACiy41McAHMjczS2p0dFg3emM" target="_blank" rel="noopener">detections</a></p><p><strong>SORT</strong>:                      Bewley, A., Ge, Z., Ott, L., Ramos, F., &amp; Upcroft, B. (2016). Simple online and realtime tracking.   In ICIP 2016.  <a href="doi.org/10.1109/ICIP.2016.7533003">paper</a>, <a href="github.com/abewley/sort">code</a></p><p><strong>RCMSS</strong> :                  Mohamed A. Naiel1, M. Omair Ahmad, M.N.S. Swamy, Jongwoo Lim, and Ming-Hsuan Yang “Online Multi-Object Tracking Via Robust Collaborative Model and Sample Selection. In CVIU2016. <a href="https://users.encs.concordia.ca/~rcmss/include/Papers/CVIU2016.pdf" target="_blank" rel="noopener">paper</a>, <a href="https://users.encs.concordia.ca/~rcmss/" target="_blank" rel="noopener">code</a> </p><p><strong>Social-LSTM</strong>:          Goel, K., Fei-Fei, L., Savarese, S., Alahi, A., Robicquet, A., &amp; Ramanathan, V. (2016). Social LSTM: Human Trajectory Prediction in Crowded Spaces.  In CVPR2016.  <a href="doi.org/10.1109/cvpr.2016.110">paper</a>, </p><blockquote><p><strong>2015</strong></p></blockquote><p><strong>MDP</strong>:                      Xiang, Y., Alahi, A., &amp; Savarese, S. (2015). Learning to Track: Online Multi-object Tracking by Decision Making.  In ICCV2015.  <a href="doi.org/10.1109/ICCV.2015.534">paper</a>, <a href="cvgl.stanford.edu/projects/MDP_tracking/">code</a></p><p><strong>CEM</strong>:                       Chari, V., Lacoste-Julien, S., Laptev, I., &amp; Sivic, J. (2014). On Pairwise Costs for Network Flow Multi-Object Tracking.  In CVPR2015.  <a href="arxiv.org/abs/1408.3304">paper</a>, <a href="milanton.de/contracking/">code</a> </p><p><strong>ALFD</strong>:                      Choi, W. (2015). Near-online multi-target tracking with aggregated local flow descriptor. In ICCV2015. <a href="doi.org/10.1109/ICCV.2015.347">paper</a> </p><p><strong>LDCT</strong>:                      Solera, F. (2015). Learning to Divide and Conquer for Online Multi-Target Tracking. In ICCV 2105.  <a href="imagelab.ing.unimore.it/imagelab/researchActivity.asp?idActivity=09">paper</a>, <a href="github.com/francescosolera/LDCT">code</a></p><p><strong>TMPORT</strong>:               Ristani, E., &amp; Tomasi, C. (2015). Tracking multiple people online and in real time. <a href="doi.org/10.1007/978-3-319-16814-2_29">paper</a>, <a href="vision.cs.duke.edu/DukeMTMC/">code</a></p><p><strong>JPDArevisited</strong>:     Rezatofighi, S. H., Milan, A., Zhang, Z., Shi, Q., Dick, A., &amp; Reid, I. (2015). Modified Joint Probabilistic Data Association.  In ICCV2015.  <a href="doi.org/10.1109/ICCV.2015.349">paper</a></p><p><strong>MHTrevisited</strong>:      Vinet, L., &amp; Zhedanov, A. (2015). Multiple Hypothesis Tracking Revisited. In ICCV2015. <a href="doi.org/10.1088/1751-8113/44/8/085201">paper</a>, <a href="rehg.org/mht/">code</a></p><p><strong>headTracking</strong>:     Zhang, S., Wang, J., Wang, Z., Gong, Y., &amp; Liu, Y. (2015). Multi-target tracking by learning local-to-global trajectory models.  In PR, 48(2), 580-590.  <a href="doi.org/10.1016/j.patcog.2014.08.013">paper</a>,  <a href="github.com/gengshan-y/headTracking">code</a></p><blockquote><p><strong>2014</strong></p></blockquote><p><strong>H2T</strong>:                         Wen, L., Li, W., Yan, J., Lei, Z., Yi, D., &amp; Li, S. Z. (2014). Multiple target tracking based on undirected hierarchical relation hypergraph.  In CSC on CVPR2014.  <a href="doi.org/10.1109/CVPR.2014.167">paper</a>,  <a href="cbsr.ia.ac.cn/users/lywen/">code</a></p><p><strong>CMOT</strong>:                     Bae, S. H., &amp; Yoon, K. J. (2014). Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning.  In CSC on CVPR2014.  <a href="doi.org/10.1109/CVPR.2014.159">paper</a>, <a href="cvl.gist.ac.kr/project/cmot.html">code</a></p><p><strong>OPCNF</strong>:                  Chari, V., Lacoste-Julien, S., Laptev, I., &amp; Sivic, J. (2014). Continuous Energy Minimization for Multi-Target Tracking,  TPAMI 2014.  <a href="milanton.de/files/pami2014/pami2014-anton.pdf">paper</a>,  <a href="di.ens.fr/willow/research/flowtrack/">code</a></p><p>Tang, S., Andriluka, M., &amp; Schiele, B. (2014). Detection and tracking of occluded people. IJCV, <em>110</em>(1), 58–69.   <a href="doi.org/10.1007/s11263-013-0664-6">paper</a></p><p>Yang, B., &amp; Nevatia, R. (2014). Multi-target tracking by online learning a CRF model of appearance and motion patterns.  IJCV, <em>107</em>(2), 203–217.  <a href="doi.org/10.1007/s11263-013-0666-4">paper</a></p><blockquote><p><strong>2013</strong></p></blockquote><p><strong>SMOT</strong>:                      Dicle, C., Camps, O. I., &amp; Sznaier, M. (2013). The way they move: Tracking multiple targets with similar appearance.  In ICCV2013.  <a href="doi.org/10.1109/ICCV.2013.286">paper</a>,, <a href="bitbucket.org/cdicle/smot">code</a></p><p>Milan, A., Schindler, K., &amp; Roth, S. (2013). Detection- and trajectory-level exclusion in multiple object tracking.  In CSC on CVPR2013.  <a href="doi.org/10.1109/CVPR.2013.472">paper</a></p><p>Salvi, D., Waggoner, J., Temlyakov, A., &amp; Wang, S. (2013). A graph-based algorithm for multi-target tracking with occlusion. In WACV 2013. <a href="doi.org/10.1109/WACV.2013.6475059">paper</a></p><blockquote><p><strong>2012</strong></p></blockquote><p><strong>GMCP-Tracker</strong>:          Zamir, A. R., Dehghan, A., &amp; Shah, M. (2012). GMCP-Tracker : Global Multi-object Tracking Using Generalized Minimum Clique Graphs, 343–356. <a href="crcv.ucf.edu/papers/eccv2012/GMCP-Tracker_ECCV12.pdf">paper</a>,  <a href="crcv.ucf.edu/projects/GMCP-Tracker/">code</a></p><p><strong>OMPTTH</strong>:                     Zhang, J., Lo Presti, L., &amp; Sclaroff, S. (2012). Online multi-person tracking by tracker hierarchy. In AVSS 2012. <a href="doi.org/10.1109/AVSS.2012.51">paper</a>, <a href="cs-people.bu.edu/jmzhang/tracker_hierarchy/Tracker_Hierarchy.htm">code</a> </p><p>Yan, X., Wu, X., Kakadiaris, I. A., &amp; Shah, S. K. (2012). To Track or To Detect ? An Ensemble Framework for Optimal Selection, 594–607. <a href="link.springer.com/conter/10.1007%2F978-3-642-33715-4_43">paper</a></p><p>Hu, W., Li, X., Luo, W., Zhang, X., Maybank, S., &amp; Zhang, Z. (2012). Single and multiple object tracking using log-euclidean riemannian subspace and block-division appearance model. PAMI, 34(12), 2420-2440. <a href="doi.org/10.1109/TPAMI.2012.42">paper</a></p><p>Yang, B., &amp; Nevatia, R. (2012). Online learned discriminative part-based appearance models for multi-human tracking. In ECCV 2012. <a href="doi.org/10.1007/978-3-642-33718-5_35">paper</a></p><p>Shu, G., Dehghan, A., Oreifej, O., Hand, E., &amp; Shah, M. (2012). Part-based multiple-person tracking with partial occlusion handling.  In CSC on CVPR2012. <a href="doi.org/10.1109/CVPR.2012.6247879">paper</a></p><blockquote><p><strong>2011 and Before</strong></p></blockquote><p><strong>KSP</strong>:                               Berclaz. (2011). Multiple Object Tracking using K-shortes Paths.  PAMI2011. <a href="cvlab.epfl.ch/files/content/sites/cvlab2/files/publications/publications/2011/BerclazFTF11.pdf">paper</a>, <a href="cvlab.epfl.ch/software/ksp">code</a> </p><p><strong>MTDF</strong>:                           Pedro F. Felzenszwalb, Ross B. Girshick, D. M. and D. R. (2010). Object detection with discriminatively trained part-based models. in TPAMI 2010. <a href="doi.org/10.1109/MC.2014.42">paper</a></p><p>Andriyenko, A., Roth, S., &amp; Schindler, K. (2011). An analytical formulation of global occlusion reasoning for multi-target tracking.  ICCV 2011.  <a href="doi.org/10.1109/ICCVW.2011.6130472">paper</a></p><p>Andriyenko, A., &amp; Schindler, K. (2011). Multi-target tracking by continuous energy minimization.  In CVPR 2011. <a href="doi.org/10.1109/CVPR.2011.5995311">paper</a></p><p>Pirsiavash, H., Ramanan, D., &amp; Fowlkes, C. (2011). Globally-Optimal Greedy Algorithms for Tracking a Variable Number of Objects.  CVPR2011. <a href="people.csail.mit.edu/hpirsiav/papers/tracking_cvpr11.pdf">paper</a></p><p>Mitzel, D., Horbert, E., Ess, A., &amp; Leibe, B. (2010). Multi-person tracking with sparse detection and continuous segmentation.  ECCV2010.  <a href="doi.org/10.1007/978-3-642-15549-9_29">paper</a></p><p>Hu, M., Ali, S., &amp; Shah, M.  Detecting global motion patterns in complex videos. ICPR2008. <a href="doi.org/10.1109/icpr.2008.4760950">paper</a></p><p>Breitenstein, M. D., Reichlin, F., Leibe, B., Koller-Meier, E., &amp; Van Gool, L. (2009). Robust tracking-by-detection using a detector confidence particle filter. ICCV2009.  <a href="doi.org/10.1109/ICCV.2009.5459278">paper</a></p><p>Zhang, L., Li, Y., &amp; Nevatia, R. (2008). Global data association for multi-object tracking using network flows. CVPR2008. <a href="doi.org/10.1109/CVPR.2008.4587584">paper</a></p><h3 id="Other-resources"><a href="#Other-resources" class="headerlink" title="Other resources"></a>Other resources</h3><p><a href="http://bbs.cvmart.net/articles/265/zi-yuan-duo-mu-biao-zhui-zong-zi-yuan-lie-biao-shu-ju-ji-lun-wen-dai-ma-he-niu-ren-zhu-ye-deng" target="_blank" rel="noopener">SpyderXu</a>,  <a href="https://github.com/SpyderXu/multi-object-tracking-paper-list" target="_blank" rel="noopener">github</a> , <a href="github.com/huanglianghua/mot-papers/blob/master/README.md">github</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> overview </tag>
            
            <tag> code and paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多目标跟踪总结(中)-深度方法</title>
      <link href="/2019/05/20/MOT-overview-2nd/"/>
      <url>/2019/05/20/MOT-overview-2nd/</url>
      
        <content type="html"><![CDATA[<p>随着深度学习的推进，基于深度学习的检测器性能提升明显。因此目前的多目标跟踪算法大都基于tracking-by-detection框架。于是MOT任务可以转化为detection+ReID问题。</p><p>但相对于与传统的ReID问题，MOT问题会更加复杂。首先，MOT任务中目标轨迹变化频繁， 图像样本库的数量和种类都不固定； 其次，检测结果可能出现新的目标，也可能出现漏检；另外，检测图像并不像行人重识别中的查询图像都是比较准确地检测结果， MOT任务中行人检测往往混杂着误检或者不准确的检测，尤其是相互遮挡产生时。检测行人的不对齐，相互遮挡给目标匹配带来了极大的挑战性。</p><p>本文将主要总结一些基于深度网络，希望更好解决MOT任务中ReID子任务的方法。</p><a id="more"></a><h3 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h3><p>随着深度学习的推进，基于深度学习的检测器性能提升明显。因此目前的多目标跟踪算法大都基于tracking-by-detection框架。于是MOT任务可以转化为detection+ReID问题。</p><p>但相对于与传统的ReID问题，MOT问题会更加复杂。首先，MOT任务中目标轨迹变化频繁， 图像样本库的数量和种类都不固定； 其次，检测结果可能出现新的目标，也可能出现漏检；另外，检测图像并不像行人重识别中的查询图像都是比较准确地检测结果， MOT任务中行人检测往往混杂着误检或者不准确的检测，尤其是相互遮挡产生时。检测行人的不对齐，相互遮挡给目标匹配带来了极大的挑战性。</p><p>本文将主要总结一些基于深度网络，希望更好解决MOT任务中ReID子任务的方法。</p><hr><h3 id="基于深度学习的多目标跟踪算法分类"><a href="#基于深度学习的多目标跟踪算法分类" class="headerlink" title="基于深度学习的多目标跟踪算法分类"></a>基于深度学习的多目标跟踪算法分类</h3><p>基于深度学习的多目标跟踪算法的主要任务是优化检测之间相似性或距离度量的设计。根据 学习特征的不同，基于深度学习的多目标跟踪可以分为表观特征的深度学习，基于相似性度 量的深度学习，以及基于高阶匹配特征的深度学习。</p><p><img src="/2019/05/20/MOT-overview-2nd/catagetory.png" alt="catagetory"></p><ol><li>利用深度网络学习目标检测的表观鉴别特征能有效提升匹配跟踪性能。比如利用图像识别或者行人重识别中的特征抽取网络替换MOT框架中的hand-craft表观特征，或者基于深度网络的光流特征计算运动相关性。</li><li>利用深度学习方法进行度量学习。比如设计网络计算检测和轨迹的距离函数，或者设计二分类代价函数，判断目标是否属于同一类。</li><li>基于深度网络学习高阶特征。比如考虑以跟踪轨迹和检测之间的匹配或者tracklets之间的匹配等。深度网络学习高阶特征匹配可以学习多帧表观特征的高阶匹配相似性，也可以学习运动特征的匹配相关度。</li></ol><h3 id="深度视觉多目标跟踪算法介绍"><a href="#深度视觉多目标跟踪算法介绍" class="headerlink" title="深度视觉多目标跟踪算法介绍"></a>深度视觉多目标跟踪算法介绍</h3><h4 id="基于Siamese网络结构的跟踪算法"><a href="#基于Siamese网络结构的跟踪算法" class="headerlink" title="基于Siamese网络结构的跟踪算法"></a>基于Siamese网络结构的跟踪算法</h4><p>该类方法以两个经crop之后尺寸相同的检测图像块作为输入，输出的是二分类判别是否属于同一类。 一般Siamese结构有三种形式。如下所示：</p><p><img src="/2019/05/20/MOT-overview-2nd/siamese.png" alt="siamese"></p><p>一般而言第一种方式属于特征学习，第二三两种属于度量学习。实验表明，第三种拓扑结构能够生成更好地实验效果。所以Lealtaixe等人采用第三种形式结构计算检测之间的匹配度。原始的检测经过预处理成对和光流对的输入网络中。这些预处理包括正则化LUV空间，resize到固定大小$121\times 53$等。</p><p>网络的输入层通道数为$10$, 后面跟有$3$个卷积层，$4$个全连接层和binary-softmax损失层。损失函数为</p><script type="math/tex; mode=display">E=\frac{1}{2N}\sum_{n=1}^N(y\Phi(d_1, d_2)+(1-y)\max(\tau-\Phi(d_1, d_2), 0))</script><p>训练过程中，从真实跟踪数据中抽取训练样本。利用检测算法得到的同一条轨迹的检测作为正样本，不同轨迹的检测作为负样本，为了增加样本多样性，增强模型的泛化能力， 负样本还包括从检测响应周围随机采集的重叠率较小的图像块。训练细节：SGD优化器， bs=128， lr=0.01, num_epoches=50</p><p>在孪生网络训练完成后， 作者采用第六层全连接网络的输出作为表观特征，然后通过gradient boosting算法融合目标的上下文信息，包括：尺寸相对变化、位置相对变化和速度相对变化。其结构图如下所示：</p><p><img src="/2019/05/20/MOT-overview-2nd/siamese2.png" alt="siamese2"></p><p>多目标跟踪过程采用全局最优算法框架，通过对任意两个检测建立连接关系，生成匹配矩阵，采用最小代价网络流的方式转化为线性规划进行求解。</p><h4 id="基于最小多个图模型的多目标跟踪算法"><a href="#基于最小多个图模型的多目标跟踪算法" class="headerlink" title="基于最小多个图模型的多目标跟踪算法"></a>基于最小多个图模型的多目标跟踪算法</h4><p>Siyu Tang等人利用深度学习计算的类似光流特征结合子图多割模型在MOT任务上取得了很好的效果。</p><p>该方法属于离线确定性推导方法。作者认为对于检测响应的处理，例如NMS过于粗糙，会导致有些正确的检测被抑制掉，同时同一帧图像中检测之间的关系也没有考虑。于是作者剔除了MOT任务中的一般假设：一条轨迹最多对应一个检测，同样一个检测最多对应一条轨迹。使用多帧之间的所有检测响应构建图模型，并提出子图划分的算法进行求解轨迹。</p><p>如下图所示, 每一个顶点对应一个检测响应，不同的颜色对应不同的目标响应，相同颜色对应相同目标的不同检测响应。</p><p><img src="/2019/05/20/MOT-overview-2nd/subgraph.png" alt="subgraph"></p><p>子图多割的模型：</p><script type="math/tex; mode=display">\min_{x\in\{0, 1\}}\sum_{e\in E}c_ex_e\\\text{s.t.}\quad \forall C\in cycles(G), \\ \forall e\in C:x_e\le\sum_{e'\in C\backslash \{e\}}x_e'</script><p>其中$c_e$表示每条边的代价， $x_e$表示每条边是否关联的示性变量， 约束条件表示对于图$G$中任意的环路$cycles(G)$之一$C$，任意两点如果存在一条通路，即不等式右边为0， 那么这两点之间必定相同，即$x_e=0$.也就是说一个环路中若存在分割边，那么至少存在两条，于是环路就可以分割成不同的图，代表不同的轨迹。该模型可以使用KLj算法求解。</p><p>在计算检测之间的关联关系时，作者利用deepmatching光流方法构建了$5$维特征。</p><script type="math/tex; mode=display">f_1 = MI/MU\\f_2 = \min(\varepsilon_v, \varepsilon_w)\\f_3 = f_1\cdot f_2~~~~~~~~~\\f_4 = f_1^2, f_5 = f_2^2</script><p>MI, MU分别表示对应响应中光流点集的交集和并集的基数比， $\varepsilon_v, \varepsilon_w$分别表示置信度。 利用5维特征学习逻辑回归分类器用于计算两者属于相同目标的概率$p_e$, 注意这里的训练是在训练集上利用特征学习回归分类器，而计算匹配概率$p_e$时则是在推断阶段。获得了匹配概率$p_e$其实已经可以采用匈牙利算法等进行匹配，但是为了实现本文的motivation， 计算(2)式中的代价$c_e=\frac{p_e}{1-p_E}$, 进而更好地计算匹配关联。</p><p>为了更好地利用长时信息，已解决遮挡或者错误关联问题，作者又在子图多割基础上提出了提升边的改进。其基本思想是将图中的连接边进一步划分为常规边和提升边，分别用来刻画短期和长期匹配关系。</p><p>示例</p><p><img src="/2019/05/20/MOT-overview-2nd/lmp.png" alt="lmp"></p><p>在子图(a),(b)中有3条真实轨迹， $v_1$对应$t$时刻的一个目标， $v_2, v_3$对应$t, t+1$时刻的同一个目标， $v_4$对应$t+1$时刻的新目标， 每条轨迹的代价如图所示，因此采用MP方法，则将$v_1v_2, v_3v_4$切断，这样就会导致$v_1, v_4$划分为一类，发生错误匹配。而对于LMP， $v_1v_4$属于提升边，提升边另外考虑，因此并不会分错。同理的是图(c)(d), 同一个目标可能因为遮挡或者光照原因，中间若干帧发生表观变化，这时候采用MP算法会导致跟踪片段， 而采用提升边方法可以有效避免这种现象。</p><p>提升边的子图多割相对于子图多割方法额外增加了两条约束</p><script type="math/tex; mode=display">\forall vw\in E'\backslash E\forall P\in vw-paths(G): x_{vw}\le \sum_{e\in P}x_e</script><p>这里$E$ 表示时间空间约束的常规边， $E’$则是常规边和提升边的合集， 提升边是指时间间隔大于某阈值，而表观非常相似的检测之间的关联。上述约束表示对于任意的两个顶点$v,w$其中存在的提升边$vw$和任意的通路$P$,那么如果常规边是连通的则提升边必定连通。</p><script type="math/tex; mode=display">\forall vw\in E' \backslash E \forall C\in vw-cuts(G): 1-x_{vw} \le \sum_{e\in C}(1-x_e)</script><p>$vw-cuts(G)$表示节点$v,w$之间所有可能存在切边的路径， 于是可以知道，如果$vw$之间不存在任何通路，则提升边必定是切边。</p><p>目标函数和MP的目标函数相同，不同的是MP构造了5维特征， 而LMP则是利用siamese结构网络提取表观特征再与MP中特征结合构建目标函数。</p><p>siamese结构的表观相似度网络</p><p><img src="/2019/05/20/MOT-overview-2nd/lmp_appearance.png" alt="lmp_appearance"></p><p>LMP算法目前在MOT16benchmark上的性能依然算是SOTA。</p><p><img src="/2019/05/20/MOT-overview-2nd/lmp_mot16.png" alt="lmp_mot16"></p><h4 id="时空注意力机制的多目标跟踪-STAM"><a href="#时空注意力机制的多目标跟踪-STAM" class="headerlink" title="时空注意力机制的多目标跟踪(STAM)"></a>时空注意力机制的多目标跟踪(STAM)</h4><p>该方法借助注意力机制处理多目标跟踪中目标遮挡问题， 并对每一个目标建立单独的分类器，判断检测和轨迹是否匹配，其本质上是单目标跟踪算法在多目标任务上的扩展。其流程图如下：</p><p><img src="/2019/05/20/MOT-overview-2nd/stam_architecture.png" alt="stam_architecture"></p><p>空间注意力机制用于给不同程度的遮挡部分不同的特征权重，从而能够更好地抽取可鉴别特征， 时间注意力机制则主要是给历史样本不同的权重用于在线更新模型。</p><p>空间注意力模型如下图中(b)所示,对于经过ROIPooling之后的对齐特征学习其每个空间位置的权重，由于训练样本不足，直接学习spatial attention map比较困难，因此作者采用了分阶段的思想。</p><p>首先希望能够回归出目标的visibility map，这部分可以提供额外的标签信息，因此学习起来相对简单，然后再有visibility map学习spatial attention map. 获得spatial attention map之后采用乘性方式叠加到池化特征上获得refined feature maps，然后从该特征训练二值分类器。</p><p><img src="/2019/05/20/MOT-overview-2nd/stam_model.png" alt="stam_model"></p><h4 id="基于LSTM的多线索融合多目标跟踪"><a href="#基于LSTM的多线索融合多目标跟踪" class="headerlink" title="基于LSTM的多线索融合多目标跟踪"></a>基于LSTM的多线索融合多目标跟踪</h4><p>前面叙述的方法很少利用时间上的性质，即使用到的话也是简单的attention机制提供权重， 而这篇文章则直接将LSTM网络应用到MOT的时序信息上，并同时融合了时序，表观，空间等线索用于多目标跟踪。</p><p>其基本流程如下所示，使用lstm分别刻画提取appearance， motion和interaction的相似特征。然后通过在线方式将三种不同的特征融合起来进行度量学习，最后有学习的度量计算相似度，构建二部图再进行数据关联。</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_architecture.png" alt="lstm_architecture"></p><ul><li><p>基于LSTM的表观特征</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_appearance.png" alt="lstm_appearance"></p><p>输入是crop和resize只有的每个目标，lstm用于抽取已获得轨迹在最后时刻的特征， 然后将lstm特征和当前待匹配的检测响应的特征输入到siamese的浅层网络中融合特征。</p></li><li><p>基于LSTM的运动特征</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_motion.png" alt="lstm_motion"></p><p>基于lstm的motion特征和appearance特征网络相似，不同点在于motio输入的是历史跟踪结果的瞬时速度信息，而appearance输入的经过CNN抽取的表观特征。</p></li><li><p>基于LSTM的空间特征</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_spatial.png" alt="lstm_spatial"></p><p>值得一提的是，基于LSTM的interaction特征，输入是2D拓扑图，刻画的是目标的spatial context。将目标的邻域范围划分网格，然后有目标的网格则置为1，否则置为0.这种做法具有一定的刻画空间上下文的能力，但其实相当粗暴。</p><h4 id="基于双线性LSTM的多目标跟踪"><a href="#基于双线性LSTM的多目标跟踪" class="headerlink" title="基于双线性LSTM的多目标跟踪"></a>基于双线性LSTM的多目标跟踪</h4><p>Kim等人在基于LSTM融合特征的多目标跟踪算法基础上提出了基于双线性LSTM的多目标跟踪方法。</p><p><img src="/2019/05/20/MOT-overview-2nd/lstm_BiLSTM.png" alt="lstm_BiLSTM"></p><p>图中(a)图示双线性LSTM的结构图，其区别在于将lstm提取的特征与检测响应的特征进行了乘性相关运算从而更好地计算相似度。</p><p>作者发现使用提出的基于双线性的LSTM结构能更好地刻画表观特征，但对于motion特征不如直接基于lstm的结构（这可能是因为LSTM目前对高维特征的刻画不够准确），所以最终融合两种方式在MHT框架下提出了MHT-bLSTM算法，其性能如下</p><p><img src="/2019/05/20/MOT-overview-2nd/mht_blstm.png" alt="mht_blstm"></p><p>可以发现本文方法其实MOTA性能并不是很好，其提升点主要在于IDF1指标，说明其对于保持跟踪的稳定具有不错的效果。</p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>近些年多目标跟踪领域的深度学习方法更多的集中在深度特征和深度度量的学习，很少有采用深度学习解决目标之间数据关联的工作，最近又一篇deep match 的文章值得关注，后续我们继续解读。</p><p>我认为目标跟踪未来的发展趋势应该集中于两点：首先如何确定轨迹的起点和终点，这部分目前存在一些利用强化学习处理的工作；其次如何直接使用深度学习的方式解决数据关联问题，这部分应该可以从图卷积网络入手。</p><p>除了上面的发展趋势，我认为目标CNN或者LSTM框架下的研究方向也包括两点：首先是继续提升目标检测的精度，尤其是遮挡环境，精确地目标检测结果对于目标跟踪影响很大；其次是如何解决遮挡导致跟踪失败的问题。</p></li></ul><hr><p>参考[SIGAI]公众号， <img src="/2019/05/20/MOT-overview-2nd/sigai.png" alt="sigai"></p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多目标跟踪总结(上)-传统方法</title>
      <link href="/2019/05/20/MOT-overview-1st/"/>
      <url>/2019/05/20/MOT-overview-1st/</url>
      
        <content type="html"><![CDATA[<p>这篇文章主要阐述了多目标跟踪任务的定义，以及多目标跟踪任务与单目标跟踪、身份重识别等任务的关系，并分析了多目标跟踪任务面临的挑战。总结了一些传统的非深度多目标跟踪方法的基本原理。</p><a id="more"></a><h3 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h3><p>参考SIGAI公众号推文，欢迎关注SIGAI公众号。</p><p>目标跟踪是计算机视觉中的一个重要任务。在计算机视觉的三层结构中，目标跟踪属于中间层，是其他高层任务，如动作识别、行为分析等，的基础。 其广泛应用于视频监控，人机交互，自动驾驶， 医学图像，虚拟现实和增强现实等现实场景中。目标跟踪根据每帧跟踪目标个数不同又划分为单目标跟踪(Single Object Tracking, SOT)和多目标跟踪(Multiple Object Tracking, MOT)。</p><p>单目标跟踪任务在图像序列第一帧中采用bounding box或者其他方式标定待跟踪目标，然后利用跟踪算法在之后帧中逐步确定该目标在每一帧的位置。单目标跟踪场景中一般会遇到物体形变，背景干扰，偶然遮挡等因素。</p><p>多目标跟踪任务是在图像序列的每一帧中同时确定目标的位置和身份，即轨迹ID。MOT任务除了会遇到单目标场景中的物体形变，背景干扰等因素，还具有一些独特的更复杂的问题。</p><ul><li>待跟踪目标如何处理目标的出现和消失。MOT任务中，目标是可以在任意时刻出现或者消失在视野中的。</li><li>如何鉴别不同的个体。MOT任务中有时多个目标来源于同一类别，比如行人跟踪，他们具有相似的表观和轮廓，如何区分他们是降低ID switch，提升性能的关键。</li><li>跟踪目标之间的交互与遮挡。MOT任务中目标个数较多，交互复杂，特别是拥挤场景中相互遮挡严重，如何解决相互遮挡是保障跟踪顺利的关键。</li><li>目标重出现时如何重识别。当目标长时间被遮挡后再次出现，如何正确的找回原来的轨迹编号也是个难题。</li></ul><p>多目标跟踪任务中出现的一些术语：</p><ul><li>目标： 在图像中，明显区别于周围环境的闭合区域往往被称为目标，这些目标一般是一些感兴趣区域ROI。</li><li>检测： 通过算法得到目标在图像中的位置的过程称为检测。近些年检测器的发展非常迅速，从最初的模板匹配到目前流行的深度学习方法，比如Faster RCNN系列， SSD系列， YOLO系列， CornerNet系列等。检测器的性能得到极大的提升。</li><li>跟踪：不同帧中，物理意义下的同一目标相关联的过程</li><li>检测响应：检测过程的输出量，即检测结果。在不引起混淆的环境中，也用‘检测’表示检测响应。</li><li>跟踪假设：每一帧中跟踪的结果。</li><li>轨迹：MOT系统的输出量，一条轨迹对应这一个目标在一个时间段内中的位置序列。</li><li>轨迹片段： 完整轨迹中的一些连贯的较短的轨迹碎片。</li><li>数据关联：目标之间的匹配。MOT任务中在数据关联阶段一般假设一条轨迹只能对应一个检测响应， 同时一个检测响应最多对应一条轨迹。</li><li></li></ul><p>这篇文章简单介绍和归纳一些经典的多目标跟踪方法。</p><hr><h3 id="多目标跟踪算法分类"><a href="#多目标跟踪算法分类" class="headerlink" title="多目标跟踪算法分类"></a>多目标跟踪算法分类</h3><p>多目标跟踪问题最早提出是在雷达信号中同时跟踪多架敌机和多枚导弹。这些算法后来被借鉴用于机器视觉领域的多目标跟踪任务。多目标跟踪算法分类不是很严格，根据不同的分类标准有不同的分类方法。比如</p><ol><li><p>按目标初始化方式的不同划分为Detection Based Tracking (DBT)和Detection Free Tracking (DFT)。 </p><p>DFT需要在目标出现的第一帧中标定目标位置，之后跟踪中边检测目标边跟踪目标。DBT是将MOT任务划分为两个阶段：Detection + Data Association. 随着检测器性能的大幅提升，基于检测的多目标跟踪算法是目前主流的跟踪算法。</p></li><li><p>按目标处理过程中用到的信息范围划分为在线跟踪和离线跟踪。</p><p>离线跟踪需要用到当前时刻之前和之后的信息，利用这些特征构建图，网络流等复杂的模型，然后跟踪当前时刻目标，这类方法能够很好的解决遮挡问题，一般性能较好。但是其明显的不足点在于很难适应实时跟踪的需求。在线跟踪则要求对当前时刻的检测响应进行跟踪是只能利用之前和当前的信息，该类算法的难点在于处理遮挡和检测的不准确问题。</p></li><li><p>按跟踪算法的表示形式和优化框架划分为确定性推导和概率统计最大化两类。</p><p>确定性推导是将检测和已跟踪的轨迹看做二元变量，通过构建整体的目标函数，计算最佳匹配，比如经典的二部图分配等。概率统计最大化方法是将检测和轨迹的关系通过概率模型表示，然后通过最大化该概率模型计算最优分配，比如基于贝叶斯的Kalman 滤波和粒子滤波， 马尔科夫链等。</p></li><li><p>按应用角度不同可分为运动场景、航拍场景等。</p><p>运动场景，比如运动员的跟踪。运动场中同一队伍的队员队服几乎相同，并且运动场景图像角度，尺寸变化较大给跟踪带来很大困难，当然运动场景中一些先验假设比如场地边缘等信息给目标跟踪带来一定帮助。航拍场景中目标往往太小，帧率太低，位移太大，较难处理。这类方法主要依靠上下文信息解决。</p></li></ol><p>一般而言，MOT的不同分类是存在重叠的。MOT的分类只是提供了不同角度理解跟踪算法，对算法本身并没有多大影响。MOT算法中起决定作用的是检测，鉴别特征和数据关联。</p><hr><h3 id="经典多目标跟踪算法介绍"><a href="#经典多目标跟踪算法介绍" class="headerlink" title="经典多目标跟踪算法介绍"></a>经典多目标跟踪算法介绍</h3><h4 id="多假设跟踪-MHT"><a href="#多假设跟踪-MHT" class="headerlink" title="多假设跟踪(MHT)"></a>多假设跟踪(MHT)</h4><p>多假设跟踪本质上是卡尔曼滤波方法在多目标跟踪任务上的拓展。定义$k$时刻之前的检测响应为$z^k$, 历史总的检测响应为$Z^k$, 多假设跟踪的目标是求解已有轨迹和当前检测之间关联的条件概率模型。 把似然关联假设$\Theta_k$划分为当前关联假设$\theta_k$和$k-1$时刻的关联假设$\Theta_{k-1}$, 那么依贝叶斯推理可以得到</p><script type="math/tex; mode=display">\begin{align}P\{\Theta_k|Z^k\} &= P\{\theta_k, \Theta_{k-1}|z^k, Z^{k-1}\} \\&= \frac{1}{c}p[Z^k|\theta_k,\Theta_{k-1}]P\{\theta_k|\Theta_{k-1}, Z^{k-1}\}P\{\Theta_{k-1}|Z^{k-1}\}\end{align}</script><p>后验概率通过贝叶斯公式转换为先验概率。最下面一行等式右边第3项是$k-1$时刻的后验概率，第2项是$k$时刻之前的检测响应和对应轨迹的条件下计算当前时刻关联假设的概率，第1项是在获得当前项和历史关联假设前提下出现当前检测响应的概率。$c$是推导过程中的分母，常数，用于保证概率范围。该公式逐帧计算是迭代过程，每一帧需要计算第1,2两项。</p><p>MHT采用两个概率模型建模第1,2两项</p><blockquote><ul><li>用均匀分布和高斯分布对关联对应的检测观察建模.</li><li>用泊松分布对当前假设的似然概率建模</li></ul></blockquote><p>前者表示，当检测响应来自轨迹$T$时，它应符合$T$在当前时刻的高斯分布，否则认为是一个均匀分布的噪声，类似于卡尔曼滤波跟踪。后者表示，在误检和新对象出现概率确定的情况下，出现当前关联的可能性可以通过泊松分布和二项分布的乘积表示。在以上假设下，关联假设的后验分布是历史累计概率密度的 连乘，转化为对数形式，可以看出总体后验概率的对数是每一步观察似然和关联假设似然的求和。因此，选择最佳的关联假设，转化为观察似然和关联假设似然累计求和的最大化。在具体算法中， I.J.Cox等人提出一种基于假设树的优化算法，如下图</p><p><img src="/2019/05/20/MOT-overview-1st/MHT.png" alt="MHT"></p><p>可以发现其实MHT有局部遍历的思想， 并且利用到了之后的信息，属于离线方式。</p><p>任何时刻都可能存在多种假设关联，因此到k时刻的假设构成了一种组合假设树的层次关系。 例如图4左边表示的是2个轨迹和3个观测之间可能形成的关联假设，可能存在的假设有{观测 23=&gt;轨迹1，观测22=&gt;轨迹2, 观测21=&gt;新轨迹}或者{观测22=&gt;轨迹1，观测21=&gt;轨迹 2, 观测23=&gt;新轨迹}，因此产生2个假设分支。图4右侧是从这2个关联假设出发的三层假设 树关系，可以看出随着假设层数的增多，关联假设出现组合爆炸的可能。因此进行必要的剪 枝减少假设空间的数目是必须的步骤。那么如何选择最佳的关联呢？I.J.Cox采用了2个步骤 来实现。首先，限制假设树的层数为3层。其次，是对每个分支的叶节点概率对数进行求和， 最大的分支进行保留，即选择边缘概率最大的那个分支假设作为最后选择的关联。可以把这 种选择方法简单的表示为：</p><script type="math/tex; mode=display">S(k-3) = \frac{1}{|H(k)|}\sum_{\theta^k\in H(k)}\log([Z^k|\theta_k,\Theta_{k-1}])+\log(P\{\theta_k|\Theta_{k-1}, Z^{k-1}\})</script><p>其中$H(k)$是所有可能的假设集合，所以上式是从多个假设中筛选置信度最高的假设作为跟踪结果。</p><p>这种基于似然概率对数累加的方法虽然方便迅速，但是存在一个主要的限制，即假定 观测关联符合高斯模型，并且在每一步选择关联假设之后，需要利用Kalman滤波更新轨迹状 态。通过对MHT基本公式(1)的扩展，可以建立不同的概率模型描述这种多假设关联的全 局概率，例如Kim等人在ICCV2015和ECCV2018通过归一化的最小均方差优化算法引入表 观模型来扩展MHT算法，取得不错的多行人跟踪结果.</p><h4 id="基于检测置信度的粒子滤波算法"><a href="#基于检测置信度的粒子滤波算法" class="headerlink" title="基于检测置信度的粒子滤波算法"></a>基于检测置信度的粒子滤波算法</h4><p>该算法可分为两个步骤：</p><ul><li>利用贪心算法，将每一帧检测结果分配给已有的轨迹结果</li><li>利用匹配结果，计算每个对象的粒子权重，作为粒子滤波框架中的观测似然概率。</li></ul><p>粒子滤波框架如下图所示：</p><p><img src="/2019/05/20/MOT-overview-1st/pf.png" alt="PF"></p><p>分别表示检测响应，构建相似度矩阵，计算每个粒子群权重，贪心算法或者匹配，重新计算当前目标位置。</p><p>具体步骤如下：</p><ol><li><p>计算历史轨迹与当前检测响应的相似度</p><script type="math/tex; mode=display">S(tr, d) = g(tr, d)[c_{tr}(d) + \alpha \sum_{p\in tr}^Np_N(d-p)]</script><p>tr表示轨迹， d表示检测。所以相似度包含3方面：$c_{tr}(d)$表示在线学习的分类器结果；粒子与检测的匹配度采用高斯密度函数度量，$p\in tr$表示轨迹对应的粒子群， $p_N(d-p)$表示高斯相似度；与检测尺寸大小相关的阈值函数$g(tr,d)$表示轨迹与轨迹尺度上的契合程度， $\alpha$用于控制粒子群的松紧度。</p><p>计算出匹配亲和度矩阵之后，可以采用二部图匹配的Hungarian算法计算匹配结果。不过作 者采用了近似的贪心匹配算法，即首先找到亲和度最大的那个匹配，然后删除这个亲和度， 寻找下一个匹配，依次类推。贪心匹配算法复杂度是线性，大部分情况下，也能得到最优匹 配结果。</p></li><li><p>采用粒子滤波框架，更新每条轨迹的粒子权重。</p><script type="math/tex; mode=display">w(tr, p) = \beta\cdot I(tr)\cdot p_N(p-d^*) + \gamma \cdot d_c(p)\cdot P_o(tr) + \eta\cdot c_{tr}(p)</script><p>$I(tr)$是示性函数,表示轨迹在该帧中是否关联成功，成功的话，则计算粒子在关联检测为中心的高斯概率；$d_c(p)$是粒子的检测可信度， $p_o(tr)$是权重函数</p><script type="math/tex; mode=display">p_o(tr)=\begin{cases}1, \quad\quad\quad&\text{if} \quad\quad I(tr)=1,\\\max_{tr':I(tr')=1}p_N(tr-tr') \quad\quad &\text{elif}\quad \exists ~~I(tr')=1 \\0 &\text{otherwise}\end{cases}</script><p>表示当轨迹匹配的时候权重为1， 否则去附近成功匹配轨迹的最大高斯密度，若周围没有成功匹配，则权重为0.</p><p>结合检测可信度的粒子滤波算法对轨迹的初始化采用了感兴趣区域的简单启发式策略。即， 进入图像区域边框时，初始化对象；当连续多帧没有关联到检测时终止跟踪。在一些典型数 据集上，基于检测可信度的粒子滤波算法可以得到不错的结果。</p></li></ol><h4 id="基于最小代价流的跟踪算法"><a href="#基于最小代价流的跟踪算法" class="headerlink" title="基于最小代价流的跟踪算法"></a>基于最小代价流的跟踪算法</h4><p>该算法是一种确定性优化离线方法，其目标函数是从已知的检测结合中找出最优的轨迹集合</p><script type="math/tex; mode=display">T^* =arg\max_T P(T|D) = arg\max_T\prod_iP(d_i|T)P(T)</script><p>表示成对数形式</p><script type="math/tex; mode=display">T^* = arg\min_T\sum_{T_k\in T}(-\log P(T_k) + \sum_i-\log P(d_i|T_k))</script><p>第一项刻画轨迹存在的概率， 第二项表示该轨迹存在前提下对应该检测的概率。</p><p>每条轨迹表示成链接的检测后，其能量可以表示为</p><script type="math/tex; mode=display">f^* = arg\min\sum_i C_{si}f_{si} + \sum_i C_if_i+\sum_{i,j}C_{ij}f_{ij} + \sum_iC_{it}f_{it}</script><p>其中$f_{si}, f_i, f_{ij}, f_{it}$均为示性变量分别表示是否是起点，是否在轨迹上，是否后续连接$j$节点，是否是终点。由于MOT任务中一一对应假设，所以节点$i$前后只存在一个连接，$f_{si} + \sum_j f_{ji} = f_i = \sum_{j}f_{ij}+f_{it}=1 ~\text{or}~0$</p><p>网络流优化问题满足最小代价流模型：</p><p><img src="/2019/05/20/MOT-overview-1st/mcn.png" alt="min-cost-flow"></p><p>边的代价函数计算规则：</p><script type="math/tex; mode=display">\begin{align}C_{si} &= -\log P_{enter}(x_i)\\C_{it} &= -\log P_{exit}(x_i)\\C_{ij} &= -\log P_{link}(x_j|x_i)\\C_i &= \log(\beta_i/(1-\beta_i))\end{align}</script><p>迹数目通过迭代比较的方法确定. 注意到检测节点代价Ci 的值是一个负数，所以轨迹对应的网络流代价也可能小于0。因此，通过遍历不同轨迹数目，可以确定一个全局 代价最小的解。这种方式效率较低。</p><h4 id="基于马尔科夫决策-MDP-的跟踪算法"><a href="#基于马尔科夫决策-MDP-的跟踪算法" class="headerlink" title="基于马尔科夫决策(MDP)的跟踪算法"></a>基于马尔科夫决策(MDP)的跟踪算法</h4><p>MDP是一种在线确定性推导方法。</p><p>该方法把目标跟踪看作是状态转移过程。 马尔科夫决策过程包含四个元素$(S, A, R, T)$, 分别表示状态集合、动作集合、状态转移集合和奖励函数集合。一个目标的跟踪过程包括如下决策过程：</p><ol><li>从Active状态转移到Tracked状态或者Inactivate状态，表示新出现的检测是否确实是轨迹。</li><li>从Tracked状态转移到Tracked状态或者Lost状态，表示跟踪继续或者终止。</li><li>从lost状态转移到Lost状态或者Inactivate状态或者Tracked状态， 即判断丢失的对象被重新跟踪或者终止或者继续处于跟丢状态。</li></ol><p><img src="/2019/05/20/MOT-overview-1st/MDP.png" alt="MDP"></p><p>三种决策过程分别对应着不同的奖励函数。</p><ol><li><script type="math/tex; mode=display">R_{activate}(s, a) = y(a)(w_{activate}^T\phi_{activate}(s)+b_{activate})</script><script type="math/tex; mode=display">y(a) = \begin{cases}1, &\text{if} \quad a=a_1\\-1, &\text{else} \quad a=a_2\end{cases}</script><p>这一步主要用来判断检测是否是目标。论文中使用离线训练的SVM实现该过程。检测样本的特征包括$(x,y, w, h, d_s)$</p></li><li><script type="math/tex; mode=display">R_{Tracked}(s, a)=\begin{cases}y(a) &\text{if}\quad e_{medFB}<e_0, \text{and} \quad o_{mean} > o_0\\-y(a) &\text(else)\end{cases}</script><p>$a=a_4$时， $y(a)=1$， 否则$y(a_3)=-1$, $e_{medFB}, o_{mean}$分别表示光流中心偏差和平均重合度， $e_0, o_0$分别表示阈值。</p></li><li><script type="math/tex; mode=display">R_{Lost}(s, a) = y(a)(\max_{k=1}^M(w^T\phi(t, d_k)+b))</script><p>计算$M$个匹配模板与检测的匹配程度，判断是否存在使得上述奖励最大且非负的匹配，当存在是选择$a=a_6$， 不存在是判断是否连续丢失次数超出阈值$T_{Lost}=50$，超出则$a=a_7$, 否则$a=a_6$. </p></li></ol><p>这部分可能理解起来存在些困难。其实需要弄明白的是，强化学习中首先根据当前的状态计算能量，然后根据能量来选择动作使得奖励最大。比如第一种奖励，首先计算等式右边第二项，根据其值大小来选择$a$是执行$a_1$还是$a_2$从而使$R_{active}$最大。</p><h4 id="基于局部流特征的近似在线多目标跟踪（NOMT）"><a href="#基于局部流特征的近似在线多目标跟踪（NOMT）" class="headerlink" title="基于局部流特征的近似在线多目标跟踪（NOMT）"></a>基于局部流特征的近似在线多目标跟踪（NOMT）</h4><p>这种方法称为近似在线多目标跟踪的原因在于跟踪$k$时刻目标时仅利用了之前的信息，但是该时刻依然允许对之前时刻跟踪结果进行修改。</p><p>其主要思想：对于当前帧$t$, 回看$\tau$帧，在这个时间间隔内构造若干轨迹片段, 利用这些轨迹片段跟之前的轨迹进行关联跟踪。轨迹片段包含了最新的$\tau$帧信息，因此如果该时间段内发生了匹配错误，依然允许进行修改。其基本思想如下图所示</p><p><img src="/2019/05/20/MOT-overview-1st/NOMT.png" alt="NOMT"></p><p>该部分的具体内容，请参考<a href="https://www.cnblogs.com/YiXiaoZhou/p/6798860.html" target="_blank" rel="noopener">博客园</a></p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>以上讨论了一些经典的传统多目标跟踪方法，这些方法虽然目前相对于基于深度的方法性能稍弱，但是作为基本的baseline，思想值得借鉴。</p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> overview </tag>
            
            <tag> traditional method </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Evaluation Multiple Object Tracking Performance-The CLEAR MOT Metrics</title>
      <link href="/2019/05/19/CLEAR/"/>
      <url>/2019/05/19/CLEAR/</url>
      
        <content type="html"><![CDATA[<p>这篇笔记主要总结了MOT任务中常用的度量指标，其表示的意义和计算方式。</p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>原发表于<a href="https://www.cnblogs.com/YiXiaoZhou/p/5937980.html" target="_blank" rel="noopener">CLEAR</a>， 参考之前笔记<a href="https://zongweizhou1.github.io/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/" target="_blank" rel="noopener">MOT16</a></p><p>多目标跟踪问题的评价指标应该能够评估三个方面：</p><ul><li>目标是否都及时的找到。</li><li>目标跟踪到的位置和真实位置一致性程度。</li><li>是否能够保持跟踪的一致性。</li></ul><p>另外作为metric的还需要具有的基本特点：</p><ul><li>参数（可调节的阈值等）需要尽可能的少，从而使评估简单直接，方法对比性较强。</li><li>尽可能直观，易解释。</li><li>应该具有较好的普适性，比如适应2D和3D的情况。</li><li>指标个数尽可能少，每个指标都具有较高表达能力。</li></ul><p>于是本文提出了CLEAR评估指标系统，用于评估MOT方法</p><h3 id="CLEAR"><a href="#CLEAR" class="headerlink" title="CLEAR"></a>CLEAR</h3><p>假设每一帧中目标为$\{o_1, o_2, …, o_n\}$, 跟踪假设为$\{h_1, h_2, …, h_m\}$, 检测结果为$\{d_1, d_2, …d_k\}$。跟踪假设和检测不同，跟踪假设是算法跟踪出来的结果。于是评价过程有以下基本步骤：</p><ol><li>建立假设和目标之间的对应关系。</li><li>对所有的对应关联关系，计算位置偏差，即一致性程度；</li><li>计算跟踪的累积结构误差：<ul><li>计算漏检数 FN</li><li>计算虚警数 FP</li><li>计算发生跳变数 IDs</li></ul></li></ol><p><strong>实现过程</strong></p><ol><li><p>确定依赖关系。通过假设与目标之间的距离判断是否关联，比如选择IOU作为相似度指标，然后给定阈值$\tau$， 采用匈牙利算法对匹配进行关联，之后将超出阈值的关联剔除，剩下的就是最终的依赖关系。如下图</p><p><img src="/2019/05/19/CLEAR/clear_corresponding.png" alt="corresponds"></p><p>注意MOT中这个匹配还不完全是这样的，MOT更注重一致性，所以假设第$t$帧有关联$o_i, h_j$ , 第$t+1$帧关联$(o_i, h_k) &lt; (o_i, h_j)&lt;\tau$这时候依然选择$(o_i, h_j)$. 如下图在$t+2$时刻，虽然$(o_1, h_2)$更匹配，但是为了连贯性，依然选择匹配$(o_1, h_1)$</p></li></ol><p>   <img src="/2019/05/19/CLEAR/IDKeep.png" alt="IDKeep"></p><ol><li><p>一致性刻画。追踪一致性能力就是指追踪器使追踪假设和对应目标长时间保持对应关系不变的能力。</p><p>一致性度量有两种方式可选。第一种是选择真实轨迹的一条最优匹配跟踪结果，然后其他的匹配都认为是错误匹配，记为IDs，如下图所示</p><p><img src="/2019/05/19/CLEAR/IDS.png" alt="IDS"></p><p>case1中认为红色的是成功匹配轨迹，于是错误数为2，同理case2中错误数为4.</p><p>但是这种方式有时候不符合常理，我们更应该关注的ID发生转变的那一刻，而不是跳变后带来的影响。比如case2中，两个跟踪轨迹都挺长，采用上一种方式就不合理。 所以文章采用了第二种方式，即只统计跳变的次数。具体而言，某一时刻的tid与之前最近的tid不同则认为发生了跳变。</p></li><li><p>指标计算过程</p><ul><li><p>计算第$t$帧中， $t-1$时刻存在的关联$(o_i, h_i)$关联是否依然有效。</p></li><li><p>对于不再有效的关联，在新的假设中寻找最有匹配，此时选择的策略是IOU最小。该过程中可以统计IDS发生的次数：若出现新的 $ (o_i, h_j)$不在历史匹配中，则认为发生一次跳变， 将$(o_i, h_j)$替代之前的$(o_i, h_i)$, 该时刻的ids记为$mmr_t$。</p></li><li><p>找到所有帧的关联后，统计所有匹配个数$c_t$, 计算每一个匹配的匹配距离$d_t^i$.</p></li><li><p>剩下未匹配的假设和目标分别记为虚警和漏检$m_t, fp_t$, 并用$g_t$表示当前时刻真正目标的个数。</p></li><li><p>从起始帧逐帧计算上述变量，最终统计下面的指标</p><script type="math/tex; mode=display">MOTP = \frac{\sum_{i,t}d_t^i}{\sum_tc_t}</script><p>即所有匹配的平均距离，该值使用IOU时取值越大越好，一般范围在$0.50\sim1$之间，因为$IOU&gt;0.50$的才认为是正确匹配。</p><script type="math/tex; mode=display">MOTA = 1-\frac{\sum_t(m_t+fp_t+mmr_t)}{\sum_tg_t}</script></li></ul></li></ol><pre><code> 其中$\frac{\sum_tm_t}{\sum_tg_T}, \frac{\sum_tfp_t}{\sum_tg_t}, \frac{\sum_tmmr_t}{\sum_tg_t}$分别表示总体的漏检率，虚警率和跳变几率。 MOTA的取值范围在$(-\infty, 1)$之间， 越大越好。</code></pre><p>   注意， MOTP，MOTA都是整体过程的均值，而不是每一帧均值之后的平均值，两个之间差别还是挺大的。如下图所示，一个8帧的数据，前面4帧都被漏检，第五帧开始只有第4个目标匹配成功，这样助阵计算漏检率的话， $(1 \times 4 + 0 \times 4)/8=0.5$, 而如果统计整个跟踪过程在计算漏检率为$\frac{4\times 4}{20}=0.8$。 显然采用整体过程更合适。</p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>论文在CLEARs workshops上使用提出的两个metric评价了不同的track system，发现</p><blockquote><p>the proposed metrics indeed reflect the strengths and weaknesses of the various used systems in an intuitive and meaningful way, allow for easy comparison of overall performance, and are applicable to a variety of scenarios.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> CLEAR </tag>
            
            <tag> Metric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Spatial-Temporal Relation Networks for Multi-Object Tracking</title>
      <link href="/2019/05/19/STRN/"/>
      <url>/2019/05/19/STRN/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/05/19/STRN/strn_architecture.png" alt="strn_architecture"></p><p>鲁棒的相似度度量是MOT取得较好性能的一个关键。而鲁棒的相似度度量应该能够变现表观、位置、时间和空间信息。由于这些线索差异性较大，不能直接组合在一起，一般的MOT方法会分别使用网络处理这些特征。</p><p>本文提出了一种Spatial-Temporal Relation network能够同时encode多种线索，并从时空关系中推理轨迹和检测的匹配关系。该网络能够端到端的训练并在MOT15-17benchmark上都取得了SOTA的性能。</p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>鲁棒的相似度度量是MOT取得较好性能的一个关键。而鲁棒的相似度度量应该能够变现表观、位置、时间和空间信息。由于这些线索差异性较大，不能直接组合在一起，一般的MOT方法会分别使用网络处理这些特征。</p><p>本文提出了一种Spatial-Temporal Relation network能够同时encode多种线索，并从时空关系中推理轨迹和检测的匹配关系。该网络能够端到端的训练并在MOT15-17benchmark上都取得了SOTA的性能。</p><p><img src="/2019/05/19/STRN/STRN.png" alt="STRN"></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>相似度指标对于数据关联的影响很大。大多数方法都只利用表观计算相似度。这种方法有两点劣势。</p><ul><li>像MOT benchmark这类数据集中，往往跟踪的目标来自于同一类，因此他们的表观很难区分，尤其是在比赛场景中，同队队员服装相同就更难区分。</li><li>多目标跟踪任务中经常出现遮挡比较严重，或者姿态变化较大的情况，都很难利用表观获得较好的匹配。</li></ul><p>因此目前一些较好的方法也利用多线索融合的方式计算目标的相似度，比如表观，时间，空间等，代表性方法</p><blockquote><p>A. Sadeghian, A. Alahi, and S. Savarese. Tracking the untrackable: Learning to track multiple cues with long-term dependencies. In ICCV2017, p300-311.</p></blockquote><p>但是由于来自于不同线索的特征是异质的，不能简单的串接在一起，所以需要为每一个特征设计复杂的网络结果，上面那篇文章中分别使用LSTM刻画了表观特征，时间位置特征和空间拓扑特征。表观特征使用CNN抽取基本特征然后再放入到LSTM中计算检测与轨迹的相似特征向量， 时间位置特征类似，放入的是位置信息抽取特征向量，空间拓扑则是在每一个目标周围选取grid区域，然后将临近点的2D分布送入LSTM中抽取空间特征，最后再将三种特征并在一起进行相似度度量学习。</p><p>这篇文章提出了一个统一框架用于融合多线索进行相似性度量。其主要思想是在空间和时间上同时利用relation network刻画目标之间关系，包括位置和表观。</p><p>如Figure 1.所示， 现在每一帧中利用关系网络进一步strengthen 每一个目标的表观特征，然后再与存在的tracklets的strengthen的表观特征利用时域的关系网络进行串联并学习相似度。</p><hr><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><h4 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h4><p>$\{T_i\}_{i=1}^N$                                      表示$N$条轨迹</p><p>$T_i=\{b_i^t\}_{t=1}^T$                             表示第$i$条轨迹中的$T$帧中匹配到的位置， bounding box。</p><p>$b_i^t = [x_i^t, y_i^t, w_i^t, h_i^t]$                  中心点的坐标和目标的宽度与高度</p><p>$D_t=\{b_j^t\}_{j=1}^{N_t}$                           时刻$t$中的所有检测响应                       </p><h4 id="跟踪流程"><a href="#跟踪流程" class="headerlink" title="跟踪流程"></a>跟踪流程</h4><p><img src="/2019/05/19/STRN/pipeline.png" alt="pipeline"></p><p>本文方法依然在tracking-by-detection框架下，因此给定图像序列，首先使用检测器对目标进行检测。然后将以跟踪的轨迹与当前帧的检测配对放入到Spatial-Temporal Relation Networks (STRN)中计算每一个点对的相似度。于是轨迹和检测就行了二部图，采用匈牙利算法进行数据关联将检测分配给不同的轨迹。</p><p>Spatial-Temporal Relation Networks的框架如下图所示：</p><p><img src="/2019/05/19/STRN/strn_architecture.png" alt="strn_architecture"></p><p>首先采用ResNet50网络提取当前帧中检测的特征以及历史轨迹中每个目标的特征，然后分别计算每个目标在关系网络中的特征和每条轨迹在时域中加权的特征，分别记为$\phi_{s,j}^t, \phi_{ST, i}^{t-k}$ , 然后将特征进行配对，送入到feature module中计算四种embedded特征，再将四种特征concat成最终特征用于计算相似度. 下面分别介绍spatial-temporal relation module和feature module。</p><h4 id="spatial-temporal-relation-module"><a href="#spatial-temporal-relation-module" class="headerlink" title="spatial-temporal relation module"></a>spatial-temporal relation module</h4><ol><li>空域的object relation module （ORM)</li></ol><p>这部分的内容应该是受下面这篇文章的启发，利用GCN网络进行目标检测</p><blockquote><p>H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks for object detection. 2018.</p></blockquote><p>每一个目标可以使用$o_i = (\phi_i, b_i)$表示，分别表示表观特征和位置信息。那么refined 特征相当于GCN网络抽取的特征。</p><script type="math/tex; mode=display">\phi_i' = \phi_i + \sum_j w_{ij} \cdot (W_V\cdot \phi_j)</script><p>$j$取同一帧中所有目标， 所以上式表示新的特征等于本身特征和其他目标的影响的结合。$W_V$是学习参数，表示对特征的进一步抽象。$w_ij$表示$j$样本对当前$i$样本的贡献量，是一种attention权重。</p><script type="math/tex; mode=display">w_{ij} = \frac{w_{ij}^G\exp(w_{ij}^A)}{\sum_k^Nw_{ik}^G\exp(w_{ik}^A)}</script><p>$w_{ij}^A$表示的是projected后的表观特征的scaled的内积。这个scale主要是让attention更加平稳。</p><script type="math/tex; mode=display">w_{ij}^A = \frac{<W_Q\phi_i, W_K\phi_j>}{\sqrt{d}}</script><p>$W_Q, W_K$分别表示project矩阵， $d$表示投影特征的维度， $w_{ij}^G$表示的是相对位置关系。$w_{ij}^G = \log\big( \frac{|x_i-x_j|}{w_j}, \frac{|y_i-y_j}{h_j}, \frac{w_i}{w_j}, \frac{h_i}{h_j}\big )$</p><ol><li>时域的关系网络</li></ol><p>时域的关系网络可直接通过节点域有单帧图像扩大到多帧图像处理，但这种方式存在两个缺陷。</p><p>首先，计算量急剧增大；其次，空间特征和时间特征本质上是不同的信息。</p><p>所以，这这篇文章中的时域关系网络本质上是时域上的attention机制，即加权平均值。</p><p>空间关系网络输出特征表示为$\phi_{S,i}$ 时间关系网络在空间关系网络的输出特征上进行操作。​</p><script type="math/tex; mode=display">\phi_{ST,i}^t = \sum_{k}^{\tau_1-1} w_i^{t-k}\cdot \phi_{s,i}^{t-k}</script><p>这里值得注意的是，计算时间关系特征的时候只计算轨迹的特征，而当前帧中的检测不参与计算。</p><script type="math/tex; mode=display">w_i^t = \frac{\exp(<w_T, \phi_{S,i}^t>)}{\sum_k\exp(<w_T, \phi_{S,i}^k>)}</script><p>$w_T$是待学习参数。</p><p><img src="/2019/05/19/STRN/spatial attention.png" alt="spatial attention"></p><p><img src="/2019/05/19/STRN/temporal attention.png" alt="temporal attention"></p><h4 id="Feature-representation"><a href="#Feature-representation" class="headerlink" title="Feature representation"></a>Feature representation</h4><p>这个模块主要用于集成不同的线索，最终计算相似度。</p><script type="math/tex; mode=display">s_{ij}^t = \text{sigmod}(W_{s2}\cdot ReLU(W_{s1}\cdot [\phi_R; \phi_C; \phi_L;\phi_M]))</script><p>下面介绍四种特征$\phi_R, \phi_C, \phi_L, \phi_M$.</p><ol><li><p>Relation features</p><p>最直接的方式是将每个待关联的轨迹最后一帧的特征和待检测目标的特征串联起来作为新的特征然后将新的特征送入网络进一步抽象。</p><script type="math/tex; mode=display">\phi_R = W_R \cdot [\phi_{ST,i}^{t-k}; \phi_{S,j}^t], 1\le k \le \tau_2</script><p>作者认为这个特征是把双刃剑，既可以直接用来计算相似度，也给学习紧致的目标特征带来了难度。因此作者又在关系特征$\phi_{ST,i}^{t-k}, \phi_{S,j}^t$上直接显示的计算了余弦相似度。</p><script type="math/tex; mode=display">\phi_C = \cos (W_C\cdot \phi_{ST,i}^{t-k}, W_C\cdot \phi_{S,j}^t), 1\le k \le \tau_2</script><p><strong>分析</strong>：</p><blockquote><p>In general, cosine value could take effect only in the scenarios where two input features are compatible in representation.</p></blockquote><p>即一般而言，cosine距离最好直接度量来自于相同空间向量的相似度，但这里的特征来自于检测和轨迹。看起来似乎不同空间。但是经过分析可以发现轨迹的特征其实是在时间域上的加权平均值，而每一个元素都是每帧图像检测的空间特征，因此特，他们其实分布在临近空间，所以可以使用cosine距离刻画。</p></li><li><p>location features</p><p>位置关系和运动信息是另外两种常用的匹配信息。文章选择每个跟踪轨迹的最后出现目标的位置的大小作为参考目标，embedding 当前检测的位置和大小信息。</p><script type="math/tex; mode=display">\phi_* = W_* \cdot \varepsilon_*(f_*(b_i^{t-k}, b_j^t))</script><p>其中$*\in \{L, M\}$分别表示位置和运动信息。</p><p>位置信息是4维特征。</p><script type="math/tex; mode=display">f_L'(b_j^t) = \big(\frac{x_j^t}{I_w^t}, \frac{y_j^t}{I_h^t}, \frac{w_j^t}{I_w^t}, \frac{h_j^t}{I_h^t}\big)</script><p>其中$I_w^t, I_h^t$表示第$t$帧图像的宽和高。  $f_L(b_i^{t-k}, b_j^t) = [f_L’(b_i^{t-k}); f_L’(b_j^t)]$ </p><p>运动信息则是刻画的目标之间的相互位置和大小关系。</p><script type="math/tex; mode=display">f_M((b_i^{t-k}, b_j^t) = \log\big( \frac{|x_i^{t-k}-x_j^t|}{kw_i^{t-k}}, \frac{|y_i^{t-k}-y_j^t|}{kh_i^{t-k}}, \frac{w_j^t}{kw_i^{t-k}}, \frac{h_j^t}{kh_i^{t-k}}\big)</script><p><strong>这部分我觉得设计存在一定问题。前面两项都是计算的变化速度，但后面两项其实不应该计算相对比例的变化速度，直接使用比值，或者平均差值或者会更好</strong></p><hr></li></ol><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Datasets-and-Evaluation-Metrics"><a href="#Datasets-and-Evaluation-Metrics" class="headerlink" title="Datasets and Evaluation Metrics"></a>Datasets and Evaluation Metrics</h4><p>dataset: 2D MOT2015, MOT16, MOT17</p><p>metrics:  CLEAR and IDF1 Score</p><h4 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h4><ul><li>backbone network是在ImageNet预训练的ResNet50网络， 并在MOT训练集上finetune</li><li>每个目标是经过crop之后再rescale到$128\times 64$之后放入特征抽取网络。</li><li>计算temporal relation时使用了最近的$9$帧图像。</li><li>网络的其他参数见原文。</li></ul><h4 id="跟踪器参数"><a href="#跟踪器参数" class="headerlink" title="跟踪器参数"></a>跟踪器参数</h4><p>若视频序列的帧率为$F$， 如果一条轨迹在初始化之后的$F$帧内匹配成功的次数低于$0.3F$则认为是错误的初始化，剔除。 </p><p>当轨迹在最近的$1.25F$内都没出现成功匹配则认为该轨迹结束。</p><h4 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h4><p>分解实验是在MOT15上进行的。</p><p><img src="/2019/05/19/STRN/ablationstudy.png" alt="ablation study"></p><p>Table 1的上三行 表示没有使用关系网络的性能。$A_u$表示只利用resnet50抽取的特征学习相似度，这部分其实是一种度量学习的方式。$A_c$表示只计算cosine距离而没有利用unary appearance， 这部分则相当于feature embedding的过程，$A$则是同时利用了两种信息，可以发现两种信息都利用的效果最好。</p><p>Table1 的下三行表示结合了location特征的性能， $L_u$表示仅利用了位置信息， $L_m$表示仅利用运动信息，可以发现两种location特征融合时，性能最好。同时加入位置信息相对于仅利用表观表观信息性能得到很大提升。这主要归功于IDS的下降。</p><p>Table 2 上三行对比了时间和空间关系网络对性能的影响。可以发现单独加入spatial 关系网络，性能既有较大提升，但是此时IDS反而变大了。FP的降低表明空间关系网络对检测是有用的，但是可能由于目标特征的融合会导致部分较近目标难以区分，从而导致IDS增大。 而加上时间attention之后性能进一步提升，表明时间上的attention能够更好的刻画历史信息。此时IDS下降。 </p><p>Table 2 下两行是将时间attention机制之间用池化方式去做，发现性能下降了，当然相对于baseline还是有所提升的，证明temporal 信息有助于跟踪，但attention主动调整权重的方式比预设的均值或者最大值方式更加有效。</p><h4 id="MOT-Benchmarks"><a href="#MOT-Benchmarks" class="headerlink" title="MOT Benchmarks"></a>MOT Benchmarks</h4><p><img src="/2019/05/19/STRN/MOT15.png" alt="MOT15"></p><p><img src="/2019/05/19/STRN/MOT16.png" alt="MOT16"></p><p><img src="/2019/05/19/STRN/MOT17.png" alt="MOT17"></p><p>实验证明了方法的有效性，但其实这篇文章对比的baseline目前性能并不是最好的。只能说该方法性能还不错。</p><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>该文章STRN说是提出了一种时空关系网络同时处理多种线索，其本质上空间关系利用的是gcn的思想，时间上利用的attention的思想，然后再将位置信息与表观信息融合来进行度量学习。</p>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Spatial-Temporal Relational Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-High-speed tracking with kernelized correlation filters</title>
      <link href="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/"/>
      <url>/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/</url>
      
        <content type="html"><![CDATA[<p>​        KCF是一种鉴别式追踪方法，这类方法一般都是在追踪过程中训练一个目标检测器，使用目标检测器去检测下一帧预测位置是否是目标，然后再使用新检测结果去更新训练集进而更新目标检测器。而在训练目标检测器时一般选取目标区域为正样本，目标的周围区域为负样本，当然越靠近目标的区域分为正样本的概率越高。</p><p>​        本篇博文希望借这篇文章阐述KCF的原理和过程，以及存在的一些问题。</p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>​        KCF是一种鉴别式追踪方法，这类方法一般都是在追踪过程中训练一个目标检测器，使用目标检测器去检测下一帧预测位置是否是目标，然后再使用新检测结果去更新训练集进而更新目标检测器。而在训练目标检测器时一般选取目标区域为正样本，目标的周围区域为负样本，当然越靠近目标的区域分为正样本的概率越高。</p><p>​        本篇博文希望借这篇文章阐述KCF的原理和过程，以及存在的一些问题。</p><!-- more --><p>原发表于<a href="https://www.cnblogs.com/YiXiaoZhou/p/5925019.html" target="_blank" rel="noopener">博客园</a></p><hr><h3 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h3><p>​        论文中关于向量是行向量还是列向量总是指示不清楚，所以本文对变量符号统一之后进行推导，首先所有的小写字母均表示列向量，所有的大写字母表示矩阵，其中矩阵的每一行是一个样本，文中的函数除了$\phi(h)$是对行向量操作，其余都是对元素操做的，四则运算符号也都是针对元素操作的。还有所有对循环矩阵使用傅里叶变换时使用的生成向量都是循环矩阵的第一行向量，这点很重要。</p><h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><ul><li>使用目标周围区域的循环矩阵采集正负样本，利用脊回归训练目标检测器，并成功的利用循环矩阵在傅里叶空间可对角化的性质将矩阵的运算转化为向量的Hadamad积，即元素的点乘，大大降低了运算量，提高了运算速度，使算法满足实时性要求。</li><li>将线性空间的脊回归通过核函数映射到非线性空间，在非线性空间通过求解一个对偶问题和某些常见的约束，同样的可以使用循环矩阵傅里叶空间对角化简化计算。</li><li>给出了一种将多通道数据融入该算法的途径。</li></ul><hr><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="一维脊回归"><a href="#一维脊回归" class="headerlink" title="一维脊回归"></a>一维脊回归</h4><p>给定训练集$(x_i, y_i)， x_i\in R^{1\times n}, y_i\in R$, 其线性回归函数表示为$f(x_i) = w^Tx_i$, 回归系数$w\in R^{n\times 1}$ 可以通过最小二乘法计算：</p><script type="math/tex; mode=display">w^* = \min_w\sum_i(f(x_i)-y_i)^2 + \lambda \Vert w\Vert^2</script><p>$\lambda$是正则化系数， 限制模型的VC维从而保证模型泛化性能。</p><p>其矩阵形式表示为:</p><script type="math/tex; mode=display">w^* = \min_w \Vert Xw - y\Vert^2 +\lambda \Vert w\Vert^2</script><p>$X=[x_1, x_2, …,x_n]^T$. </p><p>采用最小二乘法求解，可得：</p><script type="math/tex; mode=display">w^* = (X^TX + \lambda I)^{-1}X^Ty</script><p>将模型扩展到复数空间，即：</p><script type="math/tex; mode=display">w^* = (X^HX+\lambda I)^{-1}X^Hy</script><p>$X^H$是$X$的复共轭矩阵。</p><h4 id="循环矩阵"><a href="#循环矩阵" class="headerlink" title="循环矩阵"></a>循环矩阵</h4><p>KCF中 训练样本是由目标样本经过循环位移得到。向量的循环可以使用排列矩阵获得, 下式中$P, Q$分别表示循环右移和左移矩阵：</p><script type="math/tex; mode=display">x_i = [x_{i1}, x_{i2},\dots x_{in}]^T\\P = \left[\begin{array}{ccccc}0, 0, \cdots, 0, 1\\1, 0, \cdots, 0, 0\\0, 1, \cdots, 0, 0\\...\\0, 0, \cdots, 1, 0\end{array}\right],~~~~Q= P^T\\Px_i = [x_{in}, x_{i1},\dots x_{i(n-1)}]^T\\x_iQ = [ x_{i2},\dots x_{in}, x_{i1}]^T</script><p>因此样本向量经过不断的左乘或者右乘排列矩阵，就可以实现循环位移，多个循环结果按序stack就成为循环矩阵$C(x)$如下图：</p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/1D xunhuan.png" alt="1D循环矩阵"></p><p>​                                                                        1D 循环矩阵示意    </p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/2D xunhuan.png" alt="2d xunhuan"></p><p>​                                                                        2D 图像循环示例</p><h4 id="循环矩阵傅氏空间对角化"><a href="#循环矩阵傅氏空间对角化" class="headerlink" title="循环矩阵傅氏空间对角化"></a>循环矩阵傅氏空间对角化</h4><p>所有循环矩阵$C(x)$都能够在傅氏空间中使用离散傅里叶变换进行对角化</p><script type="math/tex; mode=display">X = C(x) = F \cdot \text{diag}(\hat{x}) \cdot F^H</script><p>x是循环矩阵的生成向量，即对应于$C(x)$的第一行。 $\hat{x} = \mathcal{F}(x) =\sqrt{n}Fx$是$x$的傅里叶变换， $F$是傅里叶变换矩阵，常量：</p><script type="math/tex; mode=display">F=\frac{1}{\sqrt{n}}\left[\begin{array}{lllll}1,~~~~1,~~~\cdots,~~~~~~~1, ~~~~1\\1,~~~ w, ~~~\cdots, ~~~~~w^{n-2}, w^{n-1}\\~~~~~~~~~~~~\cdots \\1, w^{n-1}, \cdots, w^{(n-1)(n-2)}, w^{(n-1)^2}\end{array}\right]</script><p>关于矩阵傅里叶对角化请参考<a href="http://blog.csdn.net/shenxiaolu1984/article/details/50884830" target="_blank" rel="noopener">循环矩阵傅里叶对角化</a>., $F$是酉矩阵， 即$FF^H=F^HF=I$</p><h4 id="傅氏对角化简化的脊回归"><a href="#傅氏对角化简化的脊回归" class="headerlink" title="傅氏对角化简化的脊回归"></a>傅氏对角化简化的脊回归</h4><p>将$X=F\text{diag}(\hat{x})F^H$代入公式(4).</p><script type="math/tex; mode=display">\begin{align}w & = (F\cdot\text{diag}(\hat{x}^*)\cdot F^HF\cdot \text{diag}(\hat{x})\cdot F^H + \lambda F^HF)^{-1}F\cdot \text{diag}(\hat{x}^*)\cdot F^Hy \\&= (F\cdot\text{diag}(\hat{x}^*\odot\hat{x}+\lambda)\cdot F^H)^{-1}F\cdot\text{diag}(\hat{x}^*)F^Hy\\&= F\cdot \text{diag}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda})\cdot F^Hy\end{align}</script><p>有公式(6)可以进一步得到：</p><script type="math/tex; mode=display">\begin{align}w &= C(F^{-1}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda}))y \\\hat{w}&=\mathcal{F}^*(F^{-1}(\frac{\hat{x}^*}{\hat{x}^*\odot\hat{x}+\lambda})) \odot \mathcal{F}(y)\\&= \frac{\hat{x}\odot \hat{y}}{\hat{x}^*\odot\hat{x}+\lambda}\\w &= \mathcal{F}^{-1}(\hat{w})\end{align}</script><p>这是因为$\mathcal{F}(C(x)y) = \mathcal{F}^*(x)\odot \mathcal{F}(y)$</p><p>于是可以使用向量电机代替矩阵运算，特别是牵涉到矩阵求逆运算。</p><h4 id="核空间的脊回归"><a href="#核空间的脊回归" class="headerlink" title="核空间的脊回归"></a>核空间的脊回归</h4><p>我们希望将样本数据映射到非线性空间内再进行回归，以期待新的空间内可分。于是新空间内回归函数为$f(x_i)=w^T\phi(x)$, 其中$\phi(\cdot)$是核函数。于是 $w = \min_w \Vert(\phi(X)w - y)\Vert^2 +\lambda \Vert w\Vert^2$</p><p>理想情形$w$是$\phi(X)w - y$零空间的向量，所以$w$可以由$\phi(X)=[\phi(x_1, x_2, \cdots, x_n)]^T$线性表示，于是$w=\sum_i \alpha_i\phi(x_i)=\phi(X)^T\alpha$, 于是</p><script type="math/tex; mode=display">\alpha = \min_a \Vert \phi(X)\phi(X)^T\alpha - y\Vert^2 + \lambda\Vert \phi(X)^T\alpha\Vert^2</script><p>该问题是$w$的对偶问题。</p><p>对$\alpha$计算导数并令其为0， 得到</p><script type="math/tex; mode=display">\begin{align}J(\alpha) &=  \alpha^T \phi(X)\phi(X)^T\phi(X)\phi(X)^T\alpha - 2y^T\phi(X)\phi(X)^T\alpha +C_{constant} + \lambda \alpha^T\phi(X)\phi(X)^T\alpha \\&= 2\phi(X)\phi(X)^T\phi(X)\phi(X)^T\alpha + 2\lambda \phi(X)\phi(X)^T\alpha - 2\phi(X)\phi(X)^Ty = 0\\\bar{\alpha} &= (\phi(X)\phi(X)^T+\lambda I)^{-1}y\end{align}</script><p>此处推导用到$\phi(X)\phi(X)^T$是协方差矩阵一定可逆的知识。</p><p>核方法中我们不知道核函数$\phi(\cdot)$的具体形式，但可以构造核矩阵$K=\phi(X)\phi(X)^T$, 于是$\bar{\alpha} = (K+\lambda I)^{-1}y$</p><script type="math/tex; mode=display">f(z) = w^T\phi(z) = \alpha^T\phi(X)\phi(z)</script><p>如果希望计算$\alpha $也可以利用傅氏空间对角化知识，那么$K$是一个循环位移矩阵。</p><blockquote><p><strong>Theorem 1.</strong> Given circulant data $C(x)$, the corresponding kernel matrix $K$ is circulatant if the kernel function satisfies $K(x, x’) = K(Mx, Mx’)$,for any permutation matrix $M$.<br>即核矩阵是循环矩阵应该满足两个条件：第一个样本和第二个样本都是由生成样本循环移位产生的，可以不是由同一个样本生成；满足$K(x, x’) = K(Mx, Mx’)$,其中$M$是排列矩阵。</p></blockquote><p>证明： 设$x\in R^n$, 则其生成向量$x’ = P^ix, \forall x’\in C(x)$, 于是：</p><script type="math/tex; mode=display">\begin{align}K_{ij} &= \phi(x_i)^T\phi(x_j)\\&= K(x_i, x_j)\\&= K(P^ix, P^jx) \\&= K(P^{-i}P^ix, P^{-i}P^{j}x), ~~\text{cause}~~ K(x, x')=K(Mx, Mx')\\&= K(x, P^{j-i}x) \\&= \phi(x)^T\phi(P^{j-i}x) \\&= \phi(x)^T\phi(x_{j-i})\end{align}</script><p>从而</p><script type="math/tex; mode=display">K_{0\cdot} = [\phi(x_0)^T\phi(x_0), \phi(x_0)^T\phi(x_1), ..., \phi(x_0)^T\phi(x_{n-1})]\\K_{1\cdot} = [\phi(x_0)^T\phi(x_{n-1}), \phi(x_0)^T\phi(x_0), ..., \phi(x_0)^T\phi(x_{n-2})]</script><p>这里，$j-i$表示位移数目， 负数表示左移数目。</p><p>因此可以证明Theorem1。</p><p>所以，只要选取核函数满足$K(x,x’)=K(Mx, Mx’)$， 那么核矩阵就是循环矩阵。当核矩阵是循环矩阵时，</p><script type="math/tex; mode=display">\begin{align}\alpha &= F\cdot \text{diag}(\hat{K}^{xx}+\lambda)^-1\cdot F y\\\hat{\alpha} &= \frac{\hat{y}}{(\hat{K}^{xx}+\lambda)^*}\\&= \frac{\hat{y}}{(\hat{K}^{xx}+\lambda)}\end{align}</script><p>其中$K^{xx}=\phi(x)^T\phi(X)^T$是核矩阵的生成向量，也是矩阵的第一行。</p><p><strong>注意，核函数一般满足对称性，即$K(x,y)=K(y,x)$, 于是可以证明$K^{xx}$是对称向量，即$K^{xx}_{0i} = K^{xx}_{0(n-i)}$, 对称向量的傅里叶变换为实数，因此其共轭可以忽略</strong></p><p>满足上述性质的核函数包括：</p><ul><li>Radial Basis Function kernels -e.g. Gaussian</li><li>Dot-Product kernels -e.g. linear, polynomial</li><li>Additive kernels - e.g. intersection, $\chi^2$ and Hellinger kernels</li><li>Exponentiated additive kernels.</li></ul><h4 id="快速检测"><a href="#快速检测" class="headerlink" title="快速检测"></a>快速检测</h4><p>首先由训练样本和标签训练检测器，其中训练集是由目标区域和由其移位得到的若干样本组成，对应的标签是根据距离越近正样本可能性越大的准则赋值的，于是得到训练样本集和对应的标签，采用上一节的核空间回归，可以计算得到$\alpha$</p><p>在检测样本时，认为待检测样本是target位移得到的，即$z_j = P^jz$, 于是其对应的回归值为$f(z_j) = \alpha^T\phi(X)\phi(z_j)$, 回归值最大则表明最可能是目标。</p><p>定义核矩阵$K^z= \phi(X)\phi(Z)^T$, 即 $K_{ij}^z = \phi(z_i)^T\phi(x_j)$, 由定理1可确定$K^z$是循环矩阵。</p><p> 于是在检测时，每个测试样本的回归值如下计算：</p><script type="math/tex; mode=display">\begin{align}f(z) &= (\alpha^T\phi(X)\phi(Z)^T)^T\\&= (K^z)^T\alpha\\&= F\cdot \text{diag}(\hat{x}^{xz})\cdot F^H\alpha\\\hat{f}(z) &= (\hat{K}^{xz})^*\hat{\alpha}\end{align}</script><p>$K^{zx}$是$K^z$的生成向量。</p><h4 id="核矩阵的快速计算"><a href="#核矩阵的快速计算" class="headerlink" title="核矩阵的快速计算"></a>核矩阵的快速计算</h4><p>目前计算瓶颈还有核矩阵的生成向量的计算。</p><h5 id="内积和多项式核"><a href="#内积和多项式核" class="headerlink" title="内积和多项式核"></a>内积和多项式核</h5><p>即$K(x_i, x_j’) = g(x_i^Tx_j’)$于是</p><script type="math/tex; mode=display">\begin{align}K^{xx'} &= g(C(x)x')^T \\&= g(\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}''))^T\end{align}</script><p>因此对于多项式核$K(x_i, x_j) = (x_i^Tx_j + a)^b$有</p><script type="math/tex; mode=display">K^{xx'} =((\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}''))^T+a)^b</script><h5 id="径向基核函数"><a href="#径向基核函数" class="headerlink" title="径向基核函数"></a>径向基核函数</h5><p>这类核函数重点在于计算$\Vert x_i-x_j\Vert^2$</p><p>$\Vert x_i-x_j\Vert^2 = \Vert x_i\Vert^2 + \Vert x_j\Vert^2 - 2 x_i^Tx_j$</p><p>所以</p><script type="math/tex; mode=display">K^{xx'} = h(\Vert x\Vert^2 + \Vert x' \Vert^2 -2\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}'))^T</script><p>对于高斯核有：</p><script type="math/tex; mode=display">K^{xx'} = exp(\frac{1}{\sigma^2}\Vert x\Vert^2 + \Vert x' \Vert^2 -2\mathcal{F}^{-1}(\hat{x}^*\odot \hat{x}'))^T</script><h4 id="1D-to-2D"><a href="#1D-to-2D" class="headerlink" title="1D to 2D"></a>1D to 2D</h4><p>2D数据时可以直接在核空间使用傅里叶变换对角化循环矩阵的特征，加速运算。</p><p>现有一个核函数$\varphi(\cdot)$, 自变量$X_i\in R^{m\times n}$, 于是$\varphi(X_i)\in R^k$, 于是核空间采用脊回归公式</p><script type="math/tex; mode=display">\alpha = (K+\lambda I)^{-1}y</script><blockquote><p>Theorem 2. The block matrix $K$ with elements $K_{(ii’)(jj’)} = \mathcal{K}(P^iXP^{i’}, P^jXP^{j’})$ is a Block-Circulant Matrix if $\mathcal{K}$ is a unitary invariant kernel.</p><p>Here, unitary invariant kernel satisfies $\mathcal{K}(X, X’) = \mathcal{K}(P^iXP^{i’}, P^iX’P^{i’})$</p></blockquote><p>块循环矩阵可以使用2D傅里叶变换实现矩阵对角化</p><script type="math/tex; mode=display">K = F_2\cdot \text{diag}(\hat{K}')\cdot F_2^H</script><p>其中$F_2$是2D傅里叶变换矩阵， $K’$是生成块循环矩阵的生成矩阵， $\hat{K}$表示对矩阵$K$进行2D傅里叶变换。</p><p>于是</p><script type="math/tex; mode=display">\alpha = F_2\cdot (\text{diag}(\hat{K}'+\lambda 1^m(1^n)^T))\cdot F_2^Hy</script><p>其中$1^m, 1^n$表示全1向量。</p><p>由公式6可得：</p><script type="math/tex; mode=display">\hat{\alpha}_M = (\hat{K}'+\lambda 1^m(1^n)^T)*\odot \hat{y}_M</script><p>$\alpha_M, y_M$分别表示矩阵形式的系数和标签。</p><p>于是回归值为</p><script type="math/tex; mode=display">\hat{f}_M = \hat{K}^{XZ}\odot \hat\alpha_M</script><p>其中$K^{XZ}$表示块循环矩阵的生成矩阵。</p><p>再往后的检测部分与1D类似，不作赘述。</p><h4 id="多通道问题"><a href="#多通道问题" class="headerlink" title="多通道问题"></a>多通道问题</h4><p>论文中在提取目标区域的特征时可以是灰度特征，但是使用Hog特征能够取得更好的效果，那么Hog特征该如何加入前面提到的模型呢？</p><p>Hog特征是将图像划分成较小的局部块，称为cell，在cell里提取梯度信息，绘制梯度方向直方图，然后为了减小光照影响，将几个cell的方向直方图串在一起进行block归一化，最终将所有的cell直方图串联起来就是图像的特征啦。</p><p>那么，按照传统的方式一张图像就提取出一个向量，但是这个向量怎么用啊？我们又不能通过该向量的移位来获得采样样本，因为，你想啊，把直方图的一个bin循环移位有什么意义啊？</p><p>所以论文中Hog特征的提取是将sample区域划分成若干的区域，然后再每个区域提取特征，代码中是在每个区域提取了32维特征，即划分9个梯度方向， 每个防线提取3个特征，然后再加上4个表观纹理特征和一个0元素表示阶段特征，详情参考<a href="http://www.cs.berkeley.edu/~rbg/latent/index.html" target="_blank" rel="noopener">FHOG</a>. 共 $3x9+5=32$维特征，于是可以使用$m\times n \times 32$表示图像，其中$m,n$是划分区域的网格数。那么这么操作之后矩阵为位移可以近似于patch块的位移，也就是网格的位移，于是矩阵的循环位移就是网格特征$m\times n \times 31$的每一通道上的循环位移，这里最后一维特征全0不考虑。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><ul><li><p>KCF相对于以前的单目标跟踪方法速度提升明显，性能也较好，思路简单。</p><p><img src="/2019/05/18/High-speed-tracking-with-kernelized-correlation-filters/KCF example.png" alt="KCF example"></p><p>​                                                                            KCF example</p></li></ul><p>借上图来总结下KCF的过程，左图是刚开始我们使用红色虚线框框定了目标，然后红色实线框就是使用的padding了，其他的框就是将padding循环移位之后对齐目标得到的样本，由这些样本就可以训练出一个分类器，当分类器设计好之后，来到了下一帧图像，也就是右图，这时候我们首先在预测区域也就是红色实线框区域采样，然后对该采样进行循环移位，对齐目标后就像图中显示的那个样子 了，（这是为了理解，实际中不用对齐。。。），就是周围那些框框啦，使用分类器对这些框框计算响应，显然这时候白色框响应最大，因为他和之前一帧红色框一样，那我们通过白色框的相对移位就能推测目标的位移了。然后继续，再训练再检测。。。。</p><p>论文中还提到几点：</p><ol><li>对特征图像进行cosine window加权，这主要是为了减轻由于边界移位导致图像不光滑。</li><li>padding的size是目标框的2.5倍，肯定要使用padding窗口，要不然移位一次目标就被分解重组合了。。。效果能好哪去。。</li><li>对于标签使用了高斯加权。 这点尤其重要。</li><li>对$\alpha$前后帧结果进行了线性插值，为了让他长记性，不至于模型剧烈变化。</li></ol><ul><li>KCF的不足点<ol><li>依赖循环矩阵，对于多尺度的目标跟踪效果并不理想。当然可以通过设置多个size，在每个size上进行KCF运算，但这样的话很难确定应预先设置多少size，什么样的size，而且对size的遍历必将影响算法的速度。KCF最大的优势就是速度。</li><li>初始化矩阵不能自适应改变，其实这个问题和上一个缺点类似，这里强调的是非刚体运动，比如跳水运动员，刚开始选定区域肯定是个瘦长的矩形框，但当运动员开始屈体的时候显然这个预选定框就很大误差了。</li><li>难处理高速运动的目标， 高速运动目标很容易超出候选区域。</li><li>难处理低帧率中目标，这个和3类似，都是说相邻帧间目标位移过大。</li></ol></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> correlation filter </tag>
            
            <tag> kernelized correlation filter </tag>
            
            <tag> tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-Frame-wise Motion and Appearance for Real-time Multiple Object Tracking</title>
      <link href="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/"/>
      <url>/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/architecture.png" alt="architecture"></p><p>文章同时利用了光流信息和表观信息去解决多目标跟踪任务。对于简单的目标使用Frame-wise Motion Field(FMF)进行跟踪，而对于复杂的目标使用Frame-wise Appearance Features(FAF)进行reid的匹配。 两个模块集成在一起实现端到端的训练，同时也缩短了inference的时间。</p><a id="more"></a><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li><p>文章认为MOT当前的主要挑战在于“不同帧中目标个数不定带来算法效率下降”。 </p><ul><li>LSTM 只处理了single object。(也有处理多目标的)</li><li><p>Re-ID exhausitively 匹配目标表观。（这个不客观，大多数方法都会通过先验条件约束范围）</p></li><li><p>单个box处理耗时严重，因为需要crop+resize+extract features</p></li></ul></li><li><p>文章针对该问题提出一种同时 关联不定数目目标的Deep Neural Network (DNN)</p><ul><li>Frame-wise Motion Fields (FMF) 估计目标运动位置，进行初步匹配。</li><li>Frame-wise Apearance Features(FAF)针对于FMF失败情形，采用表观再次匹配</li></ul></li><li><p>在MOT17 benchmark上保持SOTA性能同时，速度大幅提升。</p></li></ul><hr><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>主要贡献点：</p><blockquote><p>1) Frame-wise Motion Fields (FMF) to represent the association among indefinite number of objects between frames.</p><p>2) Frame-wise Apearance Feature (FAF) to provide Re-ID features to assist FMF-based object association.</p><p>3) A simple yet effective inference algorithm to link the objects according to FMFs, and to fix a few uncertain associations using FAFs. </p><p>4)  Experiments on the challenging MOT17 benchmark show that our method achieves real-time MOT with competitive performance as the state-of-the-art approaches.</p></blockquote><hr><h3 id="Proposed-Method-FMA"><a href="#Proposed-Method-FMA" class="headerlink" title="Proposed Method (FMA)"></a>Proposed Method (FMA)</h3><p>FMA方法网络结构如下图：</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/architecture.png" alt="architecture"></p><h4 id="Frame-wise-Motion-Fields"><a href="#Frame-wise-Motion-Fields" class="headerlink" title="Frame-wise Motion Fields"></a>Frame-wise Motion Fields</h4><p>使用$I_1, I_2\in R^{w\times h\times 3}$分别表示时间先后的两帧图像， $b_i$表示第$i$个关联目标的bounding box， 两帧中$b_i$的$x,y$坐标分别表示为$\mathcal{X}_1(b_i), \mathcal{Y}_1(b_i), \mathcal{X}_2(b_i), \mathcal{Y}_2(b_i)$ , FMFs计算了两帧图像$I_1, I_2$之间的四个运动场信息：</p><script type="math/tex; mode=display">\begin{align}F_x^1(b_i) &= \mathcal{X}_2(b_i) - \mathcal{X}_1(b_i),\\F_y^1(b_i) &= \mathcal{Y}_2(b_i) - \mathcal(Y)_1(b_i)\\F_x^2(b_i) &= -F_x^1(b_i),\\F_y^2(b_i) &= -F_y^1(b_i)\end{align}</script><p>这里预测了目标的双向motion向量，主要是为了让motion向量更加鲁棒。两方面考虑： 1）前后两帧的表观和上下文信息可能差异较大，拆开考虑更鲁棒；2）可以解决部分遮挡问题，前后分别预测。示例如下图所示</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/FMFs.png" alt="FMFs"></p><p>注意上图(b)(d)中黑色十字标注的中心点.</p><p>损失函数：</p><script type="math/tex; mode=display">\mathcal{L}_{MSE} = \sum_{k=1}^2\sum_{b_i\in B}\Vert H_x^k(b_i)-F_x^k(b_i)\Vert_F^2 +  \Vert H_y^k(b_i)-F_y^k(b_i)\Vert_F^2</script><p>$B$是关联的目标的bounding box的集合。</p><h4 id="Frame-wise-Appearance-Features"><a href="#Frame-wise-Appearance-Features" class="headerlink" title="Frame-wise Appearance Features"></a>Frame-wise Appearance Features</h4><p>FMF可以处理简单的情况，但是碰到拥挤或者干扰太多的时候就会预测失败，这时候希望通过FAF模块采用ReID实现匹配。</p><p>传统的ReID方法将每一个目标crop出来进行resize之后放到网络中抽取特征，耗时严重。该方法中利用FAF模块直接从整张图像中抽取每个目标的特征，同时FAF和FMF模块可以共享部分网络层。从而前向推理速度加快的同时也能够规避目标个数不同带来的问题。</p><p>给定同一个目标在两帧图像中的bounding boxes， FAF模块从FMF抽取的特征中crop patches，然后计算相似度。 在训练过程中同一个目标的不同特征concat一起作为正样本， 不同样本特征concat一起作为负样本。正负样本比例控制在$1:4$, 损失采用交叉熵损失</p><script type="math/tex; mode=display">\mathcal{L}_{BCE} -\frac{1}{N}\sum_{j=1}^N\mathcal{S}_j\log h(p_j) + (1-\mathcal{S_j})\log (1-h(p_j))</script><p>$p_j, \mathcal{S_j}$表示样本和对应label。</p><h4 id="Inference-Algorithm"><a href="#Inference-Algorithm" class="headerlink" title="Inference Algorithm"></a>Inference Algorithm</h4><p>$D=\{D_1, \cdots, D_N\}, T=\{T^1, \cdots, T^M\}$分别表示当前时刻的检测结果和已经存在的跟踪轨迹。 $IOU(\cdot)$表示IOU算子， $SIM(\cdot)$表示表观相似度。 Inference Algorithm分为3步：</p><ul><li>从former 到latter进行关联</li><li>从latter 到former进行关联</li><li>关联失败剩下的tracks和detections，利用FAFs进行关联。</li></ul><p>IOU和ReID的阈值分别为$\tau_1, \tau_2$</p><hr><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>Datasets: MOT17 benchmark</p><p>Setting: </p><p>​    训练样本对的选择： 两种时间间隔 每一帧和每4zhen</p><p>​    batchsize=4； lr=0.001, 70个epoch之后变为0.0001。</p><p>​    测试时样本对：连续两帧</p><p>​    $\tau_1=0.45, \tau_2=0.5$</p><h5 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h5><ol><li><p>MOT result of each component</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table2.png" alt="tab2"></p><p>训练的使用同时训练FAF和FMF模块，然后使用每个模块进行跟踪。</p><p>性能差别不是很大，但速度差异明显。 作者认为FAF需要对图像进行crop因此速度较慢。</p></li><li><p>Effectiveness of FMFs</p><p>根据FMFs, 利用IOU初步匹配效果如下：这张图分辨率太低，根本得不到什么结论。</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/FMFs_experiments.png" alt="FMFs"></p></li></ol><h5 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h5><ol><li><p>性能对比</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table3.png" alt="table3"></p><p>结论： a) 基于深度特征的方法性能优于传统特征； b）本文方法性能和MOTDT方法性能类似，但速度更快。</p><p>分析原因：a)，训练数据太少，只在MOT17训练集上训练，b)该方法比较依赖于准确的检测结果</p></li><li><p>检测器对比</p><p><img src="/2019/05/17/Frame-wise-Motion-and-Appearance-for-Real-time-Multiple-Object-Tracking/table4.png" alt="table4"></p><p>结论： 本文方法更适合于准确地检测器。</p></li></ol><hr><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><blockquote><p>Practical and real-time MOT has to scale well with indefinite number of objects. This paper addresses this problem with frame-wise representations of object motion and appearance. In particular, FMFs simultaneously handle forward and backward motions of all bounding boxes in two input frames, which is the key to achieve real-time MOT inference. FAFs helps FMFs in handling some hard cases without significantly compromising the speed. The FMFs and FAFs are efficiently used in our inference algorithm, and achieved faster and more competitive results on the MOT17 benchmark. Our frame-wise representations are very efficient and general, making it possible to achieve real-time inference on more computationally expensive tracking tasks, such as instance segmentation tracking and scene mapping.</p></blockquote><hr><h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><p>或许是因为preprint版本的原因，文章存在一些问题没有阐述明白。</p><ol><li>计算FMF的时候样本点的个数太少，过于稀疏是如何训练的？</li><li>训练细节没有提供多少epoch等</li><li>FAF进行crop时是从那一层crop的？</li><li>一般而言训练reid网络时，单独采用交叉熵损失似乎都不能取得较好效果。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Motion and Appearance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读笔记-MOT16:A Benchmark for Multi-Object Tracking</title>
      <link href="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/"/>
      <url>/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/datasets_overview.png" alt="dataset overview"></p><p>这篇文章主要介绍了MOT的2016 benchmark库。相对于MOT15的benchmark而言，MOT16 benchmark视频数据标注更加规范严格，除了标注pedestrian之外，还标注了其他部分类别，同时给出了每个标注的可见度。新的benchmark数据也更加多样。</p><a id="more"></a><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>这篇文章主要介绍了MOT的2016 benchmark库。相对于MOT15的benchmark而言，MOT16 benchmark视频数据标注更加规范严格，除了标注pedestrian之外，还标注了其他部分类别，同时给出了每个标注的可见度。新的benchmark数据也更加多样。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul><li><p>MOT15 benchmark存在的不足：</p><ul><li>数据来源不同，标注协议也不完全相同。</li><li>测试集和训练集中人群密度分布不均衡。</li><li>部分训练集过于简单，不利用实际训练。如PETS09-S2L1.</li><li>benchmark中提供的检测结果太差，导致跟踪IDs较高。</li></ul></li><li><p>MOT16 benchmark的改进点：</p><ul><li>14段视频数据，分别取自crowed scenarios, different viewpoints, camera motions 和 weather conditions， 训练数据足够复杂。</li><li>所有的序列均采用相同的标注协议进行标注。</li><li>除了pedestrian之外还标注了一些其他类别，提供训练。</li></ul></li><li><p>MOT16 benchmark网站：</p><p>​    <a href="http://www.motchallenge.net" target="_blank" rel="noopener">MOTChallenge</a></p></li></ul><h4 id="MOT16-datasets"><a href="#MOT16-datasets" class="headerlink" title="MOT16 datasets"></a>MOT16 datasets</h4><ul><li><p>数据集overview</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/datasets_overview.png" alt="dataset overview"></p><p>MOT16 benchmark 数据集示意。上面一行是训练集，下面一行是测试集。</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/overview_table.png" alt="overview_table"></p></li><li><p>MOT16 benchmark 提供的检测结果</p><p>DPM检测结果。（另外，当前网站提供了FRCNN， SDP等多种检测结果）</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/detections statistics.png" alt="detection box statistics"></p><p><strong>detection 存储格式</strong></p><p>​    示例</p>   <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1, -1, 794.2, 47.5, 71.2, 174.8, 67.5, -1, -1</span><br><span class="line">1, -1, 164.1, 19.6, 66.5, 163.2, 29.4, -1, -1</span><br><span class="line">1, -1, 875.4, 39.9, 25.3, 145.0, 19.6, -1, -1</span><br><span class="line">2, -1, 781.7, 25.1, 69.2, 170.2, 58.1, -1, -1</span><br></pre></td></tr></table></figure><p><code>det.txt</code>文件的每一行存储一个box信息， 使用逗号隔开的9(实际是10)个数值表示。第一个数值表示box所在的帧号，第二个数值表示当前box对应的target id，对于detection而言，target id未知，都标注为-1；第三到六个值表示box的坐标，即<code>(topleft_x, topleft_y, width, height)</code>,<strong>标注从1开始</strong>；第七个值表示detections的置信度， 剩下三个值占位，全为 $-1$ 。</p></li></ul><h4 id="MOT16-annotation"><a href="#MOT16-annotation" class="headerlink" title="MOT16 annotation"></a>MOT16 annotation</h4><ul><li><p>target class</p><p>MOT16 benchmark标注可以分为三类</p><blockquote><p>(i) moving or standing pedestrians;<br>(ii) people that are not in an upright position or artificial representations of humans; and<br>(iii) vehicles and occluders.</p></blockquote><p>第一类的目标是希望跟踪的目标，包括所有的moving或者upright standing的行人，以及骑车或者滑板的也算。另外弯腰捡东西或者弯腰和孩童谈话的目标在评估时也会纳入考虑。</p><p>第二类目标没办法确切分类。比如非upright姿态的人，以及海报，倒影，镜像等，还用在玻璃之后的人都属于distractors。这类目标在评估时既不惩罚也不奖励，也就是说跟踪算法对这类数据的处理效果不影响最终评估性能。</p><p>第三类目标是各种车辆等遮挡物，这类目标在评估时也不考虑，标注的主要目的是提供额外信息用于估计遮挡程度等。</p></li><li><p>bounding box alignment</p><ul><li>box是目标的外接矩形框，所有的目标区域都在box内，所以对于行人而言，其宽度变化较剧烈。</li><li>对于遮挡目标，或者超出视野的目标，根据可见部分，估计整个目标的box。</li><li>如果遮挡的目标没法用一个box准确标注，那么可能用多个box标注。这主要针对于tree的大目标。</li><li>运输工具上的行人，只有充分可见时才会标注。比如汽车中的人不标注，自行车或者摩托上的人标注。</li></ul></li><li><p>start and end of trajectories</p><ul><li>轨迹尽可能的长。the annotation starts as early and ends as late as possible such that the accuracy is not forfeited</li><li>离开视野的目标再次出现时被赋予新的轨迹编号。</li></ul></li><li><p>minimal size 和 occlusions</p><ul><li>标注时对于size没有限制，多小的目标都会标注。</li><li>遮挡程度是计算出来的，不是显式指定的。</li><li>当目标被完全遮挡且不再可见时，认为该目标终止。</li><li>如果一个目标长时间遮挡后再次出现，但是位置变化很大，那么赋予新的轨迹id。</li></ul></li><li><p><code>gt.txt</code>存储格式</p><p>和<code>det.txt</code>类似，每一行存储10个数值表示当前box的跟踪结果。每个值的意义对应于下表</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/data_format.png" alt="data_format"></p><p>target所属类别编号如下：</p><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/class_map.png" alt="class map"></p></li></ul><h4 id="MOT16-evaluation"><a href="#MOT16-evaluation" class="headerlink" title="MOT16 evaluation"></a>MOT16 evaluation</h4><ul><li><p>tracker-to-target assignment</p><p>这部分主要包括指标FN, FP, FAF,用来评估检测目标和真实目标的匹配程度，<strong>注意，只是box的匹配程度，而没有考虑label是否一致</strong> </p><p>FN: False Negative,  所有的gt中没有匹配到检测的个数</p><p>FP: False Positive, 所有检测中没有匹配到gt的个数</p><p>FAF: the number of false alarms per frame, 也称为 FPPI: false positives per image</p></li><li><p>匹配的一致性</p><p>使用IDS指标，评估跟踪的一致性。IDS表示一条跟踪轨迹和真实轨迹在第$i$帧关联成功，但之前最近的时刻没有关联，则认为发生了一次ID switch。</p><p><strong>值得注意的是， 检测和gt是否关联取决于两者之间的距离，比如IoU等，而当多个IoU符合条件时选择最大的IoU关联。但是在MOT中为了尽可能保持轨迹跟踪的一致性，如果$t-1$时刻truth object $i$ 和假设$j$ 匹配，但是$t$时刻虽然匹配，但不是最优匹配，这时候我们仍然选择$i,j$匹配。</strong></p><p>为了让IDS与recovered targets无关，最终选择IDS/Recall度量一致性。</p></li><li><p>Distance measure</p><blockquote><p>the intersection over union (a.k.a. the Jaccard index) is usually employed as the similarity criterion, while the threshold td is set to 0.5 or 50%.</p></blockquote></li><li><p>Target-like annotations</p><p>计算evaluations之前，关联假设与gt的步骤：</p><blockquote><p>1) At each frame, all bounding boxes of the result file are matched to the ground truth via the Hungarian algorithm. (使用匈牙利算法根据IoU关联）<br>2) All result boxes that overlap &gt; 50% with one of these classes (distractor, static person, reflection, person on vehicle) are removed from the solution.（类似于NMS，先把distractor匹配的假设删除）<br>3) During the final evaluation, only those boxes that are annotated as pedestrians are used. (剩下的在计算性能)</p></blockquote><p><img src="/2019/05/17/MOT16-A-Benchmark-for-Multi-Object-Tracking/assignment_examples.png" alt="annotations_examples"></p></li><li><p><strong>Multiple Object Tracking Accuracy</strong></p><script type="math/tex; mode=display">MOTA = 1- \frac{\sum_t(FN_t+FP_t+IDS_t)}{\sum_t GT_t}</script><p>MOTA取值范围为$(-\infty, 100)$</p></li><li><p>Multiple Object Tracking Precision</p><script type="math/tex; mode=display">MOTP = \frac{\sum_{t,i} d_{t,i}}{\sum_{t}c_t}</script><p>MOTP取值范围$(50, 100)$。这里只计算匹配成功的平均匹配度，而匹配成功的阈值是$50$.</p></li><li><p>Track quality measures</p><ul><li>mostly tracked (MT): 至少$80\%$ 关联跟踪</li><li>mostly lost (ML): 至少$80\%$关联失败</li><li>partially tracked (PT): 少于$20\%$关联成功</li></ul><p><strong>注意我这里说关联，而没说跟踪是因为不论跟踪的trackid是否相同，只要gt匹配到假设就认为关联成功</strong></p><p>MT， ML最终计算的是相对于ground truth trajectories的总数。</p><ul><li>track fragmentation(FM): 表示gt trajectory中的某个时刻没有关联成功，但前面存在关联成功，后续也关联成功，则认为是一次Frag，如上图中(b),(d)示意。与IDS类似，FM最终计算为FM/Recall</li></ul></li></ul><h4 id="Baseline-methods"><a href="#Baseline-methods" class="headerlink" title="Baseline methods"></a>Baseline methods</h4><p>这部分提供的baseline都太陈旧，性能很容易超越，所以需要新的baseline。</p><ul><li>DP_NMS</li><li>CEM</li><li>SMOT</li><li>TBD</li><li>JPDA_M</li></ul><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><blockquote><p>We have presented a new challenging set of sequences within the MOTChallenge benchmark. The 2016 sequences contain 3 times more targets to be tracked when compared to the initial 2015 version. Furthermore, more accurate annotations were carried out following a strict protocol, and extra classes such as vehicles, sitting people, reflections or distractors were also annotated to provide further information to the community. We believe that the MOT16 release within the already established MOTChallenge benchmark provides a fairer comparison of state-of-the-art tracking methods, and challenges researchers to develop more generic methods that perform well in unconstrained environments and on unseen data. In the future, we plan to continue our workshops and challenges series, and also introduce various other (sub-)benchmarks for targeted applications, e.g. sport analysis, or biomedical cell tracking.</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> MOT </tag>
            
            <tag> Benchmark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello world</title>
      <link href="/2019/05/17/hello-world/"/>
      <url>/2019/05/17/hello-world/</url>
      
        <content type="html"><![CDATA[<h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><p>使用Hexo + GithubPage创建个人网站总结与验证。</p><a id="more"></a><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ul><li><a href="https://www.simon96.online/2018/10/12/hexo-tutorial/" target="_blank" rel="noopener">最全Hexo博客搭建</a></li><li><a href="https://www.jianshu.com/p/8d28027fec76" target="_blank" rel="noopener">hexo+github上传图片到博客</a></li><li><a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo|Themes</a></li><li><a href="https://zongweizhou1.github.io/2019/05/17/hello-world/" target="_blank" rel="noopener">Hexo博客添加Latex</a></li></ul><h4 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h4><ol><li><p>创建新的md文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo n <span class="string">"new file"</span></span><br></pre></td></tr></table></figure></li><li><p>清除缓存</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure></li><li><p>打开本地服务，查看网页生成是否达到预期</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>然后浏览器输入<code>localhost:4000</code></p></li><li><p>上传到网站</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>此时便可登录网站查看生成的blog</p></li></ol><h4 id="主题选择"><a href="#主题选择" class="headerlink" title="主题选择"></a>主题选择</h4><p>主题的选择看个人需求，比如我博客内容更多的关于科研文章的解读，需要频繁的输入公式和贴图，论文的截图一般都是白色背景，所以我选择主题需要支持<code>mathjax</code>, 以及白色背景最好， 最终选择模板<a href="https://github.com/dongyuanxin/theme-bmw" target="_blank" rel="noopener">BMW</a></p><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><h5 id="测试公式是否能显示正常"><a href="#测试公式是否能显示正常" class="headerlink" title="测试公式是否能显示正常"></a>测试公式是否能显示正常</h5><p>行内公式： $y=\frac{\alpha^2+\sum_{i=1}^\beta\Phi(\hat{x}_i)}{\text{exp}(x^2)}$</p><p>行间公式：</p><script type="math/tex; mode=display">y=\begin{cases}x^2+z^2, \text{   if} x>z, \\(z-x)^2, \text{else}\end{cases}.</script><h5 id="测试图片是否能够正常显示"><a href="#测试图片是否能够正常显示" class="headerlink" title="测试图片是否能够正常显示"></a>测试图片是否能够正常显示</h5><p><img src="/2019/05/17/hello-world/girl.png" alt="girl"></p>]]></content>
      
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>基本信息</strong></p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:center">key</th><th style="text-align:center">value</th></tr></thead><tbody><tr><td style="text-align:center">nickname</td><td style="text-align:center">YiXiaoZhou</td></tr><tr><td style="text-align:center">birthday</td><td style="text-align:center">1994.11.01</td></tr><tr><td style="text-align:center">sex</td><td style="text-align:center">Male</td></tr><tr><td style="text-align:center">location</td><td style="text-align:center">CASIA-NLPR</td></tr><tr><td style="text-align:center">email</td><td style="text-align:center">cumtzhouzongwei@163.com</td></tr></tbody></table></div><blockquote><p><strong>学习经历</strong> </p></blockquote><ul><li>2009-2013. 中国矿业大学(徐州)， 计算机科学与技术学院，信息科学与技术专业，嵌入式方向理学学士。</li><li>2013-2016. 南京理工大学，计算机学院，模式识别与人工智能专业，工学硕士。</li><li>2016-至今.  中国科学院，自动化研究所，模式识别国家重点实验室，博士在读。</li></ul><blockquote><p><strong>发表文章</strong></p></blockquote><ul><li>周宗伟， 金忠。 非凸加权核范数及其在运动目标检测中的应用[J]. <em>中国图象图形学报</em>, 2015.</li><li>Zongwei Zhou, Zhong Jin.  Double nuclear norm-based robust principal component analysis for image disocclusion and object detection[J]. <em>Neurocomputing</em>. 2016.</li><li>Zongwei Zhou, Zhong Jin. Two-dimension principal component analysis-based motion detection framework with subspace update of background[J]. <em>IET Computer Vision</em>. 2016.</li><li>Qing Liu, Zhihui Lai, <em>Zongwei Zhou</em>, Fangjun Kuang, Zhong Jin. A truncated nuclear norm regularization method based on weighted residual error for matrix completion[J]. <em>IEEE Transactions on Image Processing</em>. 2016.</li><li>Zongwei Zhou, Junliang Xing, Mengdan Zhang, Weiming Hu. Online Multi-Target Tracking with Tensor-Based High-Order Graph Matching[C]. <em>International Conference on Pattern Recognition</em>, 2018.</li><li>Zongwei Zhou, et. al. Online Multi-Object Tracking via Distractor-aware Discrimination Learning[C]. ICCV2019, submitted.</li><li>Zongwei Zhou. et. al. A Unified Multi-cue Extraction Network for Online Multi-Object Tracking[C]. ACM MM2019. submitted.</li></ul><blockquote><p><strong>研究方向</strong></p></blockquote><ul><li>运动目标检测。 基于低秩理论实现运动目标的前背景分离。</li><li>多目标跟踪。</li><li>身份重识别。</li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>friends</title>
      <link href="/friends/index.html"/>
      <url>/friends/index.html</url>
      
        <content type="html"><![CDATA[<p>TEST</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Project</title>
      <link href="/project/index.html"/>
      <url>/project/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>index</title>
      <link href="/pages/about/index.html"/>
      <url>/pages/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
